---
title: "Data Science Capstone Project: Milestone Report"
author: "Alexey Serdyuk"
date: "16/06/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of content

* [Synopsis](#synopsis)
* [Prerequisites](#prerequisites)
* [Obtaining the data](#obtaining_data)
* [Splitting the data](#splitting_data)
* [First glance on the data and general plan](#first_glance)
* [Cleaning up and preprocessing the corpus](#cleaning_preprocessing)
* [Analyzing words (1-grams)](#analyze_1_grams)
* [Analyzing bigrams](#analyze_2_grams)
* [Pruning bigrams](#prune_2_grams)
* [3-grams to 6-grams](#3_to_6_grams)

# <a name="synopsis"></a>Synopsis

This is a milestone report for Week 2 of the capstone project for the cycle of courses
[Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)
offered on [Coursera](https://www.coursera.org) by
[Johns Hopkins University](https://www.jhu.edu).

The purpose of the capstone project is to build a Natural Language Processing
(NLP) application, that, given a chunk of text, predicts the next most probable
word. The application may be used, for example, in mobile devided to provide
suggestions as the user tips in some text.

In this report we will provide initial analysis of the data, as well as discuss
approach to building the application.

# <a name="prerequisites"></a>Prerequisites

An important question is which library to use for processing and analyzing the
corpora, as R provides several alternatives. Initially we attempted to use
the library `tm`, but quickly found that the library is very memory-hungry,
and an attempt to build bi- or trigrams for a large corpus are not practical.
After some googling we decided to use the library `quanteda` instead.

Too keep our namespace relatively clean, we load only those libraries from
which we use many functions, or if we use the same function many times.
Otherwise we will use full syntax, for example `stopwords::stopwords()`.

We start by loading required libraries.

```{r message=FALSE}
library(data.table) # For fast access in data tables.
library(ggplot2) # For plotting charts.
library(ggforce) # For plotting charts.
library(grid) # For arranging charts in a grid.
library(gridExtra) # For arranging charts in a grid.
library(kableExtra) # For pretty-printing tables.
library(parallel) # For parallel processing.
library(quanteda) # For handling the corpora.
library(readr) # For fast reading/writing.
library(R.utils) # For counting lines in files.
library(stringr) # For operations on strings.
library(tidyverse) # For cleaning up and faster modifications of data tables.
``` 
    
To speed up processing of large data sets, we will apply parallel version of
`lapply` function from the library `parallel`. To use all the available
resources, we detect a number of CPU cores and configure the library to use
them all.

```{r}
cpu.cores <- detectCores()
options(mc.cores = cpu.cores)
``` 

# <a name="obtaining_data"></a>Obtaining the data

Here and at some times later we use caching to speed up rendering of this
document. Results of long-running operations are stored, and used again during
the next run. If you wish to re-run all operations, just remove the `cache`
directory.

```{r}
if (!dir.exists("cache")) {
    dir.create("cache")
}
``` 

We download the data from the URL provided in the course description, and unzip
it.

```{r}
if (!file.exists("cache/Coursera-SwiftKey.zip")) {
    download.file(url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
                  destfile = "cache/Coursera-SwiftKey.zip", method = "curl")
    unzip("cache/Coursera-SwiftKey.zip", exdir = "cache")
}
```

# <a name="splitting_data"></a>Splitting the data

The downloaded zip file contains corpora in several languages: English, German,
Russian and Finnish. In our project we will use only English corpora.

Corpora in each language, including English, contains 3 files with content
obtained from different sources: news, blogs and twitter.

As the first step, we will split each relevant file on 3 parts:

* Training set (60%) will be used to build and train the algorithm.
* Testing set (20%) will be used to test the algorithm during it's development.
This set may be used more than once.
* Validation set (20%) will be used for a final validation and estimation
of out-of-sample performance. This set will be used only once.

We define a function which splits the specified file on parts described above:

```{r}
# Arguments:
# name - the file to split
# out.dir - output directory
splitFile <- function(name, out.dir) {
    # Reading dataset from the input file.
    data <- read_lines(name)

    # Prepare list with indexes of all data items.
    data.index <- 1:length(data)

    # Sample indices for the training data set, and create a set with remaining
    # indices.
    training.index <- sample(data.index, 0.6 * length(data.index))
    remaining.index <- data.index[! data.index %in% training.index]

    # Sample indices for the testing data set, and use remaining indices
    # for a validation data set.
    testing.index <- sample(remaining.index, 0.5 * length(remaining.index))
    validation.index <- remaining.index[! remaining.index %in% testing.index]

    # Split the data.
    data.training <- data[training.index]
    data.testing <- data[testing.index]
    data.validation <- data[validation.index]
    
    # Create an output directory, if it does not exist.
    if (!dir.exists(out.dir)) {
        dir.create(out.dir)
    }
    
    # Prepare names for output files. We append suffixes "training", "testing"
    # and "validation" to the input file name before the extension.
    base <- basename(name)
    outTraining <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.training.txt", base))
    outTesting <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.testing.txt", base))
    outValidation <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.validation.txt", base))

    # Writing datasets to output files.
    write_lines(data.training, outTraining)
    write_lines(data.testing, outTesting)
    write_lines(data.validation, outValidation)
}
```

To make results reproduceable, we set the seed of the random number generator.

```{r}
set.seed(20190530)
```

Finally, we split each of the data files.

```{r eval=FALSE}
splitFile("cache/final/en_US/en_US.blogs.txt", "cache")
splitFile("cache/final/en_US/en_US.news.txt", "cache")
splitFile("cache/final/en_US/en_US.twitter.txt", "cache")
```
```{r splitFile, echo=FALSE}
# The code above is a shortened display version.
# The real code here caches files to speed-up repeated execution.
if (!file.exists("cache/en_US.blogs.training.txt")) {
    splitFile("cache/final/en_US/en_US.blogs.txt", "cache")
}
if (!file.exists("cache/en_US.news.training.txt")) {
    splitFile("cache/final/en_US/en_US.news.txt", "cache")
}
if (!file.exists("cache/en_US.twitter.training.txt")) {
    splitFile("cache/final/en_US/en_US.twitter.txt", "cache")
}
```

As a sanity check, we count a number of lines in each source file, as well in
the partial files produced by the split.

```{r countLines, message=FALSE, cache=TRUE}
count.blogs <- R.utils::countLines("cache/final/en_US/en_US.blogs.txt")
count.blogs.training <- R.utils::countLines("cache/en_US.blogs.training.txt")
count.blogs.testing <- R.utils::countLines("cache/en_US.blogs.testing.txt")
count.blogs.validation <- R.utils::countLines("cache/en_US.blogs.validation.txt")

count.news <- R.utils::countLines("cache/final/en_US/en_US.news.txt")
count.news.training <- R.utils::countLines("cache/en_US.news.training.txt")
count.news.testing <- R.utils::countLines("cache/en_US.news.testing.txt")
count.news.validation <- R.utils::countLines("cache/en_US.news.validation.txt")

count.twitter <- R.utils::countLines("cache/final/en_US/en_US.twitter.txt")
count.twitter.training <- R.utils::countLines("cache/en_US.twitter.training.txt")
count.twitter.testing <- R.utils::countLines("cache/en_US.twitter.testing.txt")
count.twitter.validation <- R.utils::countLines("cache/en_US.twitter.validation.txt")
```

```{r showCountLines, echo=FALSE, cache=TRUE, dependson='countLines'}
corpora.count <- data.frame(
  "blogs" = c(count.blogs.training,
              count.blogs.testing,
              count.blogs.validation,
              count.blogs,
              count.blogs - (count.blogs.training +
                             count.blogs.testing +
                             count.blogs.validation)),
  "blogs.pct" = c(count.blogs.training / count.blogs * 100,
                  count.blogs.testing / count.blogs * 100,
                  count.blogs.validation / count.blogs * 100,
                  count.blogs / count.blogs * 100,
                  NA),
  "news" = c(count.news.training,
             count.news.testing,
             count.news.validation,
             count.news,
             count.news - (count.news.training +
                           count.news.testing +
                           count.news.validation)),
  "news.pct" = c(count.news.training / count.news * 100,
                 count.news.testing / count.news * 100,
                 count.news.validation / count.news * 100,
                 count.news / count.news * 100,
                 NA),
  "twitter" = c(count.twitter.training,
                count.twitter.testing,
                count.twitter.validation,
                count.twitter,
                count.twitter - (count.twitter.training +
                                 count.twitter.testing +
                                 count.twitter.validation)),
  "twitter.pct" = c(count.twitter.training / count.twitter * 100,
                    count.twitter.testing / count.twitter * 100,
                    count.twitter.validation / count.twitter * 100,
                    count.twitter / count.twitter * 100,
                    NA)
)
rownames(corpora.count) <- c("Training", "Testing", "Validation", "Total",
                             "Control (expected to be 0)")
kable(corpora.count,
      booktabs = TRUE,
      col.names = c("Rows", "%", "Rows", "%", "Rows", "%")) %>%
    add_header_above(header = c("",
                                "Blogs" = 2,
                                "News" = 2,
                                "Twitter" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

As the table shows, we have splitted the data on sub-sets as intended.

# <a name="first_glance"></a>First glance on the data and general plan

In the section above we have already counted a number of lines. Let us load
training data sets and take a look on the first 3 lines of each data set.

```{r loadTrainingData}
blogs.text <- read_lines("cache/en_US.blogs.training.txt")
news.text <- read_lines("cache/en_US.news.training.txt")
twitter.text <- read_lines("cache/en_US.twitter.training.txt")
```

```{r}
head(blogs.text, 3)
head(news.text, 3)
head(twitter.text, 3)
```

we could see that the data contains not only words, but also numbers and
punctuation. The punctuation may be non-ASCII (Unicode), as the first example
in the blogs sample shows (it contains a character "…", which is different from
3 ASCII point characters ". . ."). Some lines may contain multiple sentences,
and probably we have to take this into account.

Here is our plan:

* Split text on sentences.
* Clean up the corpus: remove non-language parts such as e-mail addresses and
URLs, etc.
* Preprocess the corpus: remove punctuation and numbers, change all words
to lower-case.
* Analyze distribution of words to decide if we should base our prediction on
the full dictionary, or just on some sub-set of it.
* Analyze n-grams for small n.

# <a name="cleaning_preprocessing"></a>Cleaning up and preprocessing the corpus

We decided to split text on sentences and do not attempt to predict words across
sentence border. We still may use information about sentences to improve
prediction of the first word, because the frequency of the first word in a
sentence may be very different from an average frequency.

```{r splitOnSentences, cache=TRUE, cache.lazy=FALSE}
blogs.text <- unlist(tokenizers::tokenize_sentences(blogs.text))
news.text <- unlist(tokenizers::tokenize_sentences(news.text))
twitter.text <- unlist(tokenizers::tokenize_sentences(twitter.text))
```

Libraries contains some functions for cleaning up and pre-processing, but
for some steps we have to write functions ourselves.

```{r}
# Remove URLs. The regular expression detects http(s) and ftp(s) protocols.
removeUrl <- function(x) gsub("(ht|f)tp(s?)://\\S+", "", x)

# Remove e-mail addresses.
# The regular expression from Stack Overflow:
# https://stackoverflow.com/questions/201323/how-to-validate-an-email-address-using-a-regular-expression
removeEmail <- function(x) gsub("(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])", "", x, perl = TRUE)

# Remove hash tags (the character # and the following word) and twitter handles
# (the character @ and the following word).
removeTagsAndHandles <- function(x) gsub("[@#]\\S+", "", x)

# Surround punctuation marks which does not appear inside a word with space
# characters. Without this step, fragments with a missing space are transformed
# to a single non-existing word when punctuation is removed.
# Example: corpus contains
# "I had the best day yesterday,it was like two kids in a candy store"
# Without this step, "yesterday,it" is transformed to a non-existing word
# "yesterdayit" when removing punctuation. This step transforms it to
# "yesterday, it"
addMissingSpace <- function(x) gsub("[,()\":;”…]", " ", x)

# Replace words in a sentence with replacements available from the table.
# Keep words which are not in the replacement table "as is".
# As a side effect, removes punctuation and transforms to a lower case.
#
# This step is required for several purposes:
# * Replace common short forms with full forms, for example "he'll" = "he will"
replacements.text <- readr::read_csv("replacements.txt",
    col_names = c("token", "replacement"),
    col_types = list(col_character(), col_character()))

replaceWords <- function(text, replacements) {
  # Split text on words.
  tokens.orig <- tokenizers::tokenize_words(text, simplify = TRUE,
                                            strip_numeric = TRUE)
  
  # Attempt to replace each word.
  tokens.replaced <- sapply(tokens.orig, function(x) {
    # Search if a replacement exist.
    replacement.index <- match(x, replacements$token)
    if (is.na(replacement.index)) {
      # Can't find a replacement, fall back on the token itself.
      return (x)
    } else {
      # Replace the token.
      return (replacements$replacement[replacement.index])
    }
  }, USE.NAMES = FALSE)
  
  paste(tokens.replaced, collapse = " ")
}

# Add tokens representing start and end of a sentence.
# SOS = Start Of Sentence
# EOS = End Of Sentence
# When we add these tokens, our text was already transformed to a lower case,
# so we could easy distinguish upper case special tokens from lower case text.
addSentenceTokens <- function(x) paste("SOS", x, "EOS")

# Collapse space characters: if there are more than 1 space character in a row,
# replace with a single one.
collapseWhitespace <- function(x) gsub("\\s+", " ", x)

# ... And now combine all functions in a pre-processing chain.
preProcessText <- function(x) {
    text <- removeUrl(x)
    text <- removeEmail(text)
    text <- removeTagsAndHandles(text)
    text <- addMissingSpace(text)
    text <- replaceWords(text, replacements.text)
    text <- addSentenceTokens(text)
    text <- collapseWhitespace(text)
}
```

Now we pre-process the data.

```{r eval=FALSE}
blogs.text.preprocessed <- unlist(mclapply(blogs.text, preProcessText))
news.text.preprocessed <- unlist(mclapply(news.text, preProcessText))
twitter.text.preprocessed <- unlist(mclapply(twitter.text, preProcessText))
```
```{r preprocessTrainingData, echo=FALSE}
if (!file.exists("cache/blogs.text.preprocessed.txt")) {
  blogs.text.preprocessed <- unlist(mclapply(blogs.text, preProcessText))
  write_lines(blogs.text.preprocessed, "cache/blogs.text.preprocessed.txt")
} else {
  blogs.text.preprocessed <- read_lines("cache/blogs.text.preprocessed.txt")
}
if (!file.exists("cache/news.text.preprocessed.txt")) {
  news.text.preprocessed <- unlist(mclapply(news.text, preProcessText))
  write_lines(news.text.preprocessed, "cache/news.text.preprocessed.txt")
} else {
  news.text.preprocessed <- read_lines("cache/news.text.preprocessed.txt")
}
if (!file.exists("cache/twitter.text.preprocessed.txt")) {
  twitter.text.preprocessed <- unlist(mclapply(twitter.text, preProcessText))
  write_lines(twitter.text.preprocessed, "cache/twitter.text.preprocessed.txt")
} else {
  twitter.text.preprocessed <- read_lines("cache/twitter.text.preprocessed.txt")
}
```


# <a name="analyze_1_grams"></a>Analyzing words (1-grams)

In this section we will study distribution of words in corpora, ignoring for the
moment interaction between words (n-grams).

We define two helper functions. The first one creates a Document Feature Matrix
(DFM) for n-grams in documents, and aggregates it over all documents to a
Feature Vector. The second helper function enriches the Feature Vector with
additional values useful for our analysis, such as cumulated coverage of text.

```{r}
# Calculate Document Feature Matrix (DFM) for n-grams in documents,
# and aggregate it over all documents to a Feature Vector.
build.ngram <- function(text, n, min_freq = 1, stop_words = NULL) {
  # Split text on 1-grams.
  text.tokens <- tokens(text)
  
  # Remove stop-words, if required.
  if (!is.null(stop_words)) {
    text.tokens <- tokens_remove(text.tokens, stop_words)
  }
  
  # Stem words. We are using an explicit stemming to make sure that it is fully
  # compatible with another places later in the code where we have to apply
  # the stemming manually.
  text.tokens <- as.tokens(
    mclapply(text.tokens, function(x) SnowballC::wordStem(x, language = "en")))
  
  # Create n-grams, if n > 1
  if (n > 1) {
    text.tokens <- tokens_ngrams(text.tokens, n = n, concatenator = " ")
  }
  
  # Special case: if our corpus contains empty sentences, than 2-grams contains
  # "SOS EOS", that is sequences of "Start-Of-Sentence" + "End-Of-Sentence"
  # tokens. This may happens, for example, if a sentence contains only stop
  # words, or in some weird cases like a twitter which contains only a time
  # like "8:12". We are not interested in empty sequences, so we are removing
  # such tokens.
  if (n == 2) {
    text.tokens <- tokens_remove(text.tokens, c("SOS EOS"))
  }
  
  # Calculate the Document Feature Matrix
  text.dfm <- dfm(text.tokens, tolower = FALSE)
  
  # Remove from DFM least frequent features, if requested.
  if (min_freq > 1) {
    text.dfm <- dfm_trim(text.dfm, min_termfreq = min_freq)
  }
  
  # Sum over all documents.
  colSums(text.dfm)
}

# Sorts a Feature Vector in descending order of frequency and enriches it with
# additional columns:
#  * Cumulated frequency of terms (words or n-grams)
#  * Cumulated frequency as percentage of total.
enrich.ngram <- function(fv) {
  # Transform Feature Vector to a table and sort by frequency descending.
  tbl <- data.table(Terms = names(fv), Freq = fv)
  tbl <- tbl[order(-Freq)]
  
  # Add columns with cumulative frequency as a number of words, and as percentage.
  tbl$Freq.Cum <- cumsum(tbl$Freq)
  tbl$Freq.Cum.Pct <- tbl$Freq.Cum / sum(tbl$Freq)
  
  return (tbl)
}
```

Now we may calculate frequency of words in each source, as well as in all
sources together (aggregated).

```{r eval=FALSE}
# Define stop-words for 1-grams: standard stop words, as well as our special
# tokens "Start-Of-Sentence" and "End-Of-Sentence".
stopwords.tokens <- c(stopwords(), "SOS", "EOS")

# Calculate a frequency of words in each sources, as well as an aggregated.
blogs.1gram.freq <- enrich.ngram(build.ngram(blogs.text.preprocessed, 1,
                                             stop_words = stopwords.tokens))
news.1gram.freq <- enrich.ngram(build.ngram(news.text.preprocessed, 1,
                                            stop_words = stopwords.tokens))
twitter.1gram.freq <- enrich.ngram(build.ngram(twitter.text.preprocessed, 1,
                                               stop_words = stopwords.tokens))
all.text.preprocessed <- c(blogs.text.preprocessed, news.text.preprocessed,
                           twitter.text.preprocessed)
all.1gram.freq <- enrich.ngram(build.ngram(all.text.preprocessed, 1,
                                           stop_words = stopwords.tokens))
```
```{r build1grams, echo=FALSE}
# Define stop-words for 1-grams: standard stop words, as well as our special
# tokens "Start-Of-Sentence" and "End-Of-Sentence".
stopwords.tokens <- c(stopwords(), "SOS", "EOS")

# Calculate a frequency of words in each sources, as well as an aggregated.
if (file.exists("cache/blogs.1gram.freq.RDS")) {
  blogs.1gram.freq <- readRDS("cache/blogs.1gram.freq.RDS")
} else {
  blogs.1gram.freq <- enrich.ngram(build.ngram(blogs.text.preprocessed, 1,
                                               stop_words = stopwords.tokens))
  saveRDS(blogs.1gram.freq, "cache/blogs.1gram.freq.RDS")
}
if (file.exists("cache/news.1gram.freq.RDS")) {
  news.1gram.freq <- readRDS("cache/news.1gram.freq.RDS")
} else {
  news.1gram.freq <- enrich.ngram(build.ngram(news.text.preprocessed, 1,
                                              stop_words = stopwords.tokens))
  saveRDS(news.1gram.freq, "cache/news.1gram.freq.RDS")
}
if (file.exists("cache/twitter.1gram.freq.RDS")) {
  twitter.1gram.freq <- readRDS("cache/twitter.1gram.freq.RDS")
} else {
  twitter.1gram.freq <- enrich.ngram(build.ngram(twitter.text.preprocessed, 1,
                                                 stop_words = stopwords.tokens))
  saveRDS(twitter.1gram.freq, "cache/twitter.1gram.freq.RDS")
}
if (file.exists("cache/all.1gram.freq.RDS")) {
  all.1gram.freq <- readRDS("cache/all.1gram.freq.RDS")
} else {
  all.text.preprocessed <- c(blogs.text.preprocessed, news.text.preprocessed,
                             twitter.text.preprocessed)
  all.1gram.freq <- enrich.ngram(build.ngram(all.text.preprocessed, 1,
                                             stop_words = stopwords.tokens))
  
  # Clean up memory.
  rm(all.text.preprocessed)
  dummy <- gc(full = TRUE, verbose = FALSE)
  
  saveRDS(all.1gram.freq, "cache/all.1gram.freq.RDS")
}
```

The following chart displays 20 most-frequent words in each source, as well
as in the aggregated corpora.

```{r mostFrequent1Grams, echo=FALSE, cache=TRUE, cache.lazy=TRUE, fig.align='center', fig.width=10, fig.height=8}
# Display 20 most-frequent-words.
blogs.freq.top <- blogs.1gram.freq[1:20]
blogs.freq.top$Terms <- factor(blogs.freq.top$Terms,
                               levels = blogs.freq.top[order(20:1)]$Terms)

news.freq.top <- news.1gram.freq[1:20]
news.freq.top$Terms <- factor(news.freq.top$Terms,
                              levels = news.freq.top[order(20:1)]$Terms)

twitter.freq.top <- twitter.1gram.freq[1:20]
twitter.freq.top$Terms <- factor(twitter.freq.top$Terms,
                                 levels = twitter.freq.top[order(20:1)]$Terms)

all.freq.top <- all.1gram.freq[1:20]
all.freq.top$Terms <- factor(all.freq.top$Terms,
                             levels = all.freq.top[order(20:1)]$Terms)

g.blogs <- ggplot(blogs.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#CC8888", high = "#CC0000") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Blogs") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.news <- ggplot(news.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#88CC88", high = "#00CC00") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "News") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.twitter <- ggplot(twitter.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#8888CC", high = "#0000CC") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Twitter") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.all <- ggplot(all.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient2(low = "#8888CC", mid = "#44CC44", high = "#CC0000", midpoint = 10) +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Aggregated") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")


grid.arrange(grobs = list(g.blogs, g.news, g.twitter, g.all),
             widths = c(1, 1, 1),
             layout_matrix = rbind(c(1, 2, 3),
                                   c(NA, 4, NA)),
             top = textGrob("Top 20 Words", gp = gpar(fontsize=20)))
```

As we see from the chart, top 20 most-frequent words differs between sources.
For example, the most frequent word in news is "said", but this word is not
included in the top-20 list for blogs and Twitter at all. At the same time,
some words are shared between lists: the word "can" is 2nd most-frequent in
blogs, 3rd-most frequest in Twitter, and 5th in and news.

Our next step is to analyze the intersection, that is to find how many words
are common to all sources, and how many are unique to a particular source.
Not only just a number of words is important, but also a source coverage, that
is what percentage of the whole text of a particular source is covered by
a particular subset of all words.

The following [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) shows
a number of unique words (stems) used in each source, as well as a percentage
of the aggregated corpora covered by those words.

```{r venn1grams, echo=FALSE, cache=TRUE, cache.lazy=TRUE, fig.align='center'}
all.venn <- data.frame(
  "Terms" = all.1gram.freq$Terms,
  "Freq" = all.1gram.freq$Freq,
  "Blogs" = all.1gram.freq$Terms %in% blogs.1gram.freq$Terms,
  "News" = all.1gram.freq$Terms %in% news.1gram.freq$Terms,
  "Twitter" = all.1gram.freq$Terms %in% twitter.1gram.freq$Terms
)
all.venn$B <- (all.venn$Blogs & !all.venn$News & !all.venn$Twitter)
all.venn$N <- (!all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$T <- (!all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$BN <- (all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$BT <- (all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$NT <- (!all.venn$Blogs & all.venn$News & all.venn$Twitter)
all.venn$BNT <- (all.venn$Blogs & all.venn$News & all.venn$Twitter)

df.venn <- data.frame(x = c(0, 0.866, -0.866),
                      y = c(1, -0.5, -0.5),
                      labels = c('Blogs', 'News', 'Twitter'))
df.venn.count <- data.frame(x = c(0, 1.2, 0.8, -1.2, -0.8, 0, 0),
                            y = c(1.2, -0.6, 0.5, -0.6, 0.5, -1, 0))
df.venn.count$Counts <- c(sum(all.venn$B),
                          sum(all.venn$N),
                          sum(all.venn$BN),
                          sum(all.venn$T),
                          sum(all.venn$BT),
                          sum(all.venn$NT),
                          sum(all.venn$BNT))
df.venn.count$Freq <- c(sum(all.venn$Freq[all.venn$B]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$N]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BN]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$T]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$NT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BNT]) / sum(all.venn$Freq)
)
df.venn.count$Label <- paste0(formatC(df.venn.count$Counts, big.mark = ",", format = "d"),
                              "\n",
                              sprintf("%.2f%%", df.venn.count$Freq * 100))

# Clean up memory.
rm(all.venn)
dummy <- gc(full = TRUE, verbose = FALSE)

ggplot(df.venn) +
  geom_circle(aes(x0 = x, y0 = y, r = 1.5, fill = labels), alpha = .3, size = 1, colour = "#666666") +
  coord_fixed() +
  theme_void(base_size = 14) +
  theme(legend.position = 'bottom') +
  scale_fill_manual(values = c("#CC0000", "#008800", "#0000FF")) +
  scale_colour_manual(values = c("#CC0000", "#008800", "#0000FF"), guide = FALSE) +
  labs(title = "Stems Common in Multiple Corpora") +
  labs(fill = NULL) +
  annotate("text", x = df.venn.count$x, y = df.venn.count$y, label = df.venn.count$Label, size = 5)
```

As we may see, `r sprintf("%d", df.venn.count$Counts[7])` words are shared by
all 3 corpora, but those words cover `r sprintf("%.2f%%", df.venn.count$Freq[7]
* 100)` of the aggregated corpora. On the other hand, there are `r sprintf("%d",
df.venn.count$Counts[1])` words unique to blogs, but these words appear very
infrequently, covering just `r sprintf("%.2f%%", df.venn.count$Freq[1] * 100)`
of the aggregated corpora.

The Venn diagram indicates that we may get a high coverage of all corpora by
choosing common words. Coverage by words specific to a particular corpus is
negligible.

The next step in our analysis is to find out how many common words we should
choose to achieve a decent coverage of the text. From the Venn diagram we
already know that by choosing `r sprintf("%d", df.venn.count$Counts[7])` words
we will cover `r sprintf("%.2f%%", df.venn.count$Freq[7] * 100)` of the
aggregated corpora, but maybe we may reduce a number of words without
significantly reducing the coverage.

The following chart shows a number of unique words in each source which cover
particular percentage of the text. For example, 1000 most-frequent words cover
`r sprintf("%.2f%%", twitter.1gram.freq[1000]$Freq.Cum.Pct * 100)` of the
Twitter corpus. An interesting observation is that Twitter requires less words
to cover particular percentage of the text, whereas news requires more words.

```{r cover1grams, echo=FALSE, cache=TRUE, fig.align='center'}
# The data set is very large, it takes too long to render a chart with all
# data points, and we can't show more sample points as there are pixels anyway.
# We will choose a sample points and interpolate between them.
# Since our X-axis has a log scale, we are choosing points to that they equally
# spaced on a log scale.

blogs.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(blogs.1gram.freq)) / 1000)))
news.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(news.1gram.freq)) / 1000)))
twitter.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(twitter.1gram.freq)) / 1000)))
all.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(all.1gram.freq)) / 1000)))

blogs.freq.pct <- data.frame("Source" = rep("Blogs", length(blogs.freq.idx)),
                             "Count" = blogs.freq.idx,
                             "Freq.Pct" = blogs.1gram.freq$Freq.Cum.Pct[blogs.freq.idx])

news.freq.pct <- data.frame("Source" = rep("News", length(news.freq.idx)),
                            "Count" = news.freq.idx,
                            "Freq.Pct" = news.1gram.freq$Freq.Cum.Pct[news.freq.idx])

twitter.freq.pct <- data.frame("Source" = rep("Twitter", length(twitter.freq.idx)),
                               "Count" = twitter.freq.idx,
                               "Freq.Pct" = twitter.1gram.freq$Freq.Cum.Pct[twitter.freq.idx])

all.freq.pct <- data.frame("Source" = rep("Aggregated", length(all.freq.idx)),
                           "Count" = all.freq.idx,
                           "Freq.Pct" = all.1gram.freq$Freq.Cum.Pct[all.freq.idx])

stat.freq.pct <- rbind(blogs.freq.pct, news.freq.pct, twitter.freq.pct, all.freq.pct)


ggplot(data = stat.freq.pct, aes(x = Count, y = Freq.Pct, group = Source)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000),
                     labels = c("10", "100", "1,000", "10,000", "100,000")) +
  scale_color_manual(values=c("#CC0000", "#008800", "#0000FF", "#666666")) +
  geom_line(aes(color = Source), size = 1.1) +
  labs(title = "Cumulative Frequency of Stems") +
  labs(x = "Unique stems") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

```{r cover1grams.tbl, echo=FALSE, cache=TRUE}
all.1gram.freq.tbl <-
    data.frame(
        "Corpora Coverage" = c("75%", "90%", "95%", "99%", "99.9%"),
        "Blogs" = c(
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "News" = c(
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Twitter" = c(
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Aggregated" = c(
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.999])
        )
    )
all.1gram.freq.tbl[,2] <- formatC(all.1gram.freq.tbl[,2], big.mark = ",", format = "d")
all.1gram.freq.tbl[,3] <- formatC(all.1gram.freq.tbl[,3], big.mark = ",", format = "d")
all.1gram.freq.tbl[,4] <- formatC(all.1gram.freq.tbl[,4], big.mark = ",", format = "d")
all.1gram.freq.tbl[,5] <- formatC(all.1gram.freq.tbl[,5], big.mark = ",", format = "d")

kable(all.1gram.freq.tbl, booktabs = TRUE,
      col.names = c("Corpora Coverage",
                    "Blogs",
                    "News",
                    "Twitter",
                    "Aggregated")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The table shows that in order to cover 95% of blogs, we require
`r all.1gram.freq.tbl$Blogs[3]` words. The same coverage of news
require `r all.1gram.freq.tbl$News[3]` words, and the coverage
of twitter `r all.1gram.freq.tbl$Twitter[3]` words. To cover 95%
of the aggregated corpora, we require
`r all.1gram.freq.tbl$Aggregated[3]` unique words. We may use this fact
later to reduce a number of n-grams required for predictions.

# <a name="analyze_2_grams"></a>Analyzing bigrams

In this section we will study distribution bigrams, that is combinations of
two words.

Using previously defined functions, we may calculate frequency of bigrams in
each source, as well as in all sources together (aggregated).

```{r eval=FALSE}
# Calculate a frequency of words in each sources, as well as an aggregated.
blogs.2gram.freq <- enrich.ngram(build.ngram(blogs.text.preprocessed, 2,
                                             stop_words = stopwords.tokens))
news.2gram.freq <- enrich.ngram(build.ngram(news.text.preprocessed, 2,
                                            stop_words = stopwords.tokens))
twitter.2gram.freq <- enrich.ngram(build.ngram(twitter.text.preprocessed, 2,
                                               stop_words = stopwords.tokens))

all.text.preprocessed <- c(blogs.text.preprocessed, news.text.preprocessed,
                           twitter.text.preprocessed)
all.2gram.freq <- enrich.ngram(build.ngram(all.text.preprocessed, 2,
                                           stop_words = stopwords.tokens))
```
```{r build2grams, echo=FALSE}
# Calculate a frequency of words in each sources, as well as an aggregated.
if (file.exists("cache/blogs.2gram.freq.RDS")) {
  blogs.2gram.freq <- readRDS("cache/blogs.2gram.freq.RDS")
} else {
  blogs.2gram.freq <- enrich.ngram(build.ngram(blogs.text.preprocessed, 2,
                                               stop_words = stopwords.tokens))
  
  saveRDS(blogs.2gram.freq, "cache/blogs.2gram.freq.RDS")
}
if (file.exists("cache/news.2gram.freq.RDS")) {
  news.2gram.freq <- readRDS("cache/news.2gram.freq.RDS")
} else {
  news.2gram.freq <- enrich.ngram(build.ngram(news.text.preprocessed, 2,
                                              stop_words = stopwords.tokens))
  
  saveRDS(news.2gram.freq, "cache/news.2gram.freq.RDS")
}
if (file.exists("cache/twitter.2gram.freq.RDS")) {
  twitter.2gram.freq <- readRDS("cache/twitter.2gram.freq.RDS")
} else {
  twitter.2gram.freq <- enrich.ngram(build.ngram(twitter.text.preprocessed, 2,
                                                 stop_words = stopwords.tokens))
  
  saveRDS(twitter.2gram.freq, "cache/twitter.2gram.freq.RDS")
}
if (file.exists("cache/all.2gram.freq.RDS")) {
  all.2gram.freq <- readRDS("cache/all.2gram.freq.RDS")
} else {
  all.text.preprocessed <- c(blogs.text.preprocessed, news.text.preprocessed,
                             twitter.text.preprocessed)
  all.2gram.freq <- enrich.ngram(build.ngram(all.text.preprocessed, 2,
                                             stop_words = stopwords.tokens))
  
  # Clean up memory.
  rm(all.text.preprocessed)
  dummy <- gc(full = TRUE, verbose = FALSE)
  
  saveRDS(all.2gram.freq, "cache/all.2gram.freq.RDS")
}
```

The following chart displays 20 most-frequent bigrams in each source, as well
as in the aggregated corpora.

```{r mostFrequent2Grams, echo=FALSE, cache=TRUE, fig.align='center', fig.width=10, fig.height=8}
blogs.freq.top <- blogs.2gram.freq[1:20]
blogs.freq.top$Terms <- factor(blogs.freq.top$Terms,
                               levels = blogs.freq.top[order(20:1)]$Terms)

news.freq.top <- news.2gram.freq[1:20]
news.freq.top$Terms <- factor(news.freq.top$Terms,
                              levels = news.freq.top[order(20:1)]$Terms)

twitter.freq.top <- twitter.2gram.freq[1:20]
twitter.freq.top$Terms <- factor(twitter.freq.top$Terms,
                                 levels = twitter.freq.top[order(20:1)]$Terms)

all.freq.top <- all.2gram.freq[1:20]
all.freq.top$Terms <- factor(all.freq.top$Terms,
                             levels = all.freq.top[order(20:1)]$Terms)

g.blogs <- ggplot(blogs.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#CC8888", high = "#CC0000") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Blogs") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.news <- ggplot(news.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#88CC88", high = "#00CC00") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "News") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.twitter <- ggplot(twitter.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#8888CC", high = "#0000CC") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Twitter") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.all <- ggplot(all.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient2(low = "#8888CC", mid = "#44CC44", high = "#CC0000", midpoint = 10) +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Aggregated") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")


grid.arrange(grobs = list(g.blogs, g.news, g.twitter, g.all),
             widths = c(1, 1, 1),
             layout_matrix = rbind(c(1, 2, 3),
                                   c(NA, 4, NA)),
             top = textGrob("Top 20 Bigrams", gp = gpar(fontsize=20)))  
```

We immediately see a difference with lists of top 20 words: there were much more
common words between sources, as there are common bigrams. There are still
some common bigrams, but the intersection is smaller.

Similar to how we proceed with words, now we will analyze intersections, that is
we will find how many bigrams are common to all sources, and how many are unique
to a particular source. We also calculate a percentage of the whole source
covered by a particular subset of all bigrams.

The following [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) shows
a number of unique bigrams used in each source, as well as a percentage
of the aggregated corpora covered by those bigrams.

```{r venn2grams, echo=FALSE, cache=TRUE, cache.lazy=FALSE, fig.align='center'}
all.venn <- data.frame(
  "Terms" = all.2gram.freq$Terms,
  "Freq" = all.2gram.freq$Freq,
  "Blogs" = all.2gram.freq$Terms %in% blogs.2gram.freq$Terms,
  "News" = all.2gram.freq$Terms %in% news.2gram.freq$Terms,
  "Twitter" = all.2gram.freq$Terms %in% twitter.2gram.freq$Terms
)
all.venn$B <- (all.venn$Blogs & !all.venn$News & !all.venn$Twitter)
all.venn$N <- (!all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$T <- (!all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$BN <- (all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$BT <- (all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$NT <- (!all.venn$Blogs & all.venn$News & all.venn$Twitter)
all.venn$BNT <- (all.venn$Blogs & all.venn$News & all.venn$Twitter)

df.venn <- data.frame(x = c(0, 0.866, -0.866),
                      y = c(1, -0.5, -0.5),
                      labels = c('Blogs', 'News', 'Twitter'))
df.venn.count <- data.frame(x = c(0, 1.2, 0.8, -1.2, -0.8, 0, 0),
                            y = c(1.2, -0.6, 0.5, -0.6, 0.5, -1, 0))
df.venn.count$Counts <- c(sum(all.venn$B),
                          sum(all.venn$N),
                          sum(all.venn$BN),
                          sum(all.venn$T),
                          sum(all.venn$BT),
                          sum(all.venn$NT),
                          sum(all.venn$BNT))
df.venn.count$Freq <- c(sum(all.venn$Freq[all.venn$B]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$N]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BN]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$T]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$NT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BNT]) / sum(all.venn$Freq)
)
df.venn.count$Label <- paste0(formatC(df.venn.count$Counts, big.mark = ",", format = "d"),
                              "\n",
                              sprintf("%.2f%%", df.venn.count$Freq * 100))

# Clean up memory
rm(all.venn)
dummy <- gc(full = TRUE, verbose = FALSE)

ggplot(df.venn) +
  geom_circle(aes(x0 = x, y0 = y, r = 1.5, fill = labels), alpha = .3, size = 1, colour = "#666666") +
  coord_fixed() +
  theme_void(base_size = 14) +
  theme(legend.position = 'bottom') +
  scale_fill_manual(values = c("#CC0000", "#008800", "#0000FF")) +
  scale_colour_manual(values = c("#CC0000", "#008800", "#0000FF"), guide = FALSE) +
  labs(title = "Bigrams Common in Multiple Corpora") +
  labs(fill = NULL) +
  annotate("text", x = df.venn.count$x, y = df.venn.count$y, label = df.venn.count$Label, size = 5)
```

The difference between words and bigrams is even more pronounced here.
Bigrams common to all sources cover just
`r sprintf("%.2f%%", df.venn.count$Freq[7] * 100)` of the text, compared to
more than 95% covered by words common to all sources.

The next step in our analysis is to find out how many common bigrams we should
choose to achieve a decent coverage of the text.

The following chart shows a number of unique bigrams in each source which cover
particular percentage of the text. For example, 1000 most-frequent bigrams cover
`r sprintf("%.2f%%", twitter.2gram.freq[1000]$Freq.Cum.Pct * 100)` of the
Twitter corpus.

```{r cover2grams, echo=FALSE, cache=TRUE, fig.align='center', fig.width=10}
# The data set is very large, it takes too long to render a chart with all
# data points, and we can't show more sample points as there are pixels anyway.
# We will choose a sample points and interpolate between them.
# Since our X-axis has a log scale, we are choosing points to that they equally
# spaced on a log scale.

blogs.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(blogs.2gram.freq)) / 1000)))
news.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(news.2gram.freq)) / 1000)))
twitter.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(twitter.2gram.freq)) / 1000)))
all.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(all.2gram.freq)) / 1000)))

blogs.freq.pct <- data.frame("Source" = rep("Blogs", length(blogs.freq.idx)),
                             "Count" = blogs.freq.idx,
                             "Freq.Pct" = blogs.2gram.freq$Freq.Cum.Pct[blogs.freq.idx])

news.freq.pct <- data.frame("Source" = rep("News", length(news.freq.idx)),
                            "Count" = news.freq.idx,
                            "Freq.Pct" = news.2gram.freq$Freq.Cum.Pct[news.freq.idx])

twitter.freq.pct <- data.frame("Source" = rep("Twitter", length(twitter.freq.idx)),
                               "Count" = twitter.freq.idx,
                               "Freq.Pct" = twitter.2gram.freq$Freq.Cum.Pct[twitter.freq.idx])

all.freq.pct <- data.frame("Source" = rep("Aggregated", length(all.freq.idx)),
                           "Count" = all.freq.idx,
                           "Freq.Pct" = all.2gram.freq$Freq.Cum.Pct[all.freq.idx])

stat.freq.pct <- rbind(blogs.freq.pct, news.freq.pct, twitter.freq.pct, all.freq.pct)

ggplot(data = stat.freq.pct, aes(x = Count, y = Freq.Pct, group = Source)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  scale_color_manual(values=c("#CC0000", "#008800", "#0000FF", "#666666")) +
  geom_line(aes(color = Source), size = 1.1) +
  labs(title = "Cumulative Frequency of Bigrams") +
  labs(x = "Unique bigrams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

```{r cover2grams.tbl, echo=FALSE, cache=TRUE}

all.freq.tbl <-
    data.frame(
        "Corpora Coverage" = c("75%", "90%", "95%", "99%", "99.9%"),
        "Blogs" = c(
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "News" = c(
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Twitter" = c(
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Aggregated" = c(
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.999])
        )
    )
all.freq.tbl[,2] <- formatC(all.freq.tbl[,2], big.mark = ",", format = "d")
all.freq.tbl[,3] <- formatC(all.freq.tbl[,3], big.mark = ",", format = "d")
all.freq.tbl[,4] <- formatC(all.freq.tbl[,4], big.mark = ",", format = "d")
all.freq.tbl[,5] <- formatC(all.freq.tbl[,5], big.mark = ",", format = "d")

kable(all.freq.tbl, booktabs = TRUE,
      col.names = c("Corpora Coverage",
                    "Blogs",
                    "News",
                    "Twitter",
                    "Aggregated")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The table shows that in order to cover 95% of blogs, we require
`r formatC(all.freq.tbl$Blogs[3], big.mark = ",", format = "d")` bigrams.
The same coverage of news
require `r formatC(all.freq.tbl$News[3], big.mark = ",", format = "d")` bigrams,
and the coverage of Twitter
`r formatC(all.freq.tbl$Twitter[3], big.mark = ",", format = "d")` bigrams.
To cover 95% of the aggregated corpora, we require
`r formatC(all.freq.tbl$Aggregated[3], big.mark = ",", format = "d")` bigrams.
  
The chart is also very different from a similar chart for words. The curve for
words had an "S"-shape, that is it's growth slowed down after some number of
words, so that adding more words results in diminishing returns. For bigrams,
there is no point of diminishing returns: curves are just rising.

As we have found in the section *[Analyzing words (1-grams)](#analyze_1_grams)*,
our corpora contains
$N_1=$ `r formatC(nrow(all.1gram.freq), big.mark = ",", format = "d")`
unique word stems. Potentially there could be
$N_1^2=$ `r formatC(nrow(all.1gram.freq)^2, big.mark = ",", format = "fg")`
bigrams, but we have observed only
$N_2=$ `r formatC(nrow(all.2gram.freq), big.mark = ",", format = "d")`,
that is `r sprintf("%.4f%%", 100 * nrow(all.2gram.freq) / (nrow(all.1gram.freq)^2))`
of all possible. Still, the number of observed bigrams is pretty large.
In the section *[Analyzing words (1-grams)](#analyze_1_grams)* we have found
that we may cover large part of the corpus by relatively small number of unique
word stems. In the next section we will see if we may reduce a number of unique
2-grams by utilizing that knowledge.

# <a name="prune_2_grams"></a>Pruning bigrams

We have found in the section *[Analyzing words (1-grams)](#analyze_1_grams)*,
that our corpora contains
$N_1=$ `r formatC(nrow(all.1gram.freq), big.mark = ",", format = "d")`
unique word stems, but just `r all.1gram.freq.tbl$Aggregated[3]` of them cover
95% of the corpus. In this section we will analyze whether we may reduce
a number of bigrams by utilizing that knowledge.

We will replace seldom words with a speial token `UNK`. This will reduce a
number of bigrams, because different word sequences may now produce the same
bigram, if those word sequences contains seldom words. For example, our word
list contains names "Andrei", "Charley" and "Fabio", but these words do not
belong to a subset of most common words required to achieve 95% coverage of the
corpus. If our corpus contains bigrams "Andrei told", "Charley told" and "Fabio
told", we will replace them all with a bigram "UNK told".

Since we will apply the same approach to 3-grams, 4-grams etc, to save time
we prune the corpora once and save results to files which we may load later.

We start by defining a function that accepts a sequence of words, a white-list
and a replacement token. All words in the sentence which are not included
in the white-list are replaced by the token.

```{r}
# Replace words not in the provided white-list with a replacement token.
replaceWordsNotIn <- function(text, whitelist, replacement) {
  # Split text on words.
  tokens.orig <- unlist(tokens(text))

  # Stem words.
  tokens.stem <- SnowballC::wordStem(tokens.orig, language = "en")

  # Check each stem against a whitelist.
  tokens.replaced <- mapply(function(word.orig, word.stem) {
    # Search if the stem in the whitelist.
    replacement.index <- match(word.stem, whitelist)
    if (is.na(replacement.index)) {
      # The word is not in the whitelist: replace it.
      return (replacement)
    } else {
      # The word in the whitelist: use the original.
      return (word.orig)
    }
  }, tokens.orig, tokens.stem, SIMPLIFY = TRUE, USE.NAMES = FALSE)
  
  paste(tokens.replaced, collapse = " ")
}
```

Now we create a white-list that contains:

* frequent words which covers 95% of the corpus,
* stop-words, that is functional words like "a" or "the" which we excluded
  from our word frequency analysis,
* special tokens *Stop-Of-Sentence* and *End-Of-Sentence* introduced earlier.

```{r}
# Calculate 95% of the most common words.
words.95 <- all.1gram.freq[Freq.Cum.Pct <= 0.95]$Terms

# Keep 95% of the most common words, all stopwords, as well as special tokens
# SOS/EOS.
words.whitelist <- c(words.95, stopwords(), "SOS", "EOS")
```

And now we apply the function defined above to replace all words not included
in the white-list with the token `UNK`. As usual, we cache produced files to
speed up repeated rendering of this document.

```{r eval=FALSE}
blogs.text.95 <- unlist(mclapply(blogs.text.preprocessed,
                                 replaceWordsNotIn,
                                 words.whitelist, "UNK"))
news.text.95 <- unlist(mclapply(news.text.preprocessed,
                                replaceWordsNotIn,
                                words.whitelist, "UNK"))
twitter.text.95 <- unlist(mclapply(twitter.text.preprocessed,
                                   replaceWordsNotIn,
                                   words.whitelist, "UNK"))
```
```{r prune.95, echo=FALSE}
if (!exists("blogs.text.95")) {
  if (!file.exists("cache/blogs.text.95.txt")) {
    blogs.text.95 <- unlist(mclapply(blogs.text.preprocessed,
                                     replaceWordsNotIn,
                                     words.whitelist, "UNK"))
    write_lines(blogs.text.95, "cache/blogs.text.95.txt")
  } else {
    blogs.text.95 <- read_lines("cache/blogs.text.95.txt")
  }
}
if (!exists("news.text.95")) {
  if (!file.exists("cache/news.text.95.txt")) {
    news.text.95 <- unlist(mclapply(news.text.preprocessed,
                                    replaceWordsNotIn,
                                    words.whitelist, "UNK"))
    write_lines(news.text.95, "cache/news.text.95.txt")
  } else {
    news.text.95 <- read_lines("cache/news.text.95.txt")
  }
}
if (!exists("twitter.text.95")) {
  if (!file.exists("cache/twitter.text.95.txt")) {
    twitter.text.95 <- unlist(mclapply(twitter.text.preprocessed,
                                       replaceWordsNotIn,
                                       words.whitelist, "UNK"))
    write_lines(twitter.text.95, "cache/twitter.text.95.txt")
  } else {
    twitter.text.95 <- read_lines("cache/twitter.text.95.txt")
  }
}
```

After pruning seldom words, we re-calculate bigrams. From now on, we will
analyze only the aggregated corpus.

```{r eval=FALSE}
# Calculate frequency of bigrams in pruned source, or load from cache.
all.text.95 <- c(blogs.text.95, news.text.95, twitter.text.95)
all.2gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 2,
                                              stop_words = stopwords.tokens))
```
```{r build2grams.95, echo=FALSE}
  # Calculate frequency of bigrams in pruned source, or load from cache.
  if (file.exists("cache/all.2gram.95.freq.RDS")) {
    all.2gram.95.freq <- readRDS("cache/all.2gram.95.freq.RDS")
  } else {
    all.text.95 <- c(blogs.text.95, news.text.95, twitter.text.95)
    all.2gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 2,
                                                    stop_words = stopwords.tokens))
    rm(all.text.95)
    dummy <- gc(full = TRUE, verbose = FALSE)
    
    saveRDS(all.2gram.95.freq, "cache/all.2gram.95.freq.RDS")
  }
```

The chart shows coverage of corpora by pruned bigrams, where different types
of bigrams are indicated by different color. The chart also shows for several
numbers points where bigrams were encountered a particular number of times.
For example, there are
`r formatC(nrow(all.2gram.95.freq[all.2gram.95.freq$Freq > 30,]), big.mark = ",", format = "d")`
unique bigrams encountered more than 30 times.

```{r cover2grams.95, echo=FALSE, cache=TRUE, cache.lazy=FALSE}
# Use log-scale for sub-sampling the X axis.
all.2gram.95.freq.idx <- unique(as.integer(exp((1:1000) * log(nrow(all.2gram.95.freq)) / 1000)))

# Enrich 2-grams: add columns indicating whether the 1st or 2nd words are
# an UNK token.
# Enrich 2-grams: add cumulative percentage for each token type.
all.2gram.95.freq.enriched <- as_tibble(all.2gram.95.freq) %>%
  dplyr::mutate(UNK_1 = grepl("UNK \\S+", Terms),
         UNK_2 = grepl("\\S+ UNK", Terms),
         Freq.Pct = Freq / sum(Freq),
         WRD_WRD.Freq.Cum.Pct = cumsum(ifelse(!UNK_1 & !UNK_2, Freq.Pct, 0)),
         UNK_WRD.Freq.Cum.Pct = cumsum(ifelse(UNK_1 & !UNK_2, Freq.Pct, 0)),
         WRD_UNK.Freq.Cum.Pct = cumsum(ifelse(!UNK_1 & UNK_2, Freq.Pct, 0)),
         UNK_UNK.Freq.Cum.Pct = cumsum(ifelse(UNK_1 & UNK_2, Freq.Pct, 0)),
         UNK_N = str_count(Terms, "UNK"),
         UNK_0.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 0, Freq.Pct, 0)),
         UNK_1.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 1, Freq.Pct, 0)),
         UNK_2.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 2, Freq.Pct, 0))
  ) %>%
  dplyr::slice(all.2gram.95.freq.idx)

# Clean up memory
dummy <- gc(full = TRUE, verbose = FALSE)

all.2gram.95.freq.pct <- rbind(
  data.frame("Type" = rep("UNK UNK", length(all.2gram.95.freq.idx)),
             "Count" = all.2gram.95.freq.idx,
             "Freq.Pct" = all.2gram.95.freq.enriched$UNK_UNK.Freq.Cum.Pct),
  data.frame("Type" = rep("UNK Word", length(all.2gram.95.freq.idx)),
             "Count" = all.2gram.95.freq.idx,
             "Freq.Pct" = all.2gram.95.freq.enriched$UNK_WRD.Freq.Cum.Pct),
  data.frame("Type" = rep("Word UNK", length(all.2gram.95.freq.idx)),
             "Count" = all.2gram.95.freq.idx,
             "Freq.Pct" = all.2gram.95.freq.enriched$WRD_UNK.Freq.Cum.Pct),
  data.frame("Type" = rep("Word Word", length(all.2gram.95.freq.idx)),
             "Count" = all.2gram.95.freq.idx,
             "Freq.Pct" = all.2gram.95.freq.enriched$WRD_WRD.Freq.Cum.Pct)
)

# How many are there unique bigrams which are encountered more than N times?
all.2gram.95.freq.30 <- nrow(all.2gram.95.freq[all.2gram.95.freq$Freq > 30,])
all.2gram.95.freq.20 <- nrow(all.2gram.95.freq[all.2gram.95.freq$Freq > 20,])
all.2gram.95.freq.10 <- nrow(all.2gram.95.freq[all.2gram.95.freq$Freq > 10,])
all.2gram.95.freq.5 <- nrow(all.2gram.95.freq[all.2gram.95.freq$Freq > 5,])
all.2gram.95.freq.1 <- nrow(all.2gram.95.freq[all.2gram.95.freq$Freq > 1,])
```

```{r cover2grams.95.g, echo=FALSE, cache=TRUE, cache.lazy=FALSE, dependson='cover2grams.95', fig.align='center', fig.width=10}
ggplot(all.2gram.95.freq.pct,
             aes(x = Count, y = Freq.Pct, fill = Type)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  geom_area() + 
  scale_fill_brewer(palette="Greens", breaks=rev(levels(all.2gram.95.freq.pct$Type))) +
  geom_segment(aes(x = all.2gram.95.freq.30, y = 0, xend = all.2gram.95.freq.30, yend = 0.5)) +
  geom_segment(aes(x = all.2gram.95.freq.20, y = 0, xend = all.2gram.95.freq.20, yend = 0.6)) +
  geom_segment(aes(x = all.2gram.95.freq.10, y = 0, xend = all.2gram.95.freq.10, yend = 0.7)) +
  geom_segment(aes(x = all.2gram.95.freq.5, y = 0, xend = all.2gram.95.freq.5, yend = 0.8)) +
  geom_segment(aes(x = all.2gram.95.freq.1, y = 0, xend = all.2gram.95.freq.1, yend = 0.9)) +
  annotate(geom="text", x=all.2gram.95.freq.30, y=0.52, label="30")+
  annotate(geom="text", x=all.2gram.95.freq.20, y=0.62, label="20")+
  annotate(geom="text", x=all.2gram.95.freq.10, y=0.72, label="10")+
  annotate(geom="text", x=all.2gram.95.freq.5, y=0.82, label="5")+
  annotate(geom="text", x=all.2gram.95.freq.1, y=0.92, label="1")+
  labs(title = "Cumulative Frequency of Bigrams") +
  labs(x = "Unique bigrams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14) 
```

By pruning we have reduced a number of unique bigrams from
`r formatC(nrow(all.2gram.freq), big.mark = ",", format = "d")` to
`r formatC(nrow(all.2gram.95.freq), big.mark = ",", format = "d")`,
that is by `r sprintf("%.2f%%", 100 * (1 - nrow(all.2gram.95.freq) / nrow(all.2gram.freq)))`.
On this stage it is hard to tell whether pruning makes sense: on one hand, it
reduces the number of unique 2-grams and thus memory requirements of our
application, on the other hand it removes information which may be required
to achieve good prediction rate.

# <a name="3_to_6_grams"></a>3-grams to 6-grams

After analyzing bigrams, now there is a time to take a look on a longer n-grams.
We decided to analyze 3-grams to 6-grams.

```{r eval=FALSE}
# Calculate frequency of 3- to 6-grams in pruned source.
all.3gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 3,
                                              stop_words = stopwords.tokens))
all.4gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 4,
                                              stop_words = stopwords.tokens))
all.5gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 5,
                                              stop_words = stopwords.tokens))
all.6gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 6,
                                              stop_words = stopwords.tokens))
```
```{r build3456grams.95, echo=FALSE}
  # Calculate frequency of 3- to 6-grams in pruned source, or load from cache.
  if (file.exists("cache/all.3gram.95.freq.RDS")) {
    all.3gram.95.freq <- readRDS("cache/all.3gram.95.freq.RDS")
  } else {
    all.text.95 <- c(blogs.text.95, news.text.95, twitter.text.95)
    all.3gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 3,
                                                    stop_words = stopwords.tokens))
    rm(all.text.95)
    dummy <- gc(full = TRUE, verbose = FALSE)
    
    saveRDS(all.3gram.95.freq, "cache/all.3gram.95.freq.RDS")
  }
  if (file.exists("cache/all.4gram.95.freq.RDS")) {
    all.4gram.95.freq <- readRDS("cache/all.4gram.95.freq.RDS")
  } else {
    all.text.95 <- c(blogs.text.95, news.text.95, twitter.text.95)
    all.4gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 4,
                                                    stop_words = stopwords.tokens))
    rm(all.text.95)
    dummy <- gc(full = TRUE, verbose = FALSE)
    
    saveRDS(all.4gram.95.freq, "cache/all.4gram.95.freq.RDS")
  }
  if (file.exists("cache/all.5gram.95.freq.RDS")) {
    all.5gram.95.freq <- readRDS("cache/all.5gram.95.freq.RDS")
  } else {
    all.text.95 <- c(blogs.text.95, news.text.95, twitter.text.95)
    all.5gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 5,
                                                    stop_words = stopwords.tokens))
    rm(all.text.95)
    dummy <- gc(full = TRUE, verbose = FALSE)

    saveRDS(all.5gram.95.freq, "cache/all.5gram.95.freq.RDS")
  }
  if (file.exists("cache/all.6gram.95.freq.RDS")) {
    all.6gram.95.freq <- readRDS("cache/all.6gram.95.freq.RDS")
  } else {
    all.text.95 <- c(blogs.text.95, news.text.95, twitter.text.95)
    all.6gram.95.freq <- enrich.ngram(build.ngram(all.text.95, 6,
                                                    stop_words = stopwords.tokens))
    rm(all.text.95)
    dummy <- gc(full = TRUE, verbose = FALSE)
    
    saveRDS(all.6gram.95.freq, "cache/all.6gram.95.freq.RDS")
  }
```

```{r cover3grams.95, echo=FALSE, cache=TRUE, cache.lazy=FALSE}
# Use log-scale for sub-sampling the X axis.
all.3gram.95.freq.idx <- unique(as.integer(exp((1:250) * log(nrow(all.3gram.95.freq)) / 250)))

# Enrich 3-grams: add a column with a count of UNK token.
# Enrich 3-grams: add cumulative percentage for each token type.
all.3gram.95.freq.enriched <- as_tibble(all.3gram.95.freq) %>%
  dplyr::mutate(UNK_N = str_count(Terms, "UNK"),
                Freq.Pct = Freq / sum(Freq),
                UNK_0.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 0, Freq.Pct, 0)),
                UNK_1.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 1, Freq.Pct, 0)),
                UNK_2.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 2, Freq.Pct, 0)),
                UNK_3.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 3, Freq.Pct, 0))
  ) %>%
  dplyr::slice(all.3gram.95.freq.idx)

# Clean up memory
dummy <- gc(full = TRUE, verbose = FALSE)

all.3gram.95.freq.pct <- rbind(
  data.frame("Type" = rep("3 UNK", length(all.3gram.95.freq.idx)),
             "Count" = all.3gram.95.freq.idx,
             "Freq.Pct" = all.3gram.95.freq.enriched$UNK_3.Freq.Cum.Pct),
  data.frame("Type" = rep("2 UNK", length(all.3gram.95.freq.idx)),
             "Count" = all.3gram.95.freq.idx,
             "Freq.Pct" = all.3gram.95.freq.enriched$UNK_2.Freq.Cum.Pct),
  data.frame("Type" = rep("1 UNK", length(all.3gram.95.freq.idx)),
             "Count" = all.3gram.95.freq.idx,
             "Freq.Pct" = all.3gram.95.freq.enriched$UNK_1.Freq.Cum.Pct),
  data.frame("Type" = rep("0 UNK", length(all.3gram.95.freq.idx)),
             "Count" = all.3gram.95.freq.idx,
             "Freq.Pct" = all.3gram.95.freq.enriched$UNK_0.Freq.Cum.Pct)
)

# How many are there unique 3-grams which are encountered more than N times?
all.3gram.95.freq.30 <- nrow(all.3gram.95.freq[all.3gram.95.freq$Freq > 30,])
all.3gram.95.freq.20 <- nrow(all.3gram.95.freq[all.3gram.95.freq$Freq > 20,])
all.3gram.95.freq.10 <- nrow(all.3gram.95.freq[all.3gram.95.freq$Freq > 10,])
all.3gram.95.freq.5 <- nrow(all.3gram.95.freq[all.3gram.95.freq$Freq > 5,])
all.3gram.95.freq.1 <- nrow(all.3gram.95.freq[all.3gram.95.freq$Freq > 1,])

# Clean up memory
rm(all.3gram.95.freq.enriched)
dummy <- gc(full = TRUE, verbose = FALSE)
```
```{r cover3grams.95.g, echo=FALSE, cache=TRUE, cache.lazy=FALSE, dependson='cover3grams.95', fig.align='center', fig.width=10}
ggplot(all.3gram.95.freq.pct,
       aes(x = Count, y = Freq.Pct, fill = Type)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  geom_area() + 
  scale_fill_brewer(palette="Greens", breaks=rev(levels(all.3gram.95.freq.pct$Type))) +
  geom_segment(aes(x = all.3gram.95.freq.30, y = 0, xend = all.3gram.95.freq.30, yend = 0.5)) +
  geom_segment(aes(x = all.3gram.95.freq.20, y = 0, xend = all.3gram.95.freq.20, yend = 0.6)) +
  geom_segment(aes(x = all.3gram.95.freq.10, y = 0, xend = all.3gram.95.freq.10, yend = 0.7)) +
  geom_segment(aes(x = all.3gram.95.freq.5, y = 0, xend = all.3gram.95.freq.5, yend = 0.8)) +
  geom_segment(aes(x = all.3gram.95.freq.1, y = 0, xend = all.3gram.95.freq.1, yend = 0.9)) +
  annotate(geom="text", x=all.3gram.95.freq.30, y=0.52, label="30")+
  annotate(geom="text", x=all.3gram.95.freq.20, y=0.62, label="20")+
  annotate(geom="text", x=all.3gram.95.freq.10, y=0.72, label="10")+
  annotate(geom="text", x=all.3gram.95.freq.5, y=0.82, label="5")+
  annotate(geom="text", x=all.3gram.95.freq.1, y=0.92, label="1")+
  labs(title = "Cumulative Frequency of 3-grams") +
  labs(x = "Unique 3-grams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14) 
```

```{r cover4grams.95, echo=FALSE, cache=TRUE, cache.lazy=FALSE}
# Use log-scale for sub-sampling the X axis.
all.4gram.95.freq.idx <- unique(as.integer(exp((1:250) * log(nrow(all.4gram.95.freq)) / 250)))

# Enrich 4-grams: add a column with a count of UNK token.
# Enrich 4-grams: add cumulative percentage for each token type.
all.4gram.95.freq.enriched <- as_tibble(all.4gram.95.freq) %>%
  dplyr::mutate(UNK_N = str_count(Terms, "UNK"),
                Freq.Pct = Freq / sum(Freq),
                UNK_0.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 0, Freq.Pct, 0)),
                UNK_1.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 1, Freq.Pct, 0)),
                UNK_2.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 2, Freq.Pct, 0)),
                UNK_3.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 3, Freq.Pct, 0)),
                UNK_4.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 4, Freq.Pct, 0))
  ) %>%
  dplyr::slice(all.4gram.95.freq.idx)

# Clean up memory
dummy <- gc(full = TRUE, verbose = FALSE)

all.4gram.95.freq.pct <- rbind(
  data.frame("Type" = rep("4 UNK", length(all.4gram.95.freq.idx)),
             "Count" = all.4gram.95.freq.idx,
             "Freq.Pct" = all.4gram.95.freq.enriched$UNK_4.Freq.Cum.Pct),
  data.frame("Type" = rep("3 UNK", length(all.4gram.95.freq.idx)),
             "Count" = all.4gram.95.freq.idx,
             "Freq.Pct" = all.4gram.95.freq.enriched$UNK_3.Freq.Cum.Pct),
  data.frame("Type" = rep("2 UNK", length(all.4gram.95.freq.idx)),
             "Count" = all.4gram.95.freq.idx,
             "Freq.Pct" = all.4gram.95.freq.enriched$UNK_2.Freq.Cum.Pct),
  data.frame("Type" = rep("1 UNK", length(all.4gram.95.freq.idx)),
             "Count" = all.4gram.95.freq.idx,
             "Freq.Pct" = all.4gram.95.freq.enriched$UNK_1.Freq.Cum.Pct),
  data.frame("Type" = rep("0 UNK", length(all.4gram.95.freq.idx)),
             "Count" = all.4gram.95.freq.idx,
             "Freq.Pct" = all.4gram.95.freq.enriched$UNK_0.Freq.Cum.Pct)
)

# How many are there unique 4-grams which are encountered more than N times?
all.4gram.95.freq.30 <- nrow(all.4gram.95.freq[all.4gram.95.freq$Freq > 30,])
all.4gram.95.freq.20 <- nrow(all.4gram.95.freq[all.4gram.95.freq$Freq > 20,])
all.4gram.95.freq.10 <- nrow(all.4gram.95.freq[all.4gram.95.freq$Freq > 10,])
all.4gram.95.freq.5 <- nrow(all.4gram.95.freq[all.4gram.95.freq$Freq > 5,])
all.4gram.95.freq.1 <- nrow(all.4gram.95.freq[all.4gram.95.freq$Freq > 1,])
```

```{r cover4grams.95.g, echo=FALSE, cache=TRUE, cache.lazy=FALSE, dependson='cover4grams.95', fig.align='center', fig.width=10}
ggplot(all.4gram.95.freq.pct,
       aes(x = Count, y = Freq.Pct, fill = Type)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  geom_area() +
  scale_fill_brewer(palette="Greens", breaks=rev(levels(all.4gram.95.freq.pct$Type))) +
  geom_segment(aes(x = all.4gram.95.freq.30, y = 0, xend = all.4gram.95.freq.30, yend = 0.5)) +
  geom_segment(aes(x = all.4gram.95.freq.20, y = 0, xend = all.4gram.95.freq.20, yend = 0.6)) +
  geom_segment(aes(x = all.4gram.95.freq.10, y = 0, xend = all.4gram.95.freq.10, yend = 0.7)) +
  geom_segment(aes(x = all.4gram.95.freq.5, y = 0, xend = all.4gram.95.freq.5, yend = 0.8)) +
  geom_segment(aes(x = all.4gram.95.freq.1, y = 0, xend = all.4gram.95.freq.1, yend = 0.9)) +
  annotate(geom="text", x=all.4gram.95.freq.30, y=0.52, label="30")+
  annotate(geom="text", x=all.4gram.95.freq.20, y=0.62, label="20")+
  annotate(geom="text", x=all.4gram.95.freq.10, y=0.72, label="10")+
  annotate(geom="text", x=all.4gram.95.freq.5, y=0.82, label="5")+
  annotate(geom="text", x=all.4gram.95.freq.1, y=0.92, label="1")+
  labs(title = "Cumulative Frequency of 4-grams") +
  labs(x = "Unique 4-grams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

```{r cover5grams.95, echo=FALSE, cache=TRUE, cache.lazy=FALSE}
# Use log-scale for sub-sampling the X axis.
all.5gram.95.freq.idx <- unique(as.integer(exp((1:250) * log(nrow(all.5gram.95.freq)) / 250)))

# Enrich 5-grams: add a column with a count of UNK token.
# Enrich 5-grams: add cumulative percentage for each token type.
all.5gram.95.freq.enriched <- as_tibble(all.5gram.95.freq) %>%
  dplyr::mutate(UNK_N = str_count(Terms, "UNK"),
                Freq.Pct = Freq / sum(Freq),
                UNK_0.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 0, Freq.Pct, 0)),
                UNK_1.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 1, Freq.Pct, 0)),
                UNK_2.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 2, Freq.Pct, 0)),
                UNK_3.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 3, Freq.Pct, 0)),
                UNK_4.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 4, Freq.Pct, 0)),
                UNK_5.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 5, Freq.Pct, 0))
  ) %>%
  dplyr::slice(all.5gram.95.freq.idx)

# Clean up memory
dummy <- gc(full = TRUE, verbose = FALSE)

all.5gram.95.freq.pct <- rbind(
  data.frame("Type" = rep("5 UNK", length(all.5gram.95.freq.idx)),
             "Count" = all.5gram.95.freq.idx,
             "Freq.Pct" = all.5gram.95.freq.enriched$UNK_5.Freq.Cum.Pct),
  data.frame("Type" = rep("4 UNK", length(all.5gram.95.freq.idx)),
             "Count" = all.5gram.95.freq.idx,
             "Freq.Pct" = all.5gram.95.freq.enriched$UNK_4.Freq.Cum.Pct),
  data.frame("Type" = rep("3 UNK", length(all.5gram.95.freq.idx)),
             "Count" = all.5gram.95.freq.idx,
             "Freq.Pct" = all.5gram.95.freq.enriched$UNK_3.Freq.Cum.Pct),
  data.frame("Type" = rep("2 UNK", length(all.5gram.95.freq.idx)),
             "Count" = all.5gram.95.freq.idx,
             "Freq.Pct" = all.5gram.95.freq.enriched$UNK_2.Freq.Cum.Pct),
  data.frame("Type" = rep("1 UNK", length(all.5gram.95.freq.idx)),
             "Count" = all.5gram.95.freq.idx,
             "Freq.Pct" = all.5gram.95.freq.enriched$UNK_1.Freq.Cum.Pct),
  data.frame("Type" = rep("0 UNK", length(all.5gram.95.freq.idx)),
             "Count" = all.5gram.95.freq.idx,
             "Freq.Pct" = all.5gram.95.freq.enriched$UNK_0.Freq.Cum.Pct)
)

# How many are there unique 5-grams which are encountered more than N times?
all.5gram.95.freq.30 <- nrow(all.5gram.95.freq[all.5gram.95.freq$Freq > 30,])
all.5gram.95.freq.20 <- nrow(all.5gram.95.freq[all.5gram.95.freq$Freq > 20,])
all.5gram.95.freq.10 <- nrow(all.5gram.95.freq[all.5gram.95.freq$Freq > 10,])
all.5gram.95.freq.5 <- nrow(all.5gram.95.freq[all.5gram.95.freq$Freq > 5,])
all.5gram.95.freq.1 <- nrow(all.5gram.95.freq[all.5gram.95.freq$Freq > 1,])
```

```{r cover5grams.95.g, echo=FALSE, cache=TRUE, cache.lazy=FALSE, dependson='cover5grams.95', fig.align='center', fig.width=10}
ggplot(all.5gram.95.freq.pct,
       aes(x = Count, y = Freq.Pct, fill = Type)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  geom_area() +
  scale_fill_brewer(palette="Greens", breaks=rev(levels(all.5gram.95.freq.pct$Type))) +
  geom_segment(aes(x = all.5gram.95.freq.30, y = 0, xend = all.5gram.95.freq.30, yend = 0.5)) +
  geom_segment(aes(x = all.5gram.95.freq.20, y = 0, xend = all.5gram.95.freq.20, yend = 0.6)) +
  geom_segment(aes(x = all.5gram.95.freq.10, y = 0, xend = all.5gram.95.freq.10, yend = 0.7)) +
  geom_segment(aes(x = all.5gram.95.freq.5, y = 0, xend = all.5gram.95.freq.5, yend = 0.8)) +
  geom_segment(aes(x = all.5gram.95.freq.1, y = 0, xend = all.5gram.95.freq.1, yend = 0.9)) +
  annotate(geom="text", x=all.5gram.95.freq.30, y=0.52, label="30")+
  annotate(geom="text", x=all.5gram.95.freq.20, y=0.62, label="20")+
  annotate(geom="text", x=all.5gram.95.freq.10, y=0.72, label="10")+
  annotate(geom="text", x=all.5gram.95.freq.5, y=0.82, label="5")+
  annotate(geom="text", x=all.5gram.95.freq.1, y=0.92, label="1")+
  labs(title = "Cumulative Frequency of 5-grams") +
  labs(x = "Unique 5-grams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

```{r cover6grams.95, echo=FALSE, cache=TRUE, cache.lazy=FALSE}
# Use log-scale for sub-sampling the X axis.
all.6gram.95.freq.idx <- unique(as.integer(exp((1:250) * log(nrow(all.6gram.95.freq)) / 250)))

# Enrich 6-grams: add a column with a count of UNK token.
# Enrich 6-grams: add cumulative percentage for each token type.
all.6gram.95.freq.enriched <- as_tibble(all.6gram.95.freq) %>%
  dplyr::mutate(UNK_N = str_count(Terms, "UNK"),
                Freq.Pct = Freq / sum(Freq),
                UNK_0.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 0, Freq.Pct, 0)),
                UNK_1.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 1, Freq.Pct, 0)),
                UNK_2.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 2, Freq.Pct, 0)),
                UNK_3.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 3, Freq.Pct, 0)),
                UNK_4.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 4, Freq.Pct, 0)),
                UNK_5.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 5, Freq.Pct, 0)),
                UNK_6.Freq.Cum.Pct = cumsum(ifelse(UNK_N == 6, Freq.Pct, 0))
  ) %>%
  dplyr::slice(all.6gram.95.freq.idx)

# Clean up memory
dummy <- gc(full = TRUE, verbose = FALSE)

all.6gram.95.freq.pct <- rbind(
  data.frame("Type" = rep("6 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_6.Freq.Cum.Pct),
  data.frame("Type" = rep("5 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_5.Freq.Cum.Pct),
  data.frame("Type" = rep("4 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_4.Freq.Cum.Pct),
  data.frame("Type" = rep("3 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_3.Freq.Cum.Pct),
  data.frame("Type" = rep("2 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_2.Freq.Cum.Pct),
  data.frame("Type" = rep("1 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_1.Freq.Cum.Pct),
  data.frame("Type" = rep("0 UNK", length(all.6gram.95.freq.idx)),
             "Count" = all.6gram.95.freq.idx,
             "Freq.Pct" = all.6gram.95.freq.enriched$UNK_0.Freq.Cum.Pct)
)

# How many are there unique 6-grams which are encountered more than N times?
all.6gram.95.freq.30 <- nrow(all.6gram.95.freq[all.6gram.95.freq$Freq > 30,])
all.6gram.95.freq.20 <- nrow(all.6gram.95.freq[all.6gram.95.freq$Freq > 20,])
all.6gram.95.freq.10 <- nrow(all.6gram.95.freq[all.6gram.95.freq$Freq > 10,])
all.6gram.95.freq.5 <- nrow(all.6gram.95.freq[all.6gram.95.freq$Freq > 5,])
all.6gram.95.freq.1 <- nrow(all.6gram.95.freq[all.6gram.95.freq$Freq > 1,])
```

```{r cover6grams.95.g, echo=FALSE, cache=TRUE, cache.lazy=FALSE, dependson='cover6grams.95', fig.align='center', fig.width=10}
ggplot(all.6gram.95.freq.pct,
             aes(x = Count, y = Freq.Pct, fill = Type)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  geom_area() +
  scale_fill_brewer(palette="Greens", breaks=rev(levels(all.6gram.95.freq.pct$Type))) +
  geom_segment(aes(x = all.6gram.95.freq.30, y = 0, xend = all.6gram.95.freq.30, yend = 0.5)) +
  geom_segment(aes(x = all.6gram.95.freq.20, y = 0, xend = all.6gram.95.freq.20, yend = 0.6)) +
  geom_segment(aes(x = all.6gram.95.freq.10, y = 0, xend = all.6gram.95.freq.10, yend = 0.7)) +
  geom_segment(aes(x = all.6gram.95.freq.5, y = 0, xend = all.6gram.95.freq.5, yend = 0.8)) +
  geom_segment(aes(x = all.6gram.95.freq.1, y = 0, xend = all.6gram.95.freq.1, yend = 0.9)) +
  annotate(geom="text", x=all.6gram.95.freq.30, y=0.52, label="30")+
  annotate(geom="text", x=all.6gram.95.freq.20, y=0.62, label="20")+
  annotate(geom="text", x=all.6gram.95.freq.10, y=0.72, label="10")+
  annotate(geom="text", x=all.6gram.95.freq.5, y=0.82, label="5")+
  annotate(geom="text", x=all.6gram.95.freq.1, y=0.92, label="1")+
  labs(title = "Cumulative Frequency of 6-grams") +
  labs(x = "Unique 6-grams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

