---
title: "Data Science Capstone Project: Milestone Report"
author: "Alexey Serdyuk"
date: "16/06/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of content

* [Synopsis](#synopsis)
* [Prerequisites](#prerequisites)
* [Obtaining the data](#obtaining_data)
* [Splitting the data](#splitting_data)
* [First glance on the data and general plan](#first_glance)
* [Cleaning up and preprocessing the corpus](#cleaning_preprocessing)
* [Analyzing words (1-grams)](#analyze_1_grams)
* [Analyzing bigrams](#analyze_2_grams)

# <a name="synopsis"></a>Synopsis

This is a milestone report for Week 2 of the capstone project for the cycle of courses
[Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)
offered on [Coursera](https://www.coursera.org) by
[Johns Hopkins University](https://www.jhu.edu).

The purpose of the capstone project is to build a Natural Language Processing
(NLP) application, that, given a chunk of text, predicts the next most probable
word. The application may be used, for example, in mobile devided to provide
suggestions as the user tips in some text.

In this report we will provide initial analysis of the data, as well as discuss
approach to building the application.

# <a name="prerequisites"></a>Prerequisites

An important question is which library to use for processing and analyzing the
corpora, as R provides several alternatives. Initially we attempted to use
the library `tm`, but quickly found that the library is very memory-hungry,
and an attempt to build bi- or trigrams for a large corpus are not practical.
After some googling we decided to use the library `quanteda` instead.

Too keep our namespace relatively clean, we load only those libraries from
which we use many functions, or if we use the same function many times.
Otherwise we will use full syntax, for example `stopwords::stopwords()`.

We start by loading required libraries.

```{r message=FALSE}
library(data.table) # For fast access in data tables.
library(ggplot2) # For plotting charts.
library(ggforce) # For plotting charts.
library(grid) # For arranging charts in a grid.
library(gridExtra) # For arranging charts in a grid.
library(kableExtra) # For pretty-printing tables.
library(parallel) # For parallel processing.
library(quanteda) # For handling the corpora.
library(readr) # For fast reading/writing.
library(R.utils) # For counting lines in files.
``` 
    
To speed up processing of large data sets, we will apply parallel version of
`lapply` function from the library `parallel`. To use all the available
resources, we detect a number of CPU cores and configure the library to use
them all.

```{r}
cpu.cores <- detectCores()
options(mc.cores = cpu.cores)
``` 

# <a name="obtaining_data"></a>Obtaining the data

Here and many times later we use caching to speed up rendering of this document.
Results of long-running operations are stored, and used again during the next
run. If you wish to re-run all operations, just remove the `cache` directory.

```{r}
if (!dir.exists("cache")) {
    dir.create("cache")
}
``` 

We download the data from the URL provided in the course description, and unzip
it.

```{r}
if (!file.exists("cache/Coursera-SwiftKey.zip")) {
    download.file(url = "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
                  destfile = "cache/Coursera-SwiftKey.zip", method = "curl")
    unzip("cache/Coursera-SwiftKey.zip", exdir = "cache")
}
```

# <a name="splitting_data"></a>Splitting the data

The downloaded zip file contains corpora in several languages: English, German,
Russian and Finnish. In our project we will use only English corpora.

Corpora in each language, including English, contains 3 files with content
obtained from different sources: news, blogs and twitter.

As the first step, we will split each relevant file on 3 parts:

* Training set (60%) will be used to build and train the algorithm.
* Testing set (20%) will be used to test the algorithm during it's development.
This set may be used more than once.
* Validation set (20%) will be used for a final validation and estimation
of out-of-sample performance. This set will be used only once.

We define a function which splits the specified file on parts described above:

```{r}
# Arguments:
# name - the file to split
# out.dir - output directory
splitFile <- function(name, out.dir) {
    # Reading dataset from the input file.
    data <- read_lines(name)

    # Prepare list with indexes of all data items.
    data.index <- 1:length(data)

    # Sample indices for the training data set, and create a set with remaining
    # indices.
    training.index <- sample(data.index, 0.6 * length(data.index))
    remaining.index <- data.index[! data.index %in% training.index]

    # Sample indices for the testing data set, and use remaining indices
    # for a validation data set.
    testing.index <- sample(remaining.index, 0.5 * length(remaining.index))
    validation.index <- remaining.index[! remaining.index %in% testing.index]

    # Split the data.
    data.training <- data[training.index]
    data.testing <- data[testing.index]
    data.validation <- data[validation.index]
    
    # Create an output directory, if it does not exist.
    if (!dir.exists(out.dir)) {
        dir.create(out.dir)
    }
    
    # Prepare names for output files. We append suffixes "training", "testing"
    # and "validation" to the input file name before the extension.
    base <- basename(name)
    outTraining <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.training.txt", base))
    outTesting <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.testing.txt", base))
    outValidation <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.validation.txt", base))

    # Writing datasets to output files.
    write_lines(data.training, outTraining)
    write_lines(data.testing, outTesting)
    write_lines(data.validation, outValidation)
}
```

To make results reproduceable, we set the seed of the random number generator.

```{r}
set.seed(20190530)
```

Finally, we split each of the data files.

```{r}
if (!file.exists("cache/en_US.blogs.training.txt")) {
    splitFile("cache/final/en_US/en_US.blogs.txt", "cache")
}
if (!file.exists("cache/en_US.news.training.txt")) {
    splitFile("cache/final/en_US/en_US.news.txt", "cache")
}
if (!file.exists("cache/en_US.twitter.training.txt")) {
    splitFile("cache/final/en_US/en_US.twitter.txt", "cache")
}
```

As a sanity check, we count a number of lines in each source file, as well in
the partial files produced by the split.

```{r countLines, message=FALSE, cache=TRUE}
count.blogs <- R.utils::countLines("cache/final/en_US/en_US.blogs.txt")
count.blogs.training <- R.utils::countLines("cache/en_US.blogs.training.txt")
count.blogs.testing <- R.utils::countLines("cache/en_US.blogs.testing.txt")
count.blogs.validation <- R.utils::countLines("cache/en_US.blogs.validation.txt")

count.news <- R.utils::countLines("cache/final/en_US/en_US.news.txt")
count.news.training <- R.utils::countLines("cache/en_US.news.training.txt")
count.news.testing <- R.utils::countLines("cache/en_US.news.testing.txt")
count.news.validation <- R.utils::countLines("cache/en_US.news.validation.txt")

count.twitter <- R.utils::countLines("cache/final/en_US/en_US.twitter.txt")
count.twitter.training <- R.utils::countLines("cache/en_US.twitter.training.txt")
count.twitter.testing <- R.utils::countLines("cache/en_US.twitter.testing.txt")
count.twitter.validation <- R.utils::countLines("cache/en_US.twitter.validation.txt")
```

```{r showCountLines, echo=FALSE, cache=TRUE, dependson=countLines}
corpora.count <- data.frame(
  "blogs" = c(count.blogs.training,
              count.blogs.testing,
              count.blogs.validation,
              count.blogs,
              count.blogs - (count.blogs.training +
                             count.blogs.testing +
                             count.blogs.validation)),
  "blogs.pct" = c(count.blogs.training / count.blogs * 100,
                  count.blogs.testing / count.blogs * 100,
                  count.blogs.validation / count.blogs * 100,
                  count.blogs / count.blogs * 100,
                  NA),
  "news" = c(count.news.training,
             count.news.testing,
             count.news.validation,
             count.news,
             count.news - (count.news.training +
                           count.news.testing +
                           count.news.validation)),
  "news.pct" = c(count.news.training / count.news * 100,
                 count.news.testing / count.news * 100,
                 count.news.validation / count.news * 100,
                 count.news / count.news * 100,
                 NA),
  "twitter" = c(count.twitter.training,
                count.twitter.testing,
                count.twitter.validation,
                count.twitter,
                count.twitter - (count.twitter.training +
                                 count.twitter.testing +
                                 count.twitter.validation)),
  "twitter.pct" = c(count.twitter.training / count.twitter * 100,
                    count.twitter.testing / count.twitter * 100,
                    count.twitter.validation / count.twitter * 100,
                    count.twitter / count.twitter * 100,
                    NA)
)
rownames(corpora.count) <- c("Training", "Testing", "Validation", "Total",
                             "Control (expected to be 0)")
kable(corpora.count,
      booktabs = TRUE,
      col.names = c("Rows", "%", "Rows", "%", "Rows", "%")) %>%
    add_header_above(header = c("",
                                "Blogs" = 2,
                                "News" = 2,
                                "Twitter" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

As the table shows, we have splitted the data on sub-sets as intended.

# <a name="first_glance"></a>First glance on the data and general plan

In the section above we have already counted a number of lines. Let us load
training data sets and take a look on the first 3 lines of each data set.

```{r}
blogs.text <- read_lines("cache/en_US.blogs.training.txt")
news.text <- read_lines("cache/en_US.news.training.txt")
twitter.text <- read_lines("cache/en_US.twitter.training.txt")
```

```{r}
head(blogs.text, 3)
head(news.text, 3)
head(twitter.text, 3)
```

we could see that the data contains not only words, but also numbers and
punctuation. The punctuation may be non-ASCII (Unicode), as the first example
in the blogs sample shows (it contains a character "…", which is different from
3 ASCII point characters ". . ."). Some lines may contain multiple sentences,
and probably we have to take this into account.

Here is our plan:

* Split text on sentences.
* Clean up the corpus: remove non-language parts such as e-mail addresses and
URLs, etc.
* Preprocess the corpus: remove punctuation and numbers, change all words
to lower-case.
* Analyze distribution of words to decide if we should base our prediction on
the full dictionary, or just on some sub-set of it.
* Analyze n-grams for small n.

# <a name="cleaning_preprocessing"></a>Cleaning up and preprocessing the corpus

We decided to split text on sentences and do not attempt to predict words across
sentence border. We still may use information about sentences to improve
prediction of the first word, because the frequency of the first word in a
sentence may be very different from an average frequency.

```{r splitOnSentences, cache=TRUE}
blogs.text <- unlist(tokenizers::tokenize_sentences(blogs.text))
news.text <- unlist(tokenizers::tokenize_sentences(news.text))
twitter.text <- unlist(tokenizers::tokenize_sentences(twitter.text))
```

Libraries contains some functions for cleaning up and pre-processing, but
for some steps we have to write functions ourselves.

```{r}
# Remove URLs. The regular expression detects http(s) and ftp(s) protocols.
removeUrl <- function(x) gsub("(ht|f)tp(s?)://\\S+", "", x)

# Remove e-mail addresses.
# The regular expression from Stack Overflow:
# https://stackoverflow.com/questions/201323/how-to-validate-an-email-address-using-a-regular-expression
removeEmail <- function(x) gsub("(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])", "", x, perl = TRUE)

# Remove hash tags (the character # and the following word) and twitter handles
# (the character @ and the following word).
removeTagsAndHandles <- function(x) gsub("[@#]\\S+", "", x)

# Surround punctuation marks which does not appear inside a word with space
# characters. Without this step, fragments with a missing space are transformed
# to a single non-existing word when punctuation is removed.
# Example: corpus contains
# "I had the best day yesterday,it was like two kids in a candy store"
# Without this step, "yesterday,it" is transformed to a non-existing word
# "yesterdayit" when removing punctuation. This step transforms it to
# "yesterday, it"
addMissingSpace <- function(x) gsub("[,()\":;”…]", " ", x)

# Replace words in a sentence with replacements available from the table.
# Keep words which are not in the replacement table "as is".
# As a side effect, removes punctuation and transforms to a lower case.
#
# This step is required for several purposes:
# * Replace common short forms with full forms, for example "he'll" = "he will"
replacements.text <- readr::read_csv("replacements.txt",
    col_names = c("token", "replacement"),
    col_types = list(col_character(), col_character()))

replaceWords <- function(text, replacements) {
  # Split text on words.
  tokens.orig <- tokenizers::tokenize_words(text, simplify = TRUE,
                                            strip_numeric = TRUE)
  
  # Attempt to replace each word.
  tokens.replaced <- sapply(tokens.orig, function(x) {
    # Search if a replacement exist.
    replacement.index <- match(x, replacements$token)
    if (is.na(replacement.index)) {
      # Can't find a replacement, fall back on the token itself.
      return (x)
    } else {
      # Replace the token.
      return (replacements$replacement[replacement.index])
    }
  }, USE.NAMES = FALSE)
  
  paste(tokens.replaced, collapse = " ")
}

# Add tokens representing start and end of a sentence.
# SOS = Start Of Sentence
# EOS = End Of Sentence
# When we add these tokens, our text was already transformed to a lower case,
# so we could easy distinguish upper case special tokens from lower case text.
addSentenceTokens <- function(x) paste("SOS", x, "EOS")

# Collapse space characters: if there are more than 1 space character in a row,
# replace with a single one.
collapseWhitespace <- function(x) gsub("\\s{2,}", " ", x)

# ... And now combine all functions in a pre-processing chain.
preProcessText <- function(x) {
    text <- removeUrl(x)
    text <- removeEmail(text)
    text <- removeTagsAndHandles(text)
    text <- addMissingSpace(text)
    text <- replaceWords(text, replacements.text)
    text <- addSentenceTokens(text)
    text <- collapseWhitespace(text)
}
```

Now we pre-process the data and cache results.

```{r}
if (!file.exists("cache/blogs.text.preprocessed.txt")) {
  blogs.text.preprocessed <- unlist(mclapply(blogs.text, preProcessText))
  write_lines(blogs.text.preprocessed, "cache/blogs.text.preprocessed.txt")
} else {
  blogs.text.preprocessed <- read_lines("cache/blogs.text.preprocessed.txt")
}
if (!file.exists("cache/news.text.preprocessed.txt")) {
  news.text.preprocessed <- unlist(mclapply(news.text, preProcessText))
  write_lines(news.text.preprocessed, "cache/news.text.preprocessed.txt")
} else {
  news.text.preprocessed <- read_lines("cache/news.text.preprocessed.txt")
}
if (!file.exists("cache/twitter.text.preprocessed.txt")) {
  twitter.text.preprocessed <- unlist(mclapply(twitter.text, preProcessText))
  write_lines(twitter.text.preprocessed, "cache/twitter.text.preprocessed.txt")
} else {
  twitter.text.preprocessed <- read_lines("cache/twitter.text.preprocessed.txt")
}
```

# <a name="analyze_1_grams"></a>Analyzing words (1-grams)

In this section we will study distribution of words in corpora, ignoring for the
moment interaction between words (n-grams).

We define two helper functions. The first one creates a Document Feature Matrix
(DFM) for n-grams in documents, and aggregates it over all documents to a
Feature Vector. The second helper function enriches the Feature Vector with
additional values useful for our analysis, such as cumulated coverage of text.

```{r}
# Calculate Document Feature Matrix (DFM) for n-grams in documents,
# and aggregate it over all documents to a Feature Vector.
build.ngram <- function(text, n, min_freq = 1, stop_words = NULL) {
  # Split text on 1-grams.
  text.tokens <- tokens(text)
  
  # Remove stop-words, if required.
  if (!is.null(stop_words)) {
    text.tokens <- tokens_remove(text.tokens, stop_words)
  }
  
  # Stem words. We are using an explicit stemming to make sure that it is fully
  # compatible with another places later in the code where we have to apply
  # the stemming manually.
  text.tokens <- as.tokens(
    mclapply(text.tokens, function(x) SnowballC::wordStem(x, language = "en")))
  
  # Create n-grams, if n > 1
  if (n > 1) {
    text.tokens <- tokens_ngrams(text.tokens, n = n, concatenator = " ")
  }
  
  # Special case: if our corpus contains empty sentences, than 2-grams contains
  # "SOS EOS", that is sequences of "Start-Of-Sentence" + "End-Of-Sentence"
  # tokens. This may happens, for example, if a sentence contains only stop
  # words, or in some weird cases like a twitter which contains only a time
  # like "8:12". We are not interested in empty sequences, so we are removing
  # such tokens.
  if (n == 2) {
    text.tokens <- tokens_remove(text.tokens, c("SOS EOS"))
  }
  
  # Calculate the Document Feature Matrix
  text.dfm <- dfm(text.tokens, tolower = FALSE)
  
  # Remove from DFM least frequent features, if requested.
  if (min_freq > 1) {
    text.dfm <- dfm_trim(text.dfm, min_termfreq = min_freq)
  }
  
  # Sum over all documents.
  colSums(text.dfm)
}

# Sorts a Feature Vector in descending order of frequency and enriches it with
# additional columns:
#  * Cumulated frequency of terms (words or n-grams)
#  * Cumulated frequency as percentage of total.
enrich.ngram <- function(fv) {
  # Transform Feature Vector to a table and sort by frequency descending.
  tbl <- data.table(Terms = names(fv), Freq = fv)
  tbl <- tbl[order(-Freq)]
  
  # Add columns with cumulative frequency as a number of words, and as percentage.
  tbl$Freq.Cum <- cumsum(tbl$Freq)
  tbl$Freq.Cum.Pct <- tbl$Freq.Cum / sum(tbl$Freq)
  
  return (tbl)
}
```

Now we may calculate frequency of words in each source, as well as in all
sources together (aggregated).

```{r}
# Define stop-words for 1-grams: standard stop words, as well as our special
# tokens "Start-Of-Sentence" and "End-Of-Sentence".
stopwords.tokens <- c(stopwords(), "SOS", "EOS")

# Calculate a frequency of words in each sources, as well as an aggregated.
if (file.exists("cache/blogs.1gram.freq.RDS")) {
  blogs.1gram.freq <- readRDS("cache/blogs.1gram.freq.RDS")
} else {
  blogs.1gram.freq <- enrich.ngram(build.ngram(blogs.text.preprocessed, 1,
                                               stop_words = stopwords.tokens))
  saveRDS(blogs.1gram.freq, "cache/blogs.1gram.freq.RDS")
}
if (file.exists("cache/news.1gram.freq.RDS")) {
  news.1gram.freq <- readRDS("cache/news.1gram.freq.RDS")
} else {
  news.1gram.freq <- enrich.ngram(build.ngram(news.text.preprocessed, 1,
                                              stop_words = stopwords.tokens))
  saveRDS(news.1gram.freq, "cache/news.1gram.freq.RDS")
}
if (file.exists("cache/twitter.1gram.freq.RDS")) {
  twitter.1gram.freq <- readRDS("cache/twitter.1gram.freq.RDS")
} else {
  twitter.1gram.freq <- enrich.ngram(build.ngram(twitter.text.preprocessed, 1,
                                                 stop_words = stopwords.tokens))
  saveRDS(twitter.1gram.freq, "cache/twitter.1gram.freq.RDS")
}
if (file.exists("cache/all.1gram.freq.RDS")) {
  all.1gram.freq <- readRDS("cache/all.1gram.freq.RDS")
} else {
  all.text.preprocessed <- c(blogs.text.preprocessed, news.text.preprocessed,
                             twitter.text.preprocessed)
  all.1gram.freq <- enrich.ngram(build.ngram(all.text.preprocessed, 1,
                                             stop_words = stopwords.tokens))
  saveRDS(all.1gram.freq, "cache/all.1gram.freq.RDS")
}
```

The following chart displays 20 most-frequent words in each source, as well
as in the aggregated corpora.

```{r mostFrequent1Grams, echo=FALSE, cache=TRUE, fig.align='center', fig.width=10, fig.height=8}
# Display 20 most-frequent-words.
blogs.freq.top <- blogs.1gram.freq[1:20]
blogs.freq.top$Terms <- factor(blogs.freq.top$Terms,
                               levels = blogs.freq.top[order(20:1)]$Terms)

news.freq.top <- news.1gram.freq[1:20]
news.freq.top$Terms <- factor(news.freq.top$Terms,
                              levels = news.freq.top[order(20:1)]$Terms)

twitter.freq.top <- twitter.1gram.freq[1:20]
twitter.freq.top$Terms <- factor(twitter.freq.top$Terms,
                                 levels = twitter.freq.top[order(20:1)]$Terms)

all.freq.top <- all.1gram.freq[1:20]
all.freq.top$Terms <- factor(all.freq.top$Terms,
                             levels = all.freq.top[order(20:1)]$Terms)

g.blogs <- ggplot(blogs.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#CC8888", high = "#CC0000") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Blogs") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.news <- ggplot(news.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#88CC88", high = "#00CC00") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "News") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.twitter <- ggplot(twitter.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#8888CC", high = "#0000CC") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Twitter") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.all <- ggplot(all.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient2(low = "#8888CC", mid = "#44CC44", high = "#CC0000", midpoint = 10) +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Aggregated") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")


grid.arrange(grobs = list(g.blogs, g.news, g.twitter, g.all),
             widths = c(1, 1, 1),
             layout_matrix = rbind(c(1, 2, 3),
                                   c(NA, 4, NA)),
             top = textGrob("Top 20 Words", gp = gpar(fontsize=20)))
```

As we see from the chart, top 20 most-frequent words differs between sources.
For example, the most frequent word in news is "said", but this word is not
included in the top-20 list for blogs and Twitter at all. At the same time,
some words are shared between lists: the word "can" is 2nd most-frequent in
blogs, 3rd-most frequest in Twitter, and 5th in and news.

Our next step is to analyze the intersection, that is to find how many words
are common to all sources, and how many are unique to a particular source.
Not only just a number of words is important, but also a source coverage, that
is what percentage of the whole text of a particular source is covered by
a particular subset of all words.

The following [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) shows
a number of unique words (stems) used in each source, as well as a percentage
of the aggregated corpora covered by those words.

```{r venn1grams, echo=FALSE, cache=TRUE, fig.align='center'}
all.venn <- data.frame(
  "Terms" = all.1gram.freq$Terms,
  "Freq" = all.1gram.freq$Freq,
  "Blogs" = all.1gram.freq$Terms %in% blogs.1gram.freq$Terms,
  "News" = all.1gram.freq$Terms %in% news.1gram.freq$Terms,
  "Twitter" = all.1gram.freq$Terms %in% twitter.1gram.freq$Terms
)
all.venn$B <- (all.venn$Blogs & !all.venn$News & !all.venn$Twitter)
all.venn$N <- (!all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$T <- (!all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$BN <- (all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$BT <- (all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$NT <- (!all.venn$Blogs & all.venn$News & all.venn$Twitter)
all.venn$BNT <- (all.venn$Blogs & all.venn$News & all.venn$Twitter)

df.venn <- data.frame(x = c(0, 0.866, -0.866),
                      y = c(1, -0.5, -0.5),
                      labels = c('Blogs', 'News', 'Twitter'))
df.venn.count <- data.frame(x = c(0, 1.2, 0.8, -1.2, -0.8, 0, 0),
                            y = c(1.2, -0.6, 0.5, -0.6, 0.5, -1, 0))
df.venn.count$Counts <- c(sum(all.venn$B),
                          sum(all.venn$N),
                          sum(all.venn$BN),
                          sum(all.venn$T),
                          sum(all.venn$BT),
                          sum(all.venn$NT),
                          sum(all.venn$BNT))
df.venn.count$Freq <- c(sum(all.venn$Freq[all.venn$B]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$N]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BN]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$T]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$NT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BNT]) / sum(all.venn$Freq)
)
df.venn.count$Label <- paste0(df.venn.count$Counts, "\n",
                              sprintf("%.2f%%", df.venn.count$Freq * 100))

ggplot(df.venn) +
  geom_circle(aes(x0 = x, y0 = y, r = 1.5, fill = labels), alpha = .3, size = 1, colour = "#666666") +
  coord_fixed() +
  theme_void(base_size = 14) +
  theme(legend.position = 'bottom') +
  scale_fill_manual(values = c("#CC0000", "#008800", "#0000FF")) +
  scale_colour_manual(values = c("#CC0000", "#008800", "#0000FF"), guide = FALSE) +
  labs(title = "Stems Common in Multiple Corpora") +
  labs(fill = NULL) +
  annotate("text", x = df.venn.count$x, y = df.venn.count$y, label = df.venn.count$Label, size = 5)
```

As we may see, `r sprintf("%d", df.venn.count$Counts[7])` words are shared by
all 3 corpora, but those words cover `r sprintf("%.2f%%", df.venn.count$Freq[7]
* 100)` of the aggregated corpora. On the other hand, there are `r sprintf("%d",
df.venn.count$Counts[1])` words unique to blogs, but these words appear very
infrequently, covering just `r sprintf("%.2f%%", df.venn.count$Freq[1] * 100)`
of the aggregated corpora.

The Venn diagram indicates that we may get a high coverage of all corpora by
choosing common words. Coverage by words specific to a particular corpus is
negligible.

The next step in our analysis is to find out how many common words we should
choose to achieve a decent coverage of the text. From the Venn diagram we
already know that by choosing `r sprintf("%d", df.venn.count$Counts[7])` words
we will cover `r sprintf("%.2f%%", df.venn.count$Freq[7] * 100)` of the
aggregated corpora, but maybe we may reduce a number of words without
significantly reducing the coverage.

The following chart shows a number of unique words in each source which cover
particular percentage of the text. For example, 1000 most-frequent words cover
`r sprintf("%.2f%%", twitter.1gram.freq[1000]$Freq.Cum.Pct * 100)` of the
Twitter corpus. An interesting observation is that Twitter requires less words
to cover particular percentage of the text, whereas news requires more words.

```{r cover1grams, echo=FALSE, cache=TRUE, fig.align='center'}
blogs.freq.pct <- data.frame("Source" = rep("Blogs", nrow(blogs.1gram.freq)),
                             "Count" = 1:nrow(blogs.1gram.freq),
                             "Freq.Pct" = blogs.1gram.freq$Freq.Cum.Pct)

news.freq.pct <- data.frame("Source" = rep("News", nrow(news.1gram.freq)),
                            "Count" = 1:nrow(news.1gram.freq),
                            "Freq.Pct" = news.1gram.freq$Freq.Cum.Pct)

twitter.freq.pct <- data.frame("Source" = rep("Twitter", nrow(twitter.1gram.freq)),
                               "Count" = 1:nrow(twitter.1gram.freq),
                               "Freq.Pct" = twitter.1gram.freq$Freq.Cum.Pct)

all.freq.pct <- data.frame("Source" = rep("Aggregated", nrow(all.1gram.freq)),
                           "Count" = 1:nrow(all.1gram.freq),
                           "Freq.Pct" = all.1gram.freq$Freq.Cum.Pct)

stat.freq.pct <- rbind(blogs.freq.pct, news.freq.pct, twitter.freq.pct, all.freq.pct)


ggplot(data = stat.freq.pct, aes(x = Count, y = Freq.Pct, group = Source)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000),
                     labels = c("10", "100", "1,000", "10,000", "100,000")) +
  scale_color_manual(values=c("#CC0000", "#008800", "#0000FF", "#666666")) +
  geom_line(aes(color = Source), size = 1.1) +
  labs(title = "Cumulative Frequency of Stems") +
  labs(x = "Unique stems") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

```{r echo=FALSE}
all.freq.tbl <-
    data.frame(
        "Corpora Coverage" = c("75%", "90%", "95%", "99%", "99.9%"),
        "Blogs" = c(
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(blogs.1gram.freq[blogs.1gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "News" = c(
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(news.1gram.freq[news.1gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Twitter" = c(
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(twitter.1gram.freq[twitter.1gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Aggregated" = c(
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(all.1gram.freq[all.1gram.freq$Freq.Cum.Pct <= 0.999])
        )
    )
kable(all.freq.tbl, booktabs = TRUE,
      col.names = c("Corpora Coverage",
                    "Blogs",
                    "News",
                    "Twitter",
                    "Aggregated")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The table shows that in order to cover 95% of blogs, we require
`r sprintf("%d", all.freq.tbl$Blogs[3])` words. The same coverage of news
require `r sprintf("%d", all.freq.tbl$News[3])` words, and the coverage
of twitter `r sprintf("%d", all.freq.tbl$Twitter[3])` words. To cover 95%
of the aggregated corpora, we require
`r sprintf("%d", all.freq.tbl$Aggregated[3])` unique words. We may use this fact
later to reduce a number of n-grams required for predictions.

# <a name="analyze_2_grams"></a>Analyzing bigrams

In this section we will study distribution bigrams, that is combinations of
two words.

Using previously defined functions, we may calculate frequency of bigrams in
each source, as well as in all sources together (aggregated).

```{r}
# Calculate a frequency of words in each sources, as well as an aggregated.
if (file.exists("cache/blogs.2gram.freq.RDS")) {
  blogs.2gram.freq <- readRDS("cache/blogs.2gram.freq.RDS")
} else {
  blogs.2gram.freq <- enrich.ngram(build.ngram(blogs.text.preprocessed, 2,
                                               stop_words = stopwords.tokens))
  
  saveRDS(blogs.2gram.freq, "cache/blogs.2gram.freq.RDS")
}
if (file.exists("cache/news.2gram.freq.RDS")) {
  news.2gram.freq <- readRDS("cache/news.2gram.freq.RDS")
} else {
  news.2gram.freq <- enrich.ngram(build.ngram(news.text.preprocessed, 2,
                                              stop_words = stopwords.tokens))
  
  saveRDS(news.2gram.freq, "cache/news.2gram.freq.RDS")
}
if (file.exists("cache/twitter.2gram.freq.RDS")) {
  twitter.2gram.freq <- readRDS("cache/twitter.2gram.freq.RDS")
} else {
  twitter.2gram.freq <- enrich.ngram(build.ngram(twitter.text.preprocessed, 2,
                                                 stop_words = stopwords.tokens))
  
  saveRDS(twitter.2gram.freq, "cache/twitter.2gram.freq.RDS")
}
if (file.exists("cache/all.2gram.freq.RDS")) {
  all.2gram.freq <- readRDS("cache/all.2gram.freq.RDS")
} else {
  all.text.preprocessed <- c(blogs.text.preprocessed, news.text.preprocessed,
                             twitter.text.preprocessed)
  all.2gram.freq <- enrich.ngram(build.ngram(all.text.preprocessed, 2,
                                             stop_words = stopwords.tokens))
  
  saveRDS(all.2gram.freq, "cache/all.2gram.freq.RDS")
}
```

The following chart displays 20 most-frequent bigrams in each source, as well
as in the aggregated corpora.

```{r mostFrequent2Grams, echo=FALSE, cache=TRUE, fig.align='center', fig.width=10, fig.height=8}
blogs.freq.top <- blogs.2gram.freq[1:20]
blogs.freq.top$Terms <- factor(blogs.freq.top$Terms,
                               levels = blogs.freq.top[order(20:1)]$Terms)

news.freq.top <- news.2gram.freq[1:20]
news.freq.top$Terms <- factor(news.freq.top$Terms,
                              levels = news.freq.top[order(20:1)]$Terms)

twitter.freq.top <- twitter.2gram.freq[1:20]
twitter.freq.top$Terms <- factor(twitter.freq.top$Terms,
                                 levels = twitter.freq.top[order(20:1)]$Terms)

all.freq.top <- all.2gram.freq[1:20]
all.freq.top$Terms <- factor(all.freq.top$Terms,
                             levels = all.freq.top[order(20:1)]$Terms)

g.blogs <- ggplot(blogs.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#CC8888", high = "#CC0000") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Blogs") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.news <- ggplot(news.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#88CC88", high = "#00CC00") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "News") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.twitter <- ggplot(twitter.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient(low = "#8888CC", high = "#0000CC") +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Twitter") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")

g.all <- ggplot(all.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
  geom_bar(stat = 'identity') +
  scale_fill_gradient2(low = "#8888CC", mid = "#44CC44", high = "#CC0000", midpoint = 10) +
  coord_flip() +
  labs(x = "Word") +
  labs(y = "Count") +
  labs(title = "Aggregated") +
  theme_bw(base_size = 12) +
  theme(legend.position = "none")


grid.arrange(grobs = list(g.blogs, g.news, g.twitter, g.all),
             widths = c(1, 1, 1),
             layout_matrix = rbind(c(1, 2, 3),
                                   c(NA, 4, NA)),
             top = textGrob("Top 20 Bigrams", gp = gpar(fontsize=20)))  
```

We immediately see a difference with lists of top 20 words: there were much more
common words between sources, as there are common bigrams. There are still
some common bigrams, but the intersection is smaller.

Similar to how we proceed with words, now we will analyze intersections, that is
we will find how many bigrams are common to all sources, and how many are unique
to a particular source. We also calculate a percentage of the whole source
covered by a particular subset of all bigrams.

The following [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) shows
a number of unique bigrams used in each source, as well as a percentage
of the aggregated corpora covered by those bigrams.

```{r venn2grams, echo=FALSE, cache=TRUE, fig.align='center'}
all.venn <- data.frame(
  "Terms" = all.2gram.freq$Terms,
  "Freq" = all.2gram.freq$Freq,
  "Blogs" = all.2gram.freq$Terms %in% blogs.2gram.freq$Terms,
  "News" = all.2gram.freq$Terms %in% news.2gram.freq$Terms,
  "Twitter" = all.2gram.freq$Terms %in% twitter.2gram.freq$Terms
)
all.venn$B <- (all.venn$Blogs & !all.venn$News & !all.venn$Twitter)
all.venn$N <- (!all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$T <- (!all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$BN <- (all.venn$Blogs & all.venn$News & !all.venn$Twitter)
all.venn$BT <- (all.venn$Blogs & !all.venn$News & all.venn$Twitter)
all.venn$NT <- (!all.venn$Blogs & all.venn$News & all.venn$Twitter)
all.venn$BNT <- (all.venn$Blogs & all.venn$News & all.venn$Twitter)

df.venn <- data.frame(x = c(0, 0.866, -0.866),
                      y = c(1, -0.5, -0.5),
                      labels = c('Blogs', 'News', 'Twitter'))
df.venn.count <- data.frame(x = c(0, 1.2, 0.8, -1.2, -0.8, 0, 0),
                            y = c(1.2, -0.6, 0.5, -0.6, 0.5, -1, 0))
df.venn.count$Counts <- c(sum(all.venn$B),
                          sum(all.venn$N),
                          sum(all.venn$BN),
                          sum(all.venn$T),
                          sum(all.venn$BT),
                          sum(all.venn$NT),
                          sum(all.venn$BNT))
df.venn.count$Freq <- c(sum(all.venn$Freq[all.venn$B]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$N]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BN]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$T]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$NT]) / sum(all.venn$Freq),
                        sum(all.venn$Freq[all.venn$BNT]) / sum(all.venn$Freq)
)
df.venn.count$Label <- paste0(df.venn.count$Counts, "\n",
                              sprintf("%.2f%%", df.venn.count$Freq * 100))

ggplot(df.venn) +
  geom_circle(aes(x0 = x, y0 = y, r = 1.5, fill = labels), alpha = .3, size = 1, colour = "#666666") +
  coord_fixed() +
  theme_void(base_size = 14) +
  theme(legend.position = 'bottom') +
  scale_fill_manual(values = c("#CC0000", "#008800", "#0000FF")) +
  scale_colour_manual(values = c("#CC0000", "#008800", "#0000FF"), guide = FALSE) +
  labs(title = "Bigrams Common in Multiple Corpora") +
  labs(fill = NULL) +
  annotate("text", x = df.venn.count$x, y = df.venn.count$y, label = df.venn.count$Label, size = 5)
```

The difference between words and bigrams is even more pronounced here.
Bigrams common to all sources cover just
`r sprintf("%.2f%%", df.venn.count$Freq[7] * 100)` of the text, compared to
more than 96% covered by words common to all sources.

The next step in our analysis is to find out how many common bigrams we should
choose to achieve a decent coverage of the text.

The following chart shows a number of unique bigrams in each source which cover
particular percentage of the text. For example, 1000 most-frequent bigrams cover
`r sprintf("%.2f%%", twitter.2gram.freq[1000]$Freq.Cum.Pct * 100)` of the
Twitter corpus.

```{r cover2grams, echo=FALSE, cache=TRUE, fig.align='center', fig.width=10}
# Cumulated frequency chart
blogs.freq.pct <- data.frame("Source" = rep("Blogs", nrow(blogs.2gram.freq)),
                             "Count" = 1:nrow(blogs.2gram.freq),
                             "Freq.Pct" = blogs.2gram.freq$Freq.Cum.Pct)

news.freq.pct <- data.frame("Source" = rep("News", nrow(news.2gram.freq)),
                            "Count" = 1:nrow(news.2gram.freq),
                            "Freq.Pct" = news.2gram.freq$Freq.Cum.Pct)

twitter.freq.pct <- data.frame("Source" = rep("Twitter", nrow(twitter.2gram.freq)),
                               "Count" = 1:nrow(twitter.2gram.freq),
                               "Freq.Pct" = twitter.2gram.freq$Freq.Cum.Pct)

all.freq.pct <- data.frame("Source" = rep("Aggregated", nrow(all.2gram.freq)),
                           "Count" = 1:nrow(all.2gram.freq),
                           "Freq.Pct" = all.2gram.freq$Freq.Cum.Pct)

stat.freq.pct <- rbind(blogs.freq.pct, news.freq.pct, twitter.freq.pct, all.freq.pct)


ggplot(data = stat.freq.pct, aes(x = Count, y = Freq.Pct, group = Source)) +
  scale_x_continuous(trans = "log10",
                     breaks = c(10, 100, 1000, 10000, 100000, 1000000, 10000000),
                     labels = c("10", "100", "1,000", "10,000", "100,000", "1,000,000", "10,000,000")) +
  scale_color_manual(values=c("#CC0000", "#008800", "#0000FF", "#666666")) +
  geom_line(aes(color = Source), size = 1.1) +
  labs(title = "Cumulative Frequency of Bigrams") +
  labs(x = "Unique bigrams") +
  labs(y = "Cumulative Frequency, %") +
  theme_bw(base_size = 14)
```

```{r echo=FALSE}

all.freq.tbl <-
    data.frame(
        "Corpora Coverage" = c("75%", "90%", "95%", "99%", "99.9%"),
        "Blogs" = c(
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(blogs.2gram.freq[blogs.2gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "News" = c(
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(news.2gram.freq[news.2gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Twitter" = c(
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(twitter.2gram.freq[twitter.2gram.freq$Freq.Cum.Pct <= 0.999])
        ),
        "Aggregated" = c(
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.75]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.90]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.95]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.99]),
            nrow(all.2gram.freq[all.2gram.freq$Freq.Cum.Pct <= 0.999])
        )
    )
kable(all.freq.tbl, booktabs = TRUE,
      col.names = c("Corpora Coverage",
                    "Blogs",
                    "News",
                    "Twitter",
                    "Aggregated")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The table shows that in order to cover 95% of blogs, we require
`r formatC(all.freq.tbl$Blogs[3], big.mark = ",", format = "d")` bigrams.
The same coverage of news
require `r formatC(all.freq.tbl$News[3], big.mark = ",", format = "d")` bigrams,
and the coverage of Twitter
`r formatC(all.freq.tbl$Twitter[3], big.mark = ",", format = "d")` bigrams.
To cover 95% of the aggregated corpora, we require
`r formatC(all.freq.tbl$Aggregated[3], big.mark = ",", format = "d")` bigrams.
  
The chart is also very different from a similar chart for words. The curve for
words had an "S"-shape, that is it's growth slowed down after some number of
words, so that adding more words resulted in diminishing returns. For bigrams,
there is no point of diminishing returns: curves are just rising.
  