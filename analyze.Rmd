---
title: "Analyzing the Corpora"
author: "Alexey Serdyuk"
date: "9 June 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a part of the capstone project for the cycle of courses
[Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)
offered on [Coursera](https://www.coursera.org) by
[Johns Hopkins University](https://www.jhu.edu).

We assume that the corpora were already downloaded and preprocessed.

We load the required libraries.

```{r message=FALSE}
library(tm) # For handling the corpora.
library(readr) # For fast reading/writing.
library(ggplot2) # For charts.
library(ggforce) # For charts.
library(grid) # For arranging charts in a grid.
library(gridExtra) # For arranging charts in a grid.
library(kableExtra) # For formatting tables.
```

We define a function that creates a Corpus from the specified file,
where each line is transformed to a separate document.

```{r}
loadCorpus <- function(fileName) {
    # Read file into a character vector.
    text <- read_lines(fileName)

    # Create a Corpus from the character vector.
    Corpus(VectorSource(text),
           readerControl = list(reader = readPlain,
                                language = "en_US",
                                load = TRUE))
}
```

Using just defined function, we load pre-processed corpora.

```{r}
blogs.corpus <- loadCorpus("data-split/blogs.training.pre.txt")
news.corpus <- loadCorpus("data-split/news.training.pre.txt")
twitter.corpus <- loadCorpus("data-split/twitter.training.pre.txt")
```

## Distribution of words

In this section we will study distribution of words in corpora, ignoring for the
moment interaction between words (n-grams).

For each corpus we calculate a Term Document Matrix.

```{r}
blogs.tdm <- TermDocumentMatrix(blogs.corpus, control = list())
news.tdm <- TermDocumentMatrix(news.corpus, control = list())
twitter.tdm <- TermDocumentMatrix(twitter.corpus, control = list())
all.tdm <- c(blogs.tdm, news.tdm, twitter.tdm)
```

We define a function that transforms a Term Document Matrix into a table
with frequency of each unique word, and with a cumulative percentage. We will
use the cumulative percentage to estimate how many unique words we require
to achieve a particular text coverage (for example, to cover 95% of the text).

```{r}
# Build a table with word frequency from the specified Term Document Matrix.
buildWordFreq <- function(tdm) {
    # Collapse TDM, aggregating count of words in all documents.
    tdm <- slam::rollup(tdm, 2)

    # Transform TDM to a table and sort by frequency descending.
    tbl <- data.table::data.table(Terms = tdm$dimnames$Terms[tdm$i], Freq = tdm$v)
    tbl <- tbl[order(-Freq)]

    # Add columns with cumulative frequency as a number of words, and as percentage.
    tbl$Freq.Cum <- cumsum(tbl$Freq)
    tbl$Freq.Pct <- tbl$Freq.Cum / sum(tbl$Freq)

    return (tbl)
}
```

Now we may calculate frequency of words in each source, as well as in all sources
together (aggregated).

```{r}
blogs.freq <- buildWordFreq(blogs.tdm)
news.freq <- buildWordFreq(news.tdm)
twitter.freq <- buildWordFreq(twitter.tdm)
all.freq <- buildWordFreq(all.tdm)
```

The following chart displays 20 most-frequent words in each source, as well
as in the aggregated corpora.

```{r echo=FALSE, fig.align='center', fig.width=10, fig.height=8}
    blogs.freq.top <- blogs.freq[1:20]
    blogs.freq.top$Terms <- factor(blogs.freq.top$Terms,
                                   levels = blogs.freq.top[order(20:1)]$Terms)

    news.freq.top <- news.freq[1:20]
    news.freq.top$Terms <- factor(news.freq.top$Terms,
                                  levels = news.freq.top[order(20:1)]$Terms)

    twitter.freq.top <- twitter.freq[1:20]
    twitter.freq.top$Terms <- factor(twitter.freq.top$Terms,
                                     levels = twitter.freq.top[order(20:1)]$Terms)

    all.freq.top <- all.freq[1:20]
    all.freq.top$Terms <- factor(all.freq.top$Terms,
                                 levels = all.freq.top[order(20:1)]$Terms)
    
    g.blogs <- ggplot(blogs.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
        geom_bar(stat = 'identity') +
        scale_fill_gradient(low = "#CC8888", high = "#CC0000") +
        coord_flip() +
        labs(x = "Word") +
        labs(y = "Count") +
        labs(title = "Blogs") +
        theme_bw(base_size = 12) +
        theme(legend.position = "none")

    g.news <- ggplot(news.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
        geom_bar(stat = 'identity') +
        scale_fill_gradient(low = "#88CC88", high = "#00CC00") +
        coord_flip() +
        labs(x = "Word") +
        labs(y = "Count") +
        labs(title = "News") +
        theme_bw(base_size = 12) +
        theme(legend.position = "none")

    g.twitter <- ggplot(twitter.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
        geom_bar(stat = 'identity') +
        scale_fill_gradient(low = "#8888CC", high = "#0000CC") +
        coord_flip() +
        labs(x = "Word") +
        labs(y = "Count") +
        labs(title = "Twitter") +
        theme_bw(base_size = 12) +
        theme(legend.position = "none")

    g.all <- ggplot(all.freq.top, aes(x = Terms, y = Freq, fill = as.integer(Terms))) +
        geom_bar(stat = 'identity') +
        scale_fill_gradient2(low = "#8888CC", mid = "#44CC44", high = "#CC0000", midpoint = 10) +
        coord_flip() +
        labs(x = "Word") +
        labs(y = "Count") +
        labs(title = "Aggregated") +
        theme_bw(base_size = 12) +
        theme(legend.position = "none")


    grid.arrange(grobs = list(g.blogs, g.news, g.twitter, g.all),
                 widths = c(1, 1, 1),
                 layout_matrix = rbind(c(1, 2, 3),
                                       c(NA, 4, NA)),
                 top = textGrob("Top 20 Words", gp = gpar(fontsize=20)))
```

As we see from the chart, top 20 most-frequent words differs between sources.
For example, the most frequent word in news is "said", but this word is not
included in the top-20 list for blogs and twitter at all. At the same time,
some words are shared between lists: the word "will" is 2nd most-frequent in
blogs and news, and 8th most-frequent in twitter.

Our next step is to analyze the intersection, that is to find how many words
are common to all sources, and how many are unique to a particular source.
Not only just a number of words is important, but also a source coverage, that
is what percentage of the whole text of a particular source is covered by
a particular subset of all words.

The following [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) shows
a number of unique words (stems) used in each source, as well as a percentage
of the aggregated corpora covered by those words.

```{r echo=FALSE, fig.align='center'}
all.venn <- data.frame(
        "Terms" = all.freq$Terms,
        "Freq" = all.freq$Freq,
        "Blogs" = all.freq$Terms %in% blogs.freq$Terms,
        "News" = all.freq$Terms %in% news.freq$Terms,
        "Twitter" = all.freq$Terms %in% twitter.freq$Terms
    )
    all.venn$B <- (all.venn$Blogs & !all.venn$News & !all.venn$Twitter)
    all.venn$N <- (!all.venn$Blogs & all.venn$News & !all.venn$Twitter)
    all.venn$T <- (!all.venn$Blogs & !all.venn$News & all.venn$Twitter)
    all.venn$BN <- (all.venn$Blogs & all.venn$News & !all.venn$Twitter)
    all.venn$BT <- (all.venn$Blogs & !all.venn$News & all.venn$Twitter)
    all.venn$NT <- (!all.venn$Blogs & all.venn$News & all.venn$Twitter)
    all.venn$BNT <- (all.venn$Blogs & all.venn$News & all.venn$Twitter)

    df.venn <- data.frame(x = c(0, 0.866, -0.866),
                          y = c(1, -0.5, -0.5),
                          labels = c('Blogs', 'News', 'Twitter'))
    df.venn.count <- data.frame(x = c(0, 1.2, 0.8, -1.2, -0.8, 0, 0),
                                y = c(1.2, -0.6, 0.5, -0.6, 0.5, -1, 0))
    df.venn.count$Counts <- c(sum(all.venn$B),
                              sum(all.venn$N),
                              sum(all.venn$BN),
                              sum(all.venn$T),
                              sum(all.venn$BT),
                              sum(all.venn$NT),
                              sum(all.venn$BNT))
    df.venn.count$Freq <- c(sum(all.venn$Freq[all.venn$B]) / sum(all.venn$Freq),
                            sum(all.venn$Freq[all.venn$N]) / sum(all.venn$Freq),
                            sum(all.venn$Freq[all.venn$BN]) / sum(all.venn$Freq),
                            sum(all.venn$Freq[all.venn$T]) / sum(all.venn$Freq),
                            sum(all.venn$Freq[all.venn$BT]) / sum(all.venn$Freq),
                            sum(all.venn$Freq[all.venn$NT]) / sum(all.venn$Freq),
                            sum(all.venn$Freq[all.venn$BNT]) / sum(all.venn$Freq)
    )
    df.venn.count$Label <- paste0(df.venn.count$Counts, "\n",
                                  sprintf("%.2f%%", df.venn.count$Freq * 100))

    ggplot(df.venn) +
        geom_circle(aes(x0 = x, y0 = y, r = 1.5, fill = labels), alpha = .3, size = 1, colour = "#666666") +
        coord_fixed() +
        theme_void(base_size = 14) +
        theme(legend.position = 'bottom') +
        scale_fill_manual(values = c("#CC0000", "#008800", "#0000FF")) +
        scale_colour_manual(values = c("#CC0000", "#008800", "#0000FF"), guide = FALSE) +
        labs(title = "Stems Common in Multiple Corpora") +
        labs(fill = NULL) +
        annotate("text", x = df.venn.count$x, y = df.venn.count$y, label = df.venn.count$Label, size = 5)
```

As we may see, `r sprintf("%d", df.venn.count$Counts[7])` words are shared by
all 3 corpora, but those words cover
`r sprintf("%.2f%%", df.venn.count$Freq[7] * 100)` of the aggregated corpora.
On the other hand, there are `r sprintf("%d", df.venn.count$Counts[1])` words
unique to blogs, but these words appear very infrequently, covering just
`r sprintf("%.2f%%", df.venn.count$Freq[1] * 100)` of the aggregated corpora.

The Venn diagram indicates that we may get a high coverage of all corpora
by choosing common words. Coverage by words specific to a particular corpus
is negligible.

The next step in our analysis is to find out how many common words we should
choose to achieve a decent coverage of the text. From the Venn diagram we
already know that by choosing `r sprintf("%d", df.venn.count$Counts[7])` words
we will cover `r sprintf("%.2f%%", df.venn.count$Freq[7] * 100)`
of the aggregated corpora, but maybe we may reduce a number of words without
significantly reducing the coverage.

The following chart shows a number of unique words in each source which cover
particular percentage of the text. For example, 1000 most-frequent words cover
`r sprintf("%.2f%%", twitter.freq[1000]$Freq.Pct * 100)` of the Twitter corpus.
An interesting observation is that Twitter requires less words to cover particular
percentage of the text, whereas news requires more words.

```{r echo=FALSE, fig.align='center'}
blogs.freq.pct <- data.frame("Source" = rep("Blogs", nrow(blogs.freq)),
                             "Count" = 1:nrow(blogs.freq),
                             "Freq.Pct" = blogs.freq$Freq.Pct)

news.freq.pct <- data.frame("Source" = rep("News", nrow(news.freq)),
                            "Count" = 1:nrow(news.freq),
                            "Freq.Pct" = news.freq$Freq.Pct)

twitter.freq.pct <- data.frame("Source" = rep("Twitter", nrow(twitter.freq)),
                               "Count" = 1:nrow(twitter.freq),
                               "Freq.Pct" = twitter.freq$Freq.Pct)

all.freq.pct <- data.frame("Source" = rep("Aggregated", nrow(all.freq)),
                           "Count" = 1:nrow(all.freq),
                           "Freq.Pct" = all.freq$Freq.Pct)

stat.freq.pct <- rbind(blogs.freq.pct, news.freq.pct, twitter.freq.pct, all.freq.pct)


ggplot(data = stat.freq.pct, aes(x = Count, y = Freq.Pct, group = Source)) +
    scale_x_continuous(trans = "log10",
                       breaks = c(10, 100, 1000, 10000, 100000),
                       labels = c("10", "100", "1,000", "10,000", "100,000")) +
    scale_color_manual(values=c("#CC0000", "#008800", "#0000FF", "#666666")) +
    geom_line(aes(color = Source), size = 1.1) +
    labs(title = "Cumulative Frequency of Stems") +
    labs(x = "Unique stems") +
    labs(y = "Cumulative Frequency, %") +
    theme_bw(base_size = 14)    
```


```{r echo=FALSE}

all.freq.tbl <-
    data.frame(
        "Corpora Coverage" = c("75%", "90%", "95%", "99%", "99.9%"),
        "Blogs" = c(
            nrow(blogs.freq[blogs.freq$Freq.Pct <= 0.75]),
            nrow(blogs.freq[blogs.freq$Freq.Pct <= 0.90]),
            nrow(blogs.freq[blogs.freq$Freq.Pct <= 0.95]),
            nrow(blogs.freq[blogs.freq$Freq.Pct <= 0.99]),
            nrow(blogs.freq[blogs.freq$Freq.Pct <= 0.999])
        ),
        "News" = c(
            nrow(news.freq[news.freq$Freq.Pct <= 0.75]),
            nrow(news.freq[news.freq$Freq.Pct <= 0.90]),
            nrow(news.freq[news.freq$Freq.Pct <= 0.95]),
            nrow(news.freq[news.freq$Freq.Pct <= 0.99]),
            nrow(news.freq[news.freq$Freq.Pct <= 0.999])
        ),
        "Twitter" = c(
            nrow(twitter.freq[twitter.freq$Freq.Pct <= 0.75]),
            nrow(twitter.freq[twitter.freq$Freq.Pct <= 0.90]),
            nrow(twitter.freq[twitter.freq$Freq.Pct <= 0.95]),
            nrow(twitter.freq[twitter.freq$Freq.Pct <= 0.99]),
            nrow(twitter.freq[twitter.freq$Freq.Pct <= 0.999])
        ),
        "Aggregated" = c(
            nrow(all.freq[all.freq$Freq.Pct <= 0.75]),
            nrow(all.freq[all.freq$Freq.Pct <= 0.90]),
            nrow(all.freq[all.freq$Freq.Pct <= 0.95]),
            nrow(all.freq[all.freq$Freq.Pct <= 0.99]),
            nrow(all.freq[all.freq$Freq.Pct <= 0.999])
        )
    )
kable(all.freq.tbl, booktabs = TRUE,
      col.names = c("Corpora Coverage",
                    "Blogs",
                    "News",
                    "Twitter",
                    "Aggregated")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The table shows that in order to cover 95% of blogs, we require
`r sprintf("%d", all.freq.tbl$Blogs[3])` words. The same coverage of news
require `r sprintf("%d", all.freq.tbl$News[3])` words, and the coverage
of twitter `r sprintf("%d", all.freq.tbl$Twitter[3])` words. To cover 95%
of the aggregated corpora, we require
`r sprintf("%d", all.freq.tbl$Aggregated[3])` unique words.
