---
title: 'Data Science Capstone Project: Testing and Tuning'
author: "Alexey Serdyuk"
date: "07/07/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Table of content

* [Synopsis](#synopsis)
* [Initial version](#initial_version)
* [Skipping stopwords](#skipping_stopwords)


# <a name="synopsis"></a>Synopsis

This report describes testing and tuning done during an implementation of the
capstone project for the cycle of courses [Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)
offered on [Coursera](https://www.coursera.org) by
[Johns Hopkins University](https://www.jhu.edu).

The purpose of the capstone project is to build a Natural Language Processing
(NLP) application, that, given a chunk of text, predicts the next most probable
word. The application may be used, for example, in mobile devided to provide
suggestions as the user tips in some text.

In this report we describe tested approaches and decision done when tuning
the algorithm.

# <a name="initial_version"></a>Initial version

We started with the KISS approach, that is "keep it simple, stupid": implement
a naive algorithm with minimum of optimizations. The algorithm is based on the
following decisions:

* Apply to the training corpora the same cleanup transformations as in the
[Milestone Report](MilestoneReportWeek2.html): remove e-mails and hashtags,
transform to the lower case etc.
* Do not remove stopwords (very common words such as "the", "and" etc).
* Use a special token `STOS` for Start Of Sentence. This gives us a chance
to improve prediction quality for first words in a sentences.
* Do not attempt to predict across sentence borders.
* Build n-grams using a custom algorithm described below.
* Use 2- to 4-grams for prediction. Attempt 4-grams first; if no match is found,
fall back on 3- and finally on 2-grams.

N-grams were created using a custom algorithm: first (n-1) words (the prefix
for matching) is simplified, the last word remains as is. The simplification
allows to reduce the number of n-grams and thus reduce the memory requirements:

* Stem words.
* Use only $M$ most frequent stems, replace all other stems with a special
token `UNK` (Unknown).
* When choosing $M$ (the number of stems to keep) we considered how we will
reduce required memory in the future. We decided to encode each stem by a 2-byte
code. On a 64-bit system we may pack 8 bytes (codes of 4 stems) in a single
integer number, so prefixes of n-grams up to 5-gram may be represented as
integers, significantly reducing memory requirements. If we use 2 bytes for
encoding each stem, we may have $256\cdot 256 = 65536$ combinations in total.
We have to reserve 2 codes for special tokens `STOS` (Start Of Sentence)
and `UNK` (Unknown), so we may keep $256\cdot 256 - 2 = 65534$ most frequent
stems.

## Testing (Quiz 3)

We have tested the initial approach on examples from Quiz 3 from the course
materials.

**Question 1:** The guy in front of me just bought a pound of bacon, a bouquet, and a case of

* soda
* beer
* cheese
* pretzels

Results of our algorithm:

* 4-gram match: a case of *the*
* 3-gram match: case of *the*
* 2-gram match: of *the*


**Question 2:** You're the reason why I smile everyday. Can you follow me please? It would mean the

* world
* best
* most
* universe

Results of our algorithm:

* 4-gram match: would mean the *world*
* 3-gram match: mean the *world*
* 2-gram match: the *first*

**Question 3:** Hey sunshine, can you follow me and make me the

* happiest
* saddest
* smelliest
* bluest

Results of our algorithm:

* 4-gram match: make me the *happiest*
* 3-gram match: me the *most*
* 2-gram match: the *first*

**Question 4:** Very early observations on the Bills game: Offense still struggling but the

* referees
* crowd
* defense
* players

Results of our algorithm:

* 4-gram match: *no match*
* 3-gram match: but the *fact*
* 2-gram match: the *first*

**Question 5:** Go on a romantic date at the

* beach
* mall
* grocery
* movies

Results of our algorithm:

* 4-gram match: date at the *time*
* 3-gram match: at the *end*
* 2-gram match: the *first*

**Question 6:** Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my

* way
* motorcycle
* horse
* phone

Results of our algorithm:

* 4-gram match: be on my *way*
* 3-gram match: on my *way*
* 2-gram match: my *life*

**Question 7:** Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some

* weeks
* time
* thing
* years

Results of our algorithm:

* 4-gram match: in quit some *time*
* 3-gram match: quite some *time*
* 2-gram match: some *of*


**Question 8:** After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little

* fingers
* toes
* ears
* eyes

Results of our algorithm:

* 4-gram match: with his littl *brother*
* 3-gram match: his littl *brother*
* 2-gram match: littl *bit*

Note that the matched prefix contains just a stem "littl" from the word "little".

**Question 9:** Be grateful for the good times and keep the faith during the

* sad
* bad
* hard
* worse

Results of our algorithm:

* 4-gram match: faith dure the *worship*
* 3-gram match: dure the *day*
* 2-gram match: the *first*

Note that the matched prefix contains just a stem "dure" from the word "during".

**Question 10:** If this isn't the cutest thing you've ever seen, then you must be

* insensitive
* callous
* insane
* asleep

Results of our algorithm:

* 4-gram match: you must be *a*
* 3-gram match: must be *a*
* 2-gram match: be *a*

**Summary:**

* In 4 cases (2, 3, 6, 7) or prediction looks at least plausible.
* In 4 cases (4, 5, 8, 9) our prediction looks wrong.
* In 2 cases (1, 10) we have predicted an article ("a", "the").





