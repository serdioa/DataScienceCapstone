---
title: "Next Word Prediction: Stupid Backoff"
author: "Alexey Serdyuk"
date: "`r format(Sys.time(), '%d.%m.%Y')`"
output: html_document
#output:
#  md_document:
#    variant: markdown_github
#always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Content

* [Synopsis](#synopsis)
* [Model](#model)
* [Downloading and splitting the corpus](#download_and_split)
* [Cleaning and preprocessing the corpus](#cleaning_preprocessing)
* [Building n-grams](#building_n_grams)
* [Binary encoding for stems](#binary_encoding)
* [Optimizing n-gram tables](#optimizing_tables).
* [Stupid Backoff algorithm](#stupid_backoff_algorithm)
* [Extension of the algorithm](#algorithm_extension)
* [Testing algorithm](#testing_algorithm)
* [Optimizing meta-parameters](#optimizing_meta_parameters)
* [Algorithm speed](#algorithm_speed)
* [Validation](#validation)
* [Application](#application)
* [References](#references)

# <a name="synopsis"></a>Synopsis

This document provides the background and describes the approach used to
implement the application "Next Word Prediction: Stupid Backoff".

The application was developed as a capstone project for the for the cycle of courses
[Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)
offered on [Coursera](https://www.coursera.org) by
[Johns Hopkins University](https://www.jhu.edu).
The purpose of the capstone project is to build a Natural Language Processing
(NLP) application that, given a chunk of text, predicts the next most probable
word. The application may be used, for example, in mobile devices to provide
suggestions as the user enters text, or as a part of a spelling correction
module in a text editor.

You may find the following links useful:

* Short version of this document: https://serdioa.github.io/DataScienceCapstone/ImplementationReportShort.html

* Presentation: https://serdioa.github.io/DataScienceCapstone/presentation/index.html

* Online application: https://serdioa.shinyapps.io/predict-sb

* Source code: https://github.com/serdioa/DataScienceCapstone

# <a name="model"></a>Model

We are going to predicts the next word in a text given a prefix by using a
Markov Chain model simplified to n-grams.

The Markov Chain model assumes that in a natural language sentence
a probability of each word depends only on previous words. The n-gram model
simplifies the Markov Chain model by considering each word to depend only on
the previous N words, thus ignoring long-range dependency. The n-gram model
combines simplicity with an acceptable prediction quality, making it a model
of choice for many Natural Language Processing (NLP) applications.

# <a name="download_and_split"></a>Downloading and splitting the corpus

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(hms)
library(kableExtra)
```

Before we start discussing the application in detail, there is a short general
consideration. We decided to not include R script sources as the part of this
document, but to keep them in separate script files which we reference.
Moreover, many steps done when preparing the data and tuning meta-parameters
require many hours to run. It would be impossible to re-run all steps each time
when we re-render this document. R Markdown provides some possibilities to cache
intermediate results, but they are not adequate to our task, since we were not
able to keep in memory all intermediate results at once. Hence our approach was
to process the data piecewise, explicitly caching intermediate results.

Coursera provides a training text corpora HC Corpora [(1)](#hc_corpora). The
corpora contains texts in several languages collected from various sources in
Web, including blogs, news web sites and Twitter. The English corpora consists
of approximately 4.2 millions lines of text.

We start by downloading and unzipping the text corpora. The downloaded data is
stored in the directory "cache".

```{r eval=FALSE}
source("include/download.R")
download.data()
```

The downloaded zip file contains corpora in several languages: English, German,
Russian and Finnish. In our project we will use only English corpora.

Corpora in each language, including English, contains 3 files with content
obtained from different sources: news, blogs and twitter.

As the first step, we split each file from the English corpora on 3 parts:

* Training set (60%) will be used to build and train the algorithm.
* Testing set (20%) will be used to test the algorithm during it's development
and tune meta-parameters. This set may be used more than once.
* Validation set (20%) will be used for a final validation and estimation
of out-of-sample prediction quality This set will be used only once.

```{r eval=FALSE}
source("include/split.R")
set.seed(20190530)
split.all()
```

As a sanity check, we count a number of lines in each source file, as well in
the partial files produced by the split.

```{r countLines, echo=FALSE, message=FALSE, cache=TRUE}
count.blogs <- R.utils::countLines(file.path("cache", "final", "en_US", "en_US.blogs.txt"))
count.blogs.training <- R.utils::countLines(file.path("cache", "en_US.blogs.training.txt"))
count.blogs.testing <- R.utils::countLines(file.path("cache", "en_US.blogs.testing.txt"))
count.blogs.validation <- R.utils::countLines(file.path("cache", "en_US.blogs.validation.txt"))

count.news <- R.utils::countLines(file.path("cache", "final", "en_US", "en_US.news.txt"))
count.news.training <- R.utils::countLines(file.path("cache", "en_US.news.training.txt"))
count.news.testing <- R.utils::countLines(file.path("cache", "en_US.news.testing.txt"))
count.news.validation <- R.utils::countLines(file.path("cache", "en_US.news.validation.txt"))

count.twitter <- R.utils::countLines(file.path("cache", "final", "en_US", "en_US.twitter.txt"))
count.twitter.training <- R.utils::countLines(file.path("cache", "en_US.twitter.training.txt"))
count.twitter.testing <- R.utils::countLines(file.path("cache", "en_US.twitter.testing.txt"))
count.twitter.validation <- R.utils::countLines(file.path("cache", "en_US.twitter.validation.txt"))

corpora.count <- data.frame(
  "blogs" = c(count.blogs.training,
              count.blogs.testing,
              count.blogs.validation,
              count.blogs,
              count.blogs - (count.blogs.training +
                             count.blogs.testing +
                             count.blogs.validation)),
  "blogs.pct" = c(count.blogs.training / count.blogs * 100,
                  count.blogs.testing / count.blogs * 100,
                  count.blogs.validation / count.blogs * 100,
                  count.blogs / count.blogs * 100,
                  NA),
  "news" = c(count.news.training,
             count.news.testing,
             count.news.validation,
             count.news,
             count.news - (count.news.training +
                           count.news.testing +
                           count.news.validation)),
  "news.pct" = c(count.news.training / count.news * 100,
                 count.news.testing / count.news * 100,
                 count.news.validation / count.news * 100,
                 count.news / count.news * 100,
                 NA),
  "twitter" = c(count.twitter.training,
                count.twitter.testing,
                count.twitter.validation,
                count.twitter,
                count.twitter - (count.twitter.training +
                                 count.twitter.testing +
                                 count.twitter.validation)),
  "twitter.pct" = c(count.twitter.training / count.twitter * 100,
                    count.twitter.testing / count.twitter * 100,
                    count.twitter.validation / count.twitter * 100,
                    count.twitter / count.twitter * 100,
                    NA)
)
rownames(corpora.count) <- c("Training", "Testing", "Validation", "Total",
                             "Control (expected to be 0)")
kable(corpora.count,
      booktabs = TRUE,
      col.names = c("Rows", "%", "Rows", "%", "Rows", "%")) %>%
    add_header_above(header = c("",
                                "Blogs" = 2,
                                "News" = 2,
                                "Twitter" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

As the table shows, we have splitted the data on sub-sets as intended.

# <a name="cleaning_preprocessing"></a>Cleaning and preprocessing the corpus

We decided to split text on sentences and do not attempt to predict words across
sentence border. We still may use information about sentences to improve
prediction of the first word, because the frequency of the first word in a
sentence may be very different from an average frequency.

We apply the following cleaning and pre-processing steps:

* Remove URLs, e-mail addresses and Twitter hash tags.

* Remove underscroll characters which sometimes are used as emphasys or to
replace a profanity.

* Add missing space characters around punctuation characters as the preparation
for removing punctuation characters altogether. For example, the corpus contains
a sentence "I had the best day yesterday,it was like two kids in a candy store".
Note that the sentence does not contain a space character after the comma, so
when punctuation characters are removed, words "yesterday,it" are transformed
into a non-existing word "yesterdayit". Adding missing space characters prevents
such error.

* Remove punctuation characters.

* Replace common short forms such as "I'm", with full forms such as "I am".
The list of replacements is available in the file
[replacements.txt](replacements.txt).

* Remove words with non-latin characters. The corpora contains some words and
even whole sentences in languages which are written using non-latin alphabets,
such as Greek or Russian. We keep only words with standard and extended latin
characters (that is, latin characters with various accents), but exclude words
with any other characters.

* Prepend the special token `STOS` (Start-of-Sentence). When building n-grams,
this token will allow us to distinguish n-grams which appear at the start of
sentences.

Natural language processing has a notion of [stop words](https://en.wikipedia.org/wiki/Stop_words),
that is words which occure very often, and are often filtered out during
processing. It is not obvious on this step if we should include or ignore stop
words for the purposes of our task, so we decided to continue in parallel with
both approaches. Thus our pre-processing function has one more optional step
which removes stop words.

```{r eval=FALSE}
source("include/preprocess.R")
preprocess.all(removeStopwords = FALSE)
preprocess.all(removeStopwords = TRUE)
```

On our hardware (Intel i5-7600K) commands above runs several minutes.

# <a name="building_n_grams"></a>Building n-grams

In the previous section we have pre-processed the corpora: removed URLs and
e-mail addresses, get rid of the punctuation etc. Our next task is to build
n-grams and corresponding frequency tables.

An important question is which library to use for processing and analyzing the
corpora, as R provides several alternatives. Initially we attempted to use
the library `tm`, but quickly found that the library is very memory-hungry,
and an attempt to build bi- or trigrams for a large corpus are not practical.
After some googling we decided to use the library `quanteda` instead.

When building n-grams, we apply the following principles:

* We build 1-grams to 5-grams.

* In each n-gram we distinguish the prefix, that is the first (n-1) words,
from the suffix, that is the last word. As the special case, 1-grams have
an empty prefix (`NA`). We treat words in the prefix and the suffix differently
as described below.

* For the prefix, we keep only stems of words. For stemming we use the function
`SnowballC::wordStem()`. Keeping only stems reduces the size of our tables,
and allows to better use word patterns which otherwise may be not found due
to slightly different word forms.

* For the suffix (the last word of an n-gram) we keep the word "as is" without
any transformations.

* We build the list of top-frequency $2^{16}-2$ stems and keep only those 
stems in n-gram prefixes. All other stems which appear less often are replaced
in prefixes by the special token `UNK` (Unknown). The number $2^{16}-2$ allows
us to represent each stem which may appear in an n-gram prefix, as well as
2 special tokens `STOS` (Start-of-Sentence) and `UNK` (Unknown) with a number
between $0$ and $2^{16}-1$. Reasons for this will be explained in the section
[Binary encoding for stems](#binary_encoding).

```{r eval=FALSE}
source("include/ngram.build.R")
for (n in 1:5) {
  ngram.freq.cache(n, removeStopwords = FALSE)
  ngram.freq.cache(n, removeStopwords = TRUE)
}
```

On our hardware (Intel i5-7600K) commands above runs approximately 6 hours.

As a result of this step, for 1- to 5-grams we obtain frequency tables with
the columns `Term` (the n-gram $w_1,\ldots,w_n$) and `Freq` (the number of times
$c(w_1,\ldots,w_n)$ this n-gram appears in the aggregated corpus).

For the purpose of our task, that is prediction of the next word, we require
the frequency table to be transformed. For each n-gram $w_1,\ldots,w_n$, the
transformed table shall contain:

* `Prefix` - the first $n-1$ words of the n-gram, that is $w_1,\ldots, w_{n-1}$.
1-grams have an empty prefix `NA`.

* `Suffix` - the last word of the n-gram, that is $w_n$.

* `Prob` - maximum likehood estimate of the conditional probability of the
`Suffix` $w_n$ given the `Prefix` $w_1,\ldots,w_{n-1}$

$$P_{ML}(w_n | w_1,\ldots,w_{n-1}) = \frac{c(w_1,\ldots,w_n)}
{\sum c(w_1,\ldots,w_{n-1},\bullet)}$$

The sum in the denominator is over all n-grams sharing the same `Prefix`
$w_1,\ldots,w_{n-1}$.

```{r eval=FALSE}
source("include/ngram.build.R")
for (n in 1:5) {
  ngram.extended.cache(n, removeStopwords = FALSE)
  ngram.extended.cache(n, removeStopwords = TRUE)
}
```

On our hardware (Intel i5-7600K) commands above runs a bit more than 1 hour
and use slightly over 30 GB of memory.

The following table shows number of rows and memory usage of n-gram tables
with and without stop words.

```{r echo=FALSE, message=FALSE}
source("include/stat.R")

stat.ngram.extended.sw <- stat.ngram.extended.all.cache(removeStopwords = FALSE)
stat.ngram.extended.nosw <- stat.ngram.extended.all.cache(removeStopwords = TRUE)
stat.ngram.extended <- data.frame(
    N = stat.ngram.extended.sw$N,
    Rows.Sw = formatC(stat.ngram.extended.sw$Rows, big.mark = ",", format = "d"),
    Size.Sw = formatC(stat.ngram.extended.sw$Size, big.mark = ",", format = "f", digits = 1),
    Rows.Nosw = formatC(stat.ngram.extended.nosw$Rows, big.mark = ",", format = "d"),
    Size.Nosw = formatC(stat.ngram.extended.nosw$Size, big.mark = ",", format = "f", digits = 1)
)
stat.ngram.extended.total <- data.frame(
    N = "Total:",
    Rows.Sw = "",
    Size.Sw = formatC(sum(stat.ngram.extended.sw$Size), big.mark = ",", format = "f", digits = 1),
    Rows.Nosw = "",
    Size.Nosw = formatC(sum(stat.ngram.extended.nosw$Size), big.mark = ",", format = "f", digits = 1)
)
stat.ngram.extended.tbl <- rbind(stat.ngram.extended, stat.ngram.extended.total)

kable(stat.ngram.extended.tbl,
      booktabs = TRUE,
      col.names = c("N", "Rows", "Size, MiB", "Rows", "Size, MiB"),
      align=c("l", rep("r", times=4))) %>%
    add_header_above(header = c("",
                                "With stop words" = 2,
                                "Without stop words" = 2)) %>%
    row_spec(6, bold = T, background = "#C0C0C0") %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)

```

As the table shows, tables are very large and do not satisfy requirements for a
Shiny application (max. 1 GB). Besides, with very large tables we may expect
a large latency. In the next sections we will reduce the size of n-gram tables.

# <a name="binary_encoding"></a>Binary encoding for stems

As we have found in the previous section, our n-gram tables are very large. A
very significant part of the memory usage is due to the prefixes. We decided to
reduce the memory usage by using a binary encoding for the prefixes.

Although R has the data type `raw` which works directly with bytes, the storage
of this data type is very inefficient, besides, in our experience it is very
inconvenient to use as a column in a data frame. We decided to encode stems
as R data type `integer` (4 bytes) and `numeric` (8 bytes) instead.

Theoretically the [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding)
may have been used to compress the data and reduce the storage size, but in our
case it is not practical: since our prefixes contains up to 4 stems, and we
want to encode $2^{16}$ tokens, even by using the Huffmann encoding we will not
be able to compress each prefix under 4 bytes, so we have to use 8-bytes storage
anyway. To simplify the implementation, we decided for a simple non-compressing
encoding instead.

* Each of top-frequency $2^{16}-2$ stems, as well as 2 special tokens `STOS`
(Start-of-Sentence) and `UNK` (Unknown) is assigned a code from $0$ to
$2^{16}-1$ which may be represented as 2 bytes.

* For 1-grams we do not require any encoding at all, because the prefix is
always `NA`. Remember, the prefix is the first (n-1) words, that is $0$ words
for 1-grams.

* For 2-grams the prefix contains just 1 stem, so the encoding is just the code
of the stem (4-bytes `integer`).

* For 3-grams the prefix contains 2 stems. We transform their codes to 2 bytes
each, concatenate these bytes into a 4-byte sequence, and interpret the sequence
as a 4-bytes `integer`.

* For 4-grams the prefix contains 3 stems. We transform their codes
to 2 bytes each, concatenate these bytes together with two `0x00`-bytes into a
8-bytes sequence, and interpret the sequence as an 8-bytes `numeric`.

* For 5-grams the prefix contains 4 stems. We use the same approach as for
4-grams, except that we do not need to add `0x00`-bytes since we already have
8 bytes from prefix codes.

Let us walk through several examples. We will encode the 4-gram "the answer to
that", as well as it's 1-, 2- and 3-word prefixes.

First, let us lookup codes for each stem in our 4-gram:
```{r message=FALSE}
source("include/ngram.encode.R")
dict.stems <- dict.stems.cache()
dict.hash <- dict.hash.cache()
dict.hash[[c("the", "answer", "to", "that")]]
```

The corresponding hexadecimal codes are `0x0002`, `0x026B`, `0x0003`, `0x000B`.

We start by encoding and decoding the first word, "the". For a 1-gram an
encoded value is just an `integer` equal to the code, that is the encoded value
is 2.

```{r}
enc.1 <- tokens.encode.1(c("the"), dict.hash)
enc.1
tokens.decode.1(enc.1, dict.stems)
```

We continue with the 2-gram "the answer". Concatenated hexadecimal codes are
`0x0002026B`. Interpreted as an `integer` (little-endian), that gives us
1795293696.

```{r}
enc.2 <- tokens.encode.2(c("the", "answer"), dict.hash)
enc.2
tokens.decode.2(enc.2, dict.stems)
```

Next we encode the 3-gram "the answer to". We concatenate the hexadecimal codes
and append two `0x00` bytes to get 8 bytes in total to obtain
`0x0002026B00030000`. Interpreted as a `numeric` (little-endian), that gives
us 1.6305797604007e-311. Note that the decimal notation is not precise enough:
to correctly decode the value, you require absolutely exact value which preserves
every single bit.

```{r}
enc.3 <- tokens.encode.3(c("the", "answer", "to"), dict.hash)
enc.3
tokens.decode.3(enc.3, dict.stems)
```

Finally, we encode the 4-gram "the answer to that". We concatenate
the hexadecimal codes to obtain 8-bytes sequence `0x0002026B0003000B`, and
interpret it as a `numeric` (little-endian) 1.0663795695223075e-255. The same
note about the decimal notation as for the 3-gram applies here as well.

```{r}
enc.4 <- tokens.encode.4(c("the", "answer", "to", "that"), dict.hash)
enc.4
tokens.decode.4(enc.4, dict.stems)
```

We will use the binary encoding of prefixes in the next section to reduce the
size of n-gram tables.

# <a name="optimizing_tables"></a>Optimizing n-gram tables

In the section [Building n-grams](#building_n_grams) we built n-gram tables,
and found that the tables are too large. In the previous section
[Binary encoding for stems](#binary_encoding) we have developed a method to
encode prefixes of n-grams either as `integer` or as `numeric`, dependent on
a size of the prefix.

Let us apply the encoding to prefixes of our n-gram tables. We also use this
opportunity to remove the column `Prefix` from 1-grams since it is always `NA`
anyway.

```{r eval=FALSE}
source("include/ngram.optimize.R")
for (n in 1:5) {
  ngram.optimize.prefix.cache(n, removeStopwords = FALSE)
  ngram.extended.prefix.cache(n, removeStopwords = TRUE)
}
```

On our hardware (Intel i5-7600K) commands above runs a bit less than 4 hours.

```{r echo=FALSE, message=FALSE}
stat.ngram.extended.sw <- stat.ngram.extended.all.cache(removeStopwords = FALSE)
stat.ngram.extended.nosw <- stat.ngram.extended.all.cache(removeStopwords = TRUE)
stat.ngram.opt.pref.sw <- stat.ngram.optimize.prefix.all.cache(removeStopwords = FALSE)
stat.ngram.opt.pref.nosw <- stat.ngram.optimize.prefix.all.cache(removeStopwords = TRUE)
```

The following table shows size of n-gram tables with encoded prefixes compared
to original tables. The improvement is more pronounced for larger n.
For instance, the size of the 5-gram table with stop words is reduced from
`r stat.ngram.extended.sw$Size[5]` MiB to `r stat.ngram.opt.pref.sw$Size[5]`
MiB, or just to `r sprintf("%.2f%%", stat.ngram.opt.pref.sw$Size[5] / stat.ngram.extended.sw$Size[5] * 100)` of the former size.
 
```{r echo=FALSE, message=FALSE}
stat.ngram.opt.pref <- data.frame(
    N = stat.ngram.extended.sw$N,
    Old.Rows.Sw = formatC(stat.ngram.extended.sw$Rows, big.mark = ",", format = "d"),
    Old.Size.Sw = formatC(stat.ngram.extended.sw$Size, big.mark = ",", format = "f", digits = 1),
    New.Size.Sw = formatC(stat.ngram.opt.pref.sw$Size, big.mark = ",", format = "f", digits = 1),
    Diff.Size.Sw = sprintf("%.2f",
                           stat.ngram.opt.pref.sw$Size /
                             stat.ngram.extended.sw$Size * 100.0),
    Old.Rows.Nosw = formatC(stat.ngram.extended.nosw$Rows, big.mark = ",", format = "d"),
    Old.Size.Nosw = formatC(stat.ngram.extended.nosw$Size, big.mark = ",", format = "f", digits = 1),
    New.Size.Nosw = formatC(stat.ngram.opt.pref.nosw$Size, big.mark = ",", format = "f", digits = 1),
    Diff.Size.Nosw = sprintf("%.2f",
                             stat.ngram.opt.pref.nosw$Size /
                               stat.ngram.extended.nosw$Size * 100.0)
)
stat.ngram.opt.pref.total <- data.frame(
    N = "Total:",
    Old.Rows.Sw = "",
    Old.Size.Sw = formatC(sum(stat.ngram.extended.sw$Size), big.mark = ",", format = "f", digits = 1),
    New.Size.Sw = formatC(sum(stat.ngram.opt.pref.sw$Size), big.mark = ",", format = "f", digits = 1),
    Diff.Size.Sw = sprintf("%.2f",
                           sum(stat.ngram.opt.pref.sw$Size) /
                             sum(stat.ngram.extended.sw$Size) * 100.0),
    Old.Rows.Nosw = "",
    Old.Size.Nosw = formatC(sum(stat.ngram.extended.nosw$Size), big.mark = ",", format = "f", digits = 1),
    New.Size.Nosw = formatC(sum(stat.ngram.opt.pref.nosw$Size), big.mark = ",", format = "f", digits = 1),
    Diff.Size.Nosw = sprintf("%.2f",
                             sum(stat.ngram.opt.pref.nosw$Size) /
                               sum(stat.ngram.extended.nosw$Size) * 100)
)
stat.ngram.opt.pref.tbl <- rbind(stat.ngram.opt.pref, stat.ngram.opt.pref.total)

kable(stat.ngram.opt.pref.tbl,
      booktabs = TRUE,
      col.names = 
        c("N", "Rows", "Original Size, MiB", "Optimized Size, MiB", "Optimized vs. Original, %",
               "Rows", "Original Size, MiB", "Optimized Size, MiB", "Optimized vs. Original, %"),
      align=c("l", rep("r", times=8))) %>%
    add_header_above(header = c("",
                                "With stop words" = 4,
                                "Without stop words" = 4)) %>%
    row_spec(6, bold = T, background = "#C0C0C0") %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)

```

Another opportunity for reducing the required memory is the column `Prob` which
contains the maximum likehood estimate of the conditional probability of the
`Suffix` given the `Prefix`. Instead of storing it as a `numeric` column
(8 bytes for each value) we may squeeze it into the `integer` (4 bytes) using
the scaled log approach. The easiest way to explain this approach is with an
example:

```{r echo=FALSE, message=FALSE}
#ngram.ext.full.2 <- ngram.extended.full.cache(2)
#ngram.ext.full.2.at <- sum((ngram.ext.full.2 %>% filter(Prefix == "at"))$Freq)
#ngram.ext.full.2.at.the <- (ngram.ext.full.2 %>% filter(Prefix == "at" & Suffix == "The"))$Freq
ngram.ext.full.2.at <- 330145
ngram.ext.full.2.at.the <- 2035
ngram.ext.full.2.at.the.p <- ngram.ext.full.2.at.the / ngram.ext.full.2.at
ngram.ext.full.2.at.the.p.log <- log(ngram.ext.full.2.at.the.p)
ngram.ext.full.2.at.the.p.log.int <- as.integer(ngram.ext.full.2.at.the.p.log * 1000000)
ngram.ext.full.2.at.the.p.back = exp(ngram.ext.full.2.at.the.p.log.int / 1000000)
ngram.ext.full.2.at.the.p.diff = ngram.ext.full.2.at.the.p.back - ngram.ext.full.2.at.the.p
ngram.ext.full.2.at.the.p.diff.rel = ngram.ext.full.2.at.the.p.diff / ngram.ext.full.2.at.the.p
```

* There are `r formatC(ngram.ext.full.2.at, big.mark = ",", format = "d")`
2-grams starting with the word "at". In
`r formatC(ngram.ext.full.2.at.the, big.mark = ",", format = "d")` cases the
word "at" is followed by the word "the", giving the 2-gram "at the". We
calculate the maximum likehood estimate of the probability of the suffix "the"
given the prefix "at" as

$$P_{ML}(\text{the} | \text{at}) = \frac{c(\text{at the})}
{\sum c(\text{at } \bullet)} = \frac{2035}{330145} \approx 0.006163958$$

* We calculate the natural logarithm of the value: $log(P_{ML}(\text{the} | \text{at})) =$
`r ngram.ext.full.2.at.the.p.log`.

* We multiply the logarithm by $10^6$ and get an integer part of the result.
Multiplying by $10^6$ ensures that we preserve 6 digits after the decimal point
in the original logarithm. In our example we get
`r ngram.ext.full.2.at.the.p.log.int`. This is the value we store in our n-gram
table as an `integer`, saving 4 bytes.

* When we require the original value, we inverse the transformation, dividing
the optimized value by $10^6$ and taking the exponent of it. Continuing our
example, exp(`r ngram.ext.full.2.at.the.p.log.int` / $10^6$) =
`r sprintf("%.9f", ngram.ext.full.2.at.the.p.back)`.

* By rounding to `integer` we lost some precision, but the difference is just
`r sprintf("%.10f", ngram.ext.full.2.at.the.p.diff)`, or
`r sprintf("%.8f%%", ngram.ext.full.2.at.the.p.diff.rel)` of the original value,
which is negligible for our purposes.

* At the price of very slight precision loss we could store our values as an
`integer` (4 bytes per value) instead of a `numeric` (8 bytes).

```{r eval=FALSE}
for (n in 1:5) {
  ngram.optimize.prob.cache(n, removeStopwords = FALSE)
  ngram.extended.prob.cache(n, removeStopwords = TRUE)
}
```

On our hardware (Intel i5-7600K) commands above runs approximately 5 minutes.

```{r echo=FALSE, message=FALSE}
stat.ngram.opt.prob.sw <- stat.ngram.optimize.prob.all.cache(removeStopwords = FALSE)
stat.ngram.opt.prob.nosw <- stat.ngram.optimize.prob.all.cache(removeStopwords = TRUE)
```

```{r echo=FALSE, message=FALSE}
stat.ngram.opt.prob.sw <- stat.ngram.optimize.prob.all.cache(removeStopwords = FALSE)
stat.ngram.opt.prob.nosw <- stat.ngram.optimize.prob.all.cache(removeStopwords = TRUE)
```

The folowing table shows size of n-gram tables with probabilities optimized for
low memory consumption compared to the previous version with encoded prefixes.
The size reduction is not as significant as in the previous step.
For instance, the total size of n-gram tables with stop words is reduced from
`r sum(stat.ngram.opt.pref.sw$Size)` MiB to `r sum(stat.ngram.opt.prob.sw$Size)`
MiB, or to
`r sprintf("%.2f%%", sum(stat.ngram.opt.prob.sw$Size) / sum(stat.ngram.opt.pref.sw$Size) * 100.0)`
of the former size.

```{r echo=FALSE, message=FALSE}
stat.ngram.opt.prob <- data.frame(
    N = stat.ngram.extended.sw$N,
    Old.Rows.Sw = formatC(stat.ngram.extended.sw$Rows, big.mark = ",", format = "d"),
    Old.Size.Sw = formatC(stat.ngram.opt.pref.sw$Size, big.mark = ",", format = "f", digits = 1),
    New.Size.Sw = formatC(stat.ngram.opt.prob.sw$Size, big.mark = ",", format = "f", digits = 1),
    Diff.Size.Sw = sprintf("%.2f",
                           stat.ngram.opt.prob.sw$Size /
                             stat.ngram.opt.pref.sw$Size * 100.0),
    Old.Rows.Nosw = formatC(stat.ngram.extended.nosw$Rows, big.mark = ",", format = "d"),
    Old.Size.Nosw = formatC(stat.ngram.opt.pref.nosw$Size, big.mark = ",", format = "f", digits = 1),
    New.Size.Nosw = formatC(stat.ngram.opt.prob.nosw$Size, big.mark = ",", format = "f", digits = 1),
    Diff.Size.Nosw = sprintf("%.2f",
                             stat.ngram.opt.prob.nosw$Size /
                               stat.ngram.opt.pref.nosw$Size * 100.0)
)
stat.ngram.opt.prob.total <- data.frame(
    N = "Total:",
    Old.Rows.Sw = "",
    Old.Size.Sw = formatC(sum(stat.ngram.opt.pref.sw$Size), big.mark = ",", format = "f", digits = 1),
    New.Size.Sw = formatC(sum(stat.ngram.opt.prob.sw$Size), big.mark = ",", format = "f", digits = 1),
    Diff.Size.Sw = sprintf("%.2f",
                           sum(stat.ngram.opt.prob.sw$Size) /
                             sum(stat.ngram.opt.pref.sw$Size) * 100.0),
    Old.Rows.Nosw = "",
    Old.Size.Nosw = formatC(sum(stat.ngram.opt.pref.nosw$Size), big.mark = ",", format = "f", digits = 1),
    New.Size.Nosw = formatC(sum(stat.ngram.opt.prob.nosw$Size), big.mark = ",", format = "f", digits = 1),
    Diff.Size.Nosw = sprintf("%.2f",
                             sum(stat.ngram.opt.prob.nosw$Size) /
                               sum(stat.ngram.opt.pref.nosw$Size) * 100)
)
stat.ngram.opt.prob.tbl <- rbind(stat.ngram.opt.prob, stat.ngram.opt.prob.total)

kable(stat.ngram.opt.prob.tbl,
      booktabs = TRUE,
      col.names = 
        c("N", "Rows", "Original Size, MiB", "Optimized Size, MiB", "Optimized vs. Original, %",
               "Rows", "Original Size, MiB", "Optimized Size, MiB", "Optimized vs. Original, %"),
      align=c("l", rep("r", times=8))) %>%
    add_header_above(header = c("",
                                "With stop words" = 4,
                                "Without stop words" = 4)) %>%
    row_spec(6, bold = T, background = "#C0C0C0") %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The total memory usage is still above our maximum allowed size of 1 GB.
To achieve it, we have to find another ways to reduce the tables with n-grams.
But before we do it, let us discuss our prediction algorithm and run some tests.

# <a name="stupid_backoff_algorithm"></a>Stupid Backoff algorithm

The prediction algorithm used by our application consists of 2 steps:
choosing candidates, and scoring candidates to select the best matches.

We decided to use very simple choosing algorithm: we choose all available
candidates, proceeding from 5-grams to 1-grams and skipping suffixes which we
have already collected. More precisely, given the input text
$w_1, w_2,\ldots,w_{n-1}$, we proceed as follows:

* Choose 5-gram candidates which starts with the 4-gram prefix
$w_{n-4}, w_{n-3}, w_{n-2}, w_{n-1}$.

* Choose 4-gram candidates which starts with the 3-gram prefix
$w_{n-3}, w_{n-2}, w_{n-1}$, *except* those which suffix already appeared
in choosen 5-grams.

* The same for 3- and 2-grams.

* Our implementation allows to choose the number of candidates to show. If
at this step we have found less candidates than requested, choose missing
candudates from the most frequent 1-grams (remember, 1-grams do not have
a prefix).

Note that we do not stop once we have choosen the required number of candidates,
but continue to n-grams for lower n. Suppose that we were requested to predict
3 most probable words, and there are exactly 3 5-grams with the appropriate
prefix. If we stop choosing candidates at this moment, we may miss 4-gram which
produces a prediction with higher score than any 5-gram.

The scoring algorithm selects best N matches from all candidates by calculating
a numerical score of each candidate and choosing N candidates with top score.
The score may be a probability, but it may be a different type of a numerical
quantifier, as it is the case for the Stupid Backoff algorithm.

The Stupid Backoff algorithm is a high-efficient scoring
algorithm proposed in 2007 by Thorsten Brants et al [(2)](#stupid_backoff).
On large data sets the
algorithm gives scoring close to [Kneser-Ney algorithm](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing),
but is significantly faster. The Stupid Backoff algorithm returns not
probabilities, but relative scores of words (they do not sum to 1), which is
sufficient for our purposes.

The Stupid Backoff algorithm is described by the following formula:

$$SB(w_n|w_1, w_2,\ldots,w_{n-1}) = \begin{cases}
P_{ML}(w_n|w_1, w_2,\ldots,w_{n-1}) \text{ if } P_{ML}(w_n|w_1, w_2,\ldots,w_{n-1}) > 0
\\
\lambda \; SB(w_n|w_2,\ldots,w_{n-1}) \text{ otherwise}
\end{cases}$$

where $P_{ML}(w_n|w_1, w_2,\ldots,w_{n-1})$ is the maximum likehood estimation
of the conditional probability of the word $w_n$ given the prefix
$w_1, w_2,\ldots,w_{n-1}$. Authors of the Stupid Backoff algorithm recommend to
use $\lambda = 0.4$.

In other words, first we attempt to look up the n-gram in the table for the
largest n available. If the n-gram is found, than the score of the last word
is the maximum likehood estimation of the conditional probability of the last
word given the prefix. Otherwise, we back off (hence the algorithm name) to a
table with (n-1)-grams, do the same calculations and multiply the result by
$\lambda = 0.4$. If the shorterned prefix is not found as well, the recursion
goes deeper, concluding on 1-grams.

Note that our n-gram tables already contains the required maximum likehood
estimation (MLE), so our calculation of the score is trivial:

* If the choosen candidate is from the 5-gram table, return it's MLE as a score.

* If the choosen candidate is from the 4-gram table, return it's MLE multiplied
by $\lambda = 0.4$.

* If the choosen candidate is from the 3-gram table, return it's MLE multiplied
by $\lambda^2 = 0.4^2 = 0.16$.

* The same for 2- and 1-grams, each time multiplying by $\lambda$ one more time.

To better understand the algorithm, we will walk through an example. We want to
predict the next word after the sequence "you pay to get".

* We lookup up the 4-word prefix "you pay to get" in the table with 5-grams.
We find there 3 matches, the score for each of them equals to the probability:

```{r echo=FALSE, message=FALSE}
predict.example.5 <- data.frame(
  Prefix = rep("you pay to get", 3),
  Suffix = c("in", "published", "there"),
  Prob = c(0.5, 0.25, 0.25),
  Score = c(0.5, 0.25, 0.25)
)

kable(predict.example.5,
      booktabs = TRUE,
      col.names = c("Prefix", "Suffix", "Probability", "Score"),
      align=c("l", "l", "r", "r")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

We add all these matches to the list of candidates.

* We look up the 3-word prefix "pay to get" in the table with 4-grams. We find
22 matches, the score for each of them equals the probability multiplied by
$\lambda = 0.4$. The table below lists the top 5 of the candidates.

```{r echo=FALSE, message=FALSE}
predict.example.4 <- data.frame(
  Prefix = rep("pay to get", 5),
  Suffix = c("in", "the", "a", "my", "their"),
  Prob = c(0.143, 0.114, 0.0857, 0.0857, 0.0571),
  Score = c(0.143, 0.114, 0.0857, 0.0857, 0.0571) * 0.4
)

kable(predict.example.4,
      booktabs = TRUE,
      col.names = c("Prefix", "Suffix", "Probability", "Score"),
      align=c("l", "l", "r", "r")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

The first suffix, "in", is already in our list, since we have already found it
in the table with 5-grams. The next suffix, "the", is not in our list yet, so
we add it. We continue the same way for all 4-grams, adding suffixes which are
not in our list yet.

* Although we have already found 22 candidates, we continue to 3-grams, because
it could happens that some 3-gram candidate will get a score high enough to get
into the top list. We look up the 2-word prefix "to get" in the table with
3-grams. We find more than 3000 matches, the score for each of them equals the probability multiplied by $\lambda^2 = 0.4^2 = 0.16$. The table below lists the top 5 of them.

```{r echo=FALSE, message=FALSE}
predict.example.3 <- data.frame(
  Prefix = rep("to get", 5),
  Suffix = c("a", "the", "to", "out", "back"),
  Prob = c(0.0858, 0.0700, 0.0508, 0.0338, 0.0334),
  Score = c(0.0858, 0.0700, 0.0508, 0.0338, 0.0334) * 0.16
)

kable(predict.example.3,
      booktabs = TRUE,
      col.names = c("Prefix", "Suffix", "Probability", "Score"),
      align=c("l", "l", "r", "r")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

We add to our list candidates which are not present it in yet.

* We repeat the same step with 2-grams, looking up 1-word prefix "get". We will
find nearly 10.000 matches, the score for each of them equals the probability
multiplied by $\lambda^3 = 0.4^3 = 0.064$. We add to our lists candidates which
are not present it in yet.

* If we already have enough candidates (which is the case, unless we want to
find more than 10.000 top-score candidates), we do not need to get context-free
candidates from the 1-gram table.

* Suppose our goal is to find the top 10 candidates. We sort our list by score
in the descending order, and select the top 10 rows:

```{r echo=FALSE, message=FALSE}
predict.example.all <- data.frame(
  Prefix = c(rep("you pay to get", 3), rep("pay to get", 7)),
  Suffix = c("in", "published", "there", "the", "a",
  "my", "their", "out", "inside", "past"),
  Prob = c(0.5000, 0.2500, 0.2500, 0.1142, 0.08571,
           0.0857, 0.0571, 0.0571, 0.0285, 0.02857),
  Score = c(0.5000, 0.2500, 0.2500, 0.0457, 0.0342,
            0.0342, 0.0228, 0.0228, 0.0114, 0.0114)
)

kable(predict.example.all,
      booktabs = TRUE,
      col.names = c("Prefix", "Suffix", "Probability", "Score"),
      align=c("l", "l", "r", "r")) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

# <a name="algorithm_extension"></a>Extension of the algorithm

We added to the base algoritm a simple optional extension which allows to
predict a word the user is currently typing. As long as the last typed character
is not the space character, the algorithm assumes that the user continues to
type the current word, and predicts it. Only after a space character is typed
the algorithm predicts the next word.

The extension pretty easy integrates in the step when we choose candidate words.
Given the prefix $w_1,w_2,\ldots,w_{n-1}$ and a partially typed word
$w^\prime_n$ we choose all 5-gram candidates which starts with the prefix
$w_1,w_2,\ldots,w_{n-1}$ and the 5th word starts with the word prefix
$w^\prime_n$, and similarly for 4- to 1-grams.

Some words may be missing in our dictionary, and the user may misspell some
words. If we can't find any candidates using our n-grams, we attempt to predict
the next word using the partialy entered word by applying a spelling correction
algorithm provided by the R package `hunspell`. For example, if the user enters
a misspelled word "mashine", we propose the spell-corrected word "machine".

# <a name="testing_algorithm"></a>Testing algorithm

Now we are ready to test our algorithm for predicting the next word using 20%
of the data [we have saved for testing before](#download_and_split). We run
tests for each data source, as well as for the aggregated data.

* Choose 100.000 random sample sentences from each source (blogs, news,
Twitter). Split each selection on 100 batches, each of 1000 sentences.

* Create an aggregated test set of 100.000 sentences by choosing 1/3 of
sentences from each source. Split the aggregated test set on batches as well.

* Choose a random word in a sentence, but not the very first word. Use the part
of the sentence before the selected word as a prefix, and attempt to predict
the selected word.

* Run the prediction algorithm for all samples, predicting top 5 candidates.

* For each batch of 1000 sentences, calculate percentage of cases when the
word actually present in the sentence was in top 1, top 3 or top 5 of predicted
candidates.

```{r eval=FALSE}
source("include/predict.test.R")

for (source in c("blogs", "news", "twitter", "all")) {
    set.seed(12345)
    predict.test.sb.cache(source, "testing", n.samples = 100000,
                          threshold = 1, removeStopwords = TRUE)
    
    set.seed(12345)
    predict.test.sb.cache(source, "testing", n.samples = 100000,
                          threshold = 1, removeStopwords = FALSE)
}
```

The meaning of the parameter "threshold" will be explained later, when we will
[optimize meta-parameters](#optimizing_meta_parameters) of the prediction
algorithm. Now it is enough to say that "threshold = 1" uses complete tables
with n-grams we have built before.

On our hardware (Intel i5-7600K) commands above runs a bit more than 7 hours.

The chart below shows the the precision of our prediction algorithm for the
aggregated data sources, as well as separately for blogs, news and Twitter,
with and without stop words. The first chart column shows percentage of correct
predictions (our top candidate is exactly the word present in the original
text). The second and third chart columns show percentage of cases when the
right word was amongst our Top-3 or Top-5 predicted candidates.

```{r threshold.1.chart, echo=FALSE, message=FALSE, fig.width=10}
stat.predict.threshold.1.chart("boxplot")
```

```{r threshold.1.tbl.sw, echo=FALSE, message=FALSE}
stat.predict.threshold.1.tbl.sw <- function() {
  stat.sw <- stat.predict.sources.agg.n.build(1, "testing", removeStopwords = FALSE) %>%
    arrange(Source) %>%
    mutate(Mean = Mean * 100,
           ConfIntLow = ConfIntLow * 100,
           ConfIntHigh = ConfIntHigh * 100)
  
  stat.sw.1 <- stat.sw %>% filter(Rank == 1)
  stat.sw.3 <- stat.sw %>% filter(Rank == 3)
  stat.sw.5 <- stat.sw %>% filter(Rank == 5)

  stat.sw.1.tbl <- data.frame(
    Source = c("Aggregated", "Blogs", "News", "Twitter"),
    Mean.1 = sprintf("%.2f", stat.sw.1$Mean),
    ConfInt.1 = sprintf("(%.2f, %.2f)", stat.sw.1$ConfIntLow, stat.sw.1$ConfIntHigh),
    Mean.3 = sprintf("%.2f", stat.sw.3$Mean),
    ConfInt.3 = sprintf("(%.2f, %.2f)", stat.sw.3$ConfIntLow, stat.sw.3$ConfIntHigh),
    Mean.5 = sprintf("%.2f", stat.sw.5$Mean),
    ConfInt.5 = sprintf("(%.2f, %.2f)", stat.sw.5$ConfIntLow, stat.sw.5$ConfIntHigh),
    stringsAsFactors = FALSE
  )
  
  kable(stat.sw.1.tbl,
        caption = "Prediction precision, with stop words",
        booktabs = TRUE,
        col.names = c("Source", rep(c("Mean, %", "Conf. Int. 95%"), 3))) %>%
    add_header_above(header = c("",
                                "Word correctly predicted" = 2,
                                "Word in Top 3" = 2,
                                "Word in Top 5" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
}

stat.predict.threshold.1.tbl.nosw <- function() {
  stat.sw <- stat.predict.sources.agg.n.build(1, "testing", removeStopwords = TRUE) %>%
    arrange(Source) %>%
    mutate(Mean = Mean * 100,
           ConfIntLow = ConfIntLow * 100,
           ConfIntHigh = ConfIntHigh * 100)
  
  stat.sw.1 <- stat.sw %>% filter(Rank == 1)
  stat.sw.3 <- stat.sw %>% filter(Rank == 3)
  stat.sw.5 <- stat.sw %>% filter(Rank == 5)

  stat.sw.1.tbl <- data.frame(
    Source = c("Aggregated", "Blogs", "News", "Twitter"),
    Mean.1 = sprintf("%.2f", stat.sw.1$Mean),
    ConfInt.1 = sprintf("(%.2f, %.2f)", stat.sw.1$ConfIntLow, stat.sw.1$ConfIntHigh),
    Mean.3 = sprintf("%.2f", stat.sw.3$Mean),
    ConfInt.3 = sprintf("(%.2f, %.2f)", stat.sw.3$ConfIntLow, stat.sw.3$ConfIntHigh),
    Mean.5 = sprintf("%.2f", stat.sw.5$Mean),
    ConfInt.5 = sprintf("(%.2f, %.2f)", stat.sw.5$ConfIntLow, stat.sw.5$ConfIntHigh),
    stringsAsFactors = FALSE
  )
  
  kable(stat.sw.1.tbl,
        caption = "Prediction precision, without stop words",
        booktabs = TRUE,
        col.names = c("Source", rep(c("Mean, %", "Conf. Int. 95%"), 3))) %>%
    add_header_above(header = c("",
                                "Word correctly predicted" = 2,
                                "Word in Top 3" = 2,
                                "Word in Top 5" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
}

stat.predict.threshold.1.tbl.sw()
stat.predict.threshold.1.tbl.nosw()
```

The first important insight from the chart and tables above is that the
prediction based on n-grams with stop words outperforms the same algorithm
based on n-grams without stop words by more than 10%. From now on we continue
development and tuning only using n-grams with stop words.

It is interesting to note that our algorithm predicts the next word for
sentences from the news source best. Blogs and Twitter are very close, with
Twitter slightly better for exact match and slightly worse for Top-3 and Top-5
prediction rating. The likely explanation is that news tend to use the same
expressions more often than less formal blogs and Twitter.

Now, when we know that our algorithm generally works, it's a time to optimize
it.

# <a name="optimizing_meta_parameters"></a>Optimizing meta-parameters

Until now we have used all collected n-grams to predict the next word using
the Stupid Backoff algorithm. As we have calculated, even after 
[all previous optimizations](#optimizing_tables) our n-gram tables are too
large to be used in a Shiny application. Could we remove some n-grams without
losing much prediction power? We are going to answer the question in this
section.

Our n-gram tables contain all collected n-grams, but are all of them equally
useful for our prediction algorithm? For example, the 5-gram "at the end of the"
appears in our corpora 2181 times, whereas the 5-gram
"can't go anywhere with them" appear only once. Intuitively the first n-gram
is much more useful, because the probability to encounter it again in other
texts is much higher than the probability to find a seldom n-gram in another
text.

```{r ngram.coverage.percentage.cache, echo=FALSE, message=FALSE}
ngram.total.size <- function(n) {
  ngram.freq <- ngram.freq.cache(n)
  nrow(ngram.freq)
}

ngram.threshold.size <- function(n, threshold) {
  ngram.freq <- ngram.freq.cache(n)
  ngram.freq %>% filter(Freq <= threshold) %>% nrow()
}

ngram.1.size <- get.var.cache("ngram.1.size", function() ngram.total.size(1),
                              FALSE)
ngram.5.size <- get.var.cache("ngram.5.size", function() ngram.total.size(5),
                              FALSE)

ngram.1.1.size <- get.var.cache("ngram.1.1.size",
                                function() ngram.threshold.size(1, 1),
                                FALSE)
ngram.5.1.size <- get.var.cache("ngram.5.1.size", 
                                function() ngram.threshold.size(5, 1),
                                FALSE)

ngram.1.pct <- ngram.1.1.size / ngram.1.size
ngram.5.pct <- ngram.5.1.size / ngram.5.size
```

In the [Mileston Report](MilestoneReportWeek2.html#3_to_6_grams) we have also
found out that seldom n-grams comprise a very large part of tables for large n.
For example, there are `r formatC(ngram.1.size, big.mark = ",", format = "d")`
distinct 1-grams, and `r formatC(ngram.1.1.size, big.mark = ",", format = "d")`
(`r sprintf("%.2f%%", ngram.1.pct * 100)`) appear just once.
For 5-grams it looks even more dramatic: our corpora contains
`r formatC(ngram.5.size, big.mark = ",", format = "d")` distinct 5-grams,
and `r formatC(ngram.5.1.size, big.mark = ",", format = "d")`
(`r sprintf("%.2f%%", ngram.5.pct * 100)`) appear just once.
Removing from our tables entries which appear just once could significantly
reduce the memory usage.

We will tell that the n-gram table has a *threshold* $m$, if we keep in the
table only entries which appear in the corpus at least $m$ times and remove the
rest. For example, a n-gram table with the threshold 3 contains only n-grams
which appear at least 3 times. Obviously, the complete table with all available
n-grams is a table with the threshold 1.

To analyze how removing low-frequency n-grams affects precision of our
prediction algorithm, we build n-gram tables with thresholds 2 to 6, and run
our prediction algorithm with these tables. Our approach is the same as in the
previous section: for each source and threshold we process 100 batches of 1.000
samples each.

```{r eval=FALSE}
source("include/predict.test.R")

for (source in c("blogs", "news", "twitter", "all")) {
    for (thres in 2:6) {
        set.seed(12345)
        predict.test.sb.cache(source, "testing", n.samples = 100000,
                              threshold = thres, removeStopwords = TRUE)
    
        set.seed(12345)
        predict.test.sb.cache(source, "testing", n.samples = 100000,
                              threshold = thres, removeStopwords = FALSE)
    }
}
```

On our hardware (Intel i5-7600K) commands above runs a bit more than 32 hours.

The chart and table below shows how our prediction quality changes as we
increase the threshold.

```{r threshold.all.chart, echo=FALSE, message=FALSE, fig.width=10}
stat.predict.threshold.all.chart()
```

```{r threshold.all.tbl, echo=FALSE, message=FALSE}
stat.predict.threshold.all.tbl <- function() {
  stat.testing <- stat.predict.sources.agg.all.build("testing")
  
  stat.testing.all <- stat.testing %>% filter(Source == "all")
  stat.testing.blogs <- stat.testing %>% filter(Source == "blogs")
  stat.testing.news <- stat.testing %>% filter(Source == "news")
  stat.testing.twitter <- stat.testing %>% filter(Source == "twitter")
  
  # Collect prediction precision for each combination of source and threshold.
  stat.testing.tbl <- data.frame(
    Threshold = 1:6,
    All.1 = stat.testing.all %>% filter(Rank == 1) %>% pull(Mean),
    Blogs.1 = stat.testing.blogs %>% filter(Rank == 1) %>% pull(Mean),
    News.1 = stat.testing.news %>% filter(Rank == 1) %>% pull(Mean),
    Twitter.1 = stat.testing.twitter %>% filter(Rank == 1) %>% pull(Mean),
    All.3 = stat.testing.all %>% filter(Rank == 3) %>% pull(Mean),
    Blogs.3 = stat.testing.blogs %>% filter(Rank == 3) %>% pull(Mean),
    News.3 = stat.testing.news %>% filter(Rank == 3) %>% pull(Mean),
    Twitter.3 = stat.testing.twitter %>% filter(Rank == 5) %>% pull(Mean),
    All.5 = stat.testing.all %>% filter(Rank == 5) %>% pull(Mean),
    Blogs.5 = stat.testing.blogs %>% filter(Rank == 5) %>% pull(Mean),
    News.5 = stat.testing.news %>% filter(Rank == 5) %>% pull(Mean),
    Twitter.5 = stat.testing.twitter %>% filter(Rank == 5) %>% pull(Mean),
    stringsAsFactors = FALSE
  )
  
  # Calculate the maximum value in each column and the corresponding index.
  stat.testing.max <- apply(stat.testing.tbl[,2:13], 2, max)
  All.1.max.idx <- match(stat.testing.max[1], stat.testing.tbl$All.1)
  Blogs.1.max.idx <- match(stat.testing.max[2], stat.testing.tbl$Blogs.1)
  News.1.max.idx <- match(stat.testing.max[3], stat.testing.tbl$News.1)
  Twitter.1.max.idx <- match(stat.testing.max[4], stat.testing.tbl$Twitter.1)
  All.3.max.idx <- match(stat.testing.max[5], stat.testing.tbl$All.3)
  Blogs.3.max.idx <- match(stat.testing.max[6], stat.testing.tbl$Blogs.3)
  News.3.max.idx <- match(stat.testing.max[7], stat.testing.tbl$News.3)
  Twitter.3.max.idx <- match(stat.testing.max[8], stat.testing.tbl$Twitter.3)
  All.5.max.idx <- match(stat.testing.max[9], stat.testing.tbl$All.5)
  Blogs.5.max.idx <- match(stat.testing.max[10], stat.testing.tbl$Blogs.5)
  News.5.max.idx <- match(stat.testing.max[11], stat.testing.tbl$News.5)
  Twitter.5.max.idx <- match(stat.testing.max[12], stat.testing.tbl$Twitter.5)

  # Calculate prediction precision with threshold 6 vs. best.
  stat.testing.6vsBest.tbl <- data.frame(
    Threshold = "Thres. 6 vs. best",
    All.1 = stat.testing.tbl$All.1[6] - max(stat.testing.tbl$All.1),
    Blogs.1 = stat.testing.tbl$Blogs.1[6] - max(stat.testing.tbl$Blogs.1),
    News.1 = stat.testing.tbl$News.1[6] - max(stat.testing.tbl$News.1),
    Twitter.1 = stat.testing.tbl$Twitter.1[6] - max(stat.testing.tbl$Twitter.1),
    All.3 = stat.testing.tbl$All.3[6] - max(stat.testing.tbl$All.3),
    Blogs.3 = stat.testing.tbl$Blogs.3[6] - max(stat.testing.tbl$Blogs.3),
    News.3 = stat.testing.tbl$News.3[6] - max(stat.testing.tbl$News.3),
    Twitter.3 = stat.testing.tbl$Twitter.3[6] - max(stat.testing.tbl$Twitter.3),
    All.5 = stat.testing.tbl$All.5[6] - max(stat.testing.tbl$All.5),
    Blogs.5 = stat.testing.tbl$Blogs.5[6] - max(stat.testing.tbl$Blogs.5),
    News.5 = stat.testing.tbl$News.5[6] - max(stat.testing.tbl$News.5),
    Twitter.5 = stat.testing.tbl$Twitter.5[6] - max(stat.testing.tbl$Twitter.5),
    stringsAsFactors = FALSE
  )

  # Merge the last 2 tables.
  stat.testing.tbl <- rbind(stat.testing.tbl, stat.testing.6vsBest.tbl)  
  
  # Format as percentage.
  stat.testing.tbl.fmt <- data.frame(cbind(
    stat.testing.tbl[,1],
    apply(stat.testing.tbl[,2:13], 2, function(x) sprintf("%.2f", x * 100))),
    stringsAsFactors = FALSE)
  
  max.background <- "#4CBB1740"
  
  # Highlight max. value in each column.
  stat.testing.tbl.fmt$All.1[All.1.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$All.1[All.1.max.idx], background = max.background)
  stat.testing.tbl.fmt$Blogs.1[Blogs.1.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$Blogs.1[Blogs.1.max.idx], background = max.background)
  stat.testing.tbl.fmt$News.1[News.1.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$News.1[News.1.max.idx], background = max.background)
  stat.testing.tbl.fmt$Twitter.1[Twitter.1.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$Twitter.1[Twitter.1.max.idx], background = max.background)

  stat.testing.tbl.fmt$All.3[All.3.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$All.3[All.3.max.idx], background = max.background)
  stat.testing.tbl.fmt$Blogs.3[Blogs.3.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$Blogs.3[Blogs.3.max.idx], background = max.background)
  stat.testing.tbl.fmt$News.3[News.3.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$News.3[News.3.max.idx], background = max.background)
  stat.testing.tbl.fmt$Twitter.3[Twitter.3.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$Twitter.3[Twitter.3.max.idx], background = max.background)

  stat.testing.tbl.fmt$All.5[All.5.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$All.5[All.5.max.idx], background = max.background)
  stat.testing.tbl.fmt$Blogs.5[Blogs.5.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$Blogs.5[Blogs.5.max.idx], background = max.background)
  stat.testing.tbl.fmt$News.5[News.5.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$News.5[News.5.max.idx], background = max.background)
  stat.testing.tbl.fmt$Twitter.5[Twitter.5.max.idx] <-
    cell_spec(stat.testing.tbl.fmt$Twitter.5[Twitter.5.max.idx], background = max.background)
  
  kable(stat.testing.tbl.fmt,
        caption = "Prediction precision (%) vs. threshold",
        booktabs = TRUE,
        format = "html", escape = FALSE,
        col.names = c("Threshold", rep(c("Aggr.", "Blogs", "News", "Twitter"), 3)),
        align = c("l", rep("r", 12))) %>%
    add_header_above(header = c("",
                                "Word correctly predicted" = 4,
                                "Word in Top 3" = 4,
                                "Word in Top 5" = 4)) %>%
    row_spec(7, bold = T, background = "#C0C0C0") %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
}
stat.predict.threshold.all.tbl()
```

The table above shows how precision of our prediction algorithm changes
dependent on the threshold. In each column the cell highlighted by a light
green contains the highest value, that is the best precision of the prediction.
The last line compares precision with threshold 6 vs. the best precision in the
same column.

From the chart and the table we may conclude the following:

* In most cases, we achieve the highest precision using threshold 2, that is
keeping only n-grams which appears in the corpora at least 2 times. The
intuitive explanation of this fact is that the Stupid Backoff algorithm scores
longer n-grams significantly higher than shorter n-grams, without taking into
account the absolute frequency. A 5-gram which appears just once could be 
a typo or an extremely seldom expression, but the Stupid Backoff algorithm very
often will assign to it a high score. Therefore, removing n-grams which appears
just once improves the precision of prediction.

* As we increase the threshold, the precision of our prediction is reduced, but
not very dramatically. If instead of aiming for the best precision of prediction
we use threshold 6, we lose in average 1.5%.

Now let us check size of n-gram tables in memory for various thresholds. The
following chart and table shows an impact on table sizes of gradually increasing
the threshold from 1 (include all n-grams) to 6 (include only n-grams which
appears in the corpora at least 6 times).

```{r echo=FALSE, message=FALSE, fig.align = "center"}
stat.ngram.optimize.all.chart()
```

```{r echo=FALSE, message=FALSE}
stat.size <- stat.ngram.optimize.all.cache()

Size.1 <- stat.size %>% filter(N == 1) %>% pull(Size)
Size.2 <- stat.size %>% filter(N == 2) %>% pull(Size)
Size.3 <- stat.size %>% filter(N == 3) %>% pull(Size)
Size.4 <- stat.size %>% filter(N == 4) %>% pull(Size)
Size.5 <- stat.size %>% filter(N == 5) %>% pull(Size)
Size.All <- Size.1 + Size.2 + Size.3 + Size.4 + Size.5

stat.size.tbl <- data.frame(
  Threshold = 1:6,
  Size.1 = Size.1,
  Size.1.Pct = sprintf("%.2f", Size.1 / Size.1[1] * 100),
  Size.2 = Size.2,
  Size.2.Pct = sprintf("%.2f", Size.2 / Size.2[1] * 100),
  Size.3 = Size.3,
  Size.3.Pct = sprintf("%.2f", Size.3 / Size.3[1] * 100),
  Size.4 = Size.4,
  Size.4.Pct = sprintf("%.2f", Size.4 / Size.4[1] * 100),
  Size.5 = Size.5,
  Size.5.Pct = sprintf("%.2f", Size.5 / Size.5[1] * 100),
  Size.All = Size.All,
  Size.All.Pct = sprintf("%.2f", Size.All / Size.All[1] * 100)
)

kable(stat.size.tbl,
      caption = "Size of n-gram tables in memory",
      booktabs = TRUE,
      col.names = c("Threshold", rep(c("Size, MiB", "% of Max"), 6)),
      align = c("l", rep("r", 12))) %>%
  add_header_above(header = c("",
                              "1-grams" = 2,
                              "2-grams" = 2,
                              "3-grams" = 2,
                              "4-grams" = 2,
                              "5-grams" = 2,
                              "Total" = 2)) %>%
  column_spec(12:13, bold = T, background = "#C0C0C0") %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                full_width = FALSE)
```

As the chart and the table demonstrate, increasing the threshold dramatically
reduces the required memory. As we increase the threshold from 1 to 6,
the size reduction is less for 1-grams (from `r stat.size.tbl$Size.1[1]` MiB to
`r stat.size.tbl$Size.1[6]` MiB, that is to `r stat.size.tbl$Size.1.Pct[6]`% of
the original), but is pretty spectacular for 5-grams (from
`r stat.size.tbl$Size.5[1]` MiB to `r stat.size.tbl$Size.5[6]` MiB, that is to
`r stat.size.tbl$Size.5.Pct[6]`% of the original). The total required memory
goes down from `r stat.size.tbl$Size.All[1]` MiB to
`r stat.size.tbl$Size.All[6]` MiB, that is to `r stat.size.tbl$Size.All.Pct[6]`%
of the original.

If we use threshold 6, that is include only n-grams which appears in the corpora
at least 6 times, the precision of our prediction is reduced by approximately
1.5%, but required memory falls to just `r stat.size.tbl$Size.All.Pct[6]`% of
the original, and is acceptable for a Shiny application. This looks like a good
deal, so we decided to use n-gram tables with the threshold 6.

# <a name="algorithm_speed"></a>Algorithm speed

A note on the speed of the algorithm. For each value of the threshold we
processed 100.000 samples for each of the sources (blogs, news and Twitter)
as well as additional 100.000 samples aggregated from all sources, for 400.000
samples in total.

```{r echo=FALSE, message=FALSE, fig.align = "center"}
stat.predict.time.chart()
```

```{r echo=FALSE, message=FALSE}
stat.predict.time.tbl <- stat.predict.time()

stat.predict.time.fmt <- data.frame(
  Threshold = stat.predict.time.tbl$Threshold,
  Duration.Fmt = stat.predict.time.tbl$Duration.Fmt,
  Samples = rep("400.000", 6),
  Processing.Avg.Ms = sprintf("%.1f", stat.predict.time.tbl$Processing.Avg.Ms))

kable(stat.predict.time.fmt,
      caption = "Average duration of prediction vs. threshold",
      booktabs = TRUE,
      col.names = c("Threshold", "Duration, hours", "No. of samples", " Duration per sample, ms"),
      align = c("l", "r", "r")) %>%
  kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                full_width = FALSE)
```

As the chart and table shows, increasing the threshold from 1 to 6 reduces an
average prediction duration from `r stat.predict.time.fmt$Processing.Avg.Ms[1]`
ms to `r stat.predict.time.fmt$Processing.Avg.Ms[6]` ms, or by
`r sprintf("%.2f%%", 100 * (1.0 - stat.predict.time.tbl$Processing.Avg.Ms[6] / stat.predict.time.tbl$Processing.Avg.Ms[1]))`.

# <a name="validation"></a>Validation

As the last step, we are going to validate our prediction algorithm on
out-of-sample data, that is on the data set
[we have saved for validation](#download_and_split) earlier. Our testing
approach is the same as before: 100 batches of 1.000 samples each for each
corpus, as well as for the aggregated corpora.

```{r eval=FALSE}
source("include/predict.test.R")

for (source in c("blogs", "news", "twitter", "all")) {
    set.seed(12345)
    predict.test.sb.cache(source, "validation", n.samples = 100000,
                          threshold = 6, removeStopwords = TRUE)
    
    set.seed(12345)
    predict.test.sb.cache(source, "validation", n.samples = 100000,
                          threshold = 6, removeStopwords = FALSE)
}
```

On our hardware (Intel i5-7600K) commands above runs a approximately 2.5 hours.

```{r echo=FALSE, message=FALSE, fig.align = "center", fig.width = 10}
stat.predict.validation.chart()
```

```{r echo=FALSE, message=FALSE}
stat.predict.validation.tbl.sw <- function() {
  stat.sw <- stat.predict.sources.agg.n.build(6, "validation", removeStopwords = FALSE) %>%
    arrange(Source) %>%
    mutate(Mean = Mean * 100,
           ConfIntLow = ConfIntLow * 100,
           ConfIntHigh = ConfIntHigh * 100)
  
  stat.sw.1 <- stat.sw %>% filter(Rank == 1)
  stat.sw.3 <- stat.sw %>% filter(Rank == 3)
  stat.sw.5 <- stat.sw %>% filter(Rank == 5)

  stat.sw.1.tbl <- data.frame(
    Source = c("Aggregated", "Blogs", "News", "Twitter"),
    Mean.1 = sprintf("%.2f", stat.sw.1$Mean),
    ConfInt.1 = sprintf("(%.2f, %.2f)", stat.sw.1$ConfIntLow, stat.sw.1$ConfIntHigh),
    Mean.3 = sprintf("%.2f", stat.sw.3$Mean),
    ConfInt.3 = sprintf("(%.2f, %.2f)", stat.sw.3$ConfIntLow, stat.sw.3$ConfIntHigh),
    Mean.5 = sprintf("%.2f", stat.sw.5$Mean),
    ConfInt.5 = sprintf("(%.2f, %.2f)", stat.sw.5$ConfIntLow, stat.sw.5$ConfIntHigh),
    stringsAsFactors = FALSE
  )
  
  kable(stat.sw.1.tbl,
        caption = "Prediction precision (validation)",
        booktabs = TRUE,
        col.names = c("Source", rep(c("Mean, %", "Conf. Int. 95%"), 3))) %>%
    add_header_above(header = c("",
                                "Word correctly predicted" = 2,
                                "Word in Top 3" = 2,
                                "Word in Top 5" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
}

stat.predict.validation.tbl.sw()
```

The off-sample validation results are very close to those observed before,
proving that the model is not overfitted.

# <a name="application"></a>Application

A [Shiny application](https://shiny.rstudio.com) using our prediction algorithm
is available online: https://serdioa.shinyapps.io/sb-predict.

The application GUI provides the following elements:

* Text area: enter the text here, and predicted words will appear on the chart
below. The chart shows multiple candidates ordered by score, providing a visual
clue on how probable each candidate is. When "Predict partially entered words"
is activated, **you have to type the space character to predict the next word**,
otherwise an ending of the current word is predicted.

* Number of predictions: choose from 1 to 10 candidates to predict.

* Predict partially entered words: select the checkbox to activate the extension
which predicts partially entered words.

* Random sample: populate the text area with a random sample from a selection
of over 1000 prepared sample texts.

![](presentation/application.png)

* The application shows predictions as you type, displaying a chart with ranks
of predicted words.

![](presentation/prediction.png)

* The prediction itself takes less than 20 ms, most of the observed delay is due
to the Shiny framework and network latency.

# <a name="references"></a>References

<a name="hc_corpora"></a>(1) HC Corpora provided by [corpora.epizy.com](http://corpora.epizy.com).
[About the corpora](http://corpora.epizy.com/about.html). [Download the corpora](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).

<a name="stupid_backoff"></a>(2) Thorsten Brants, Ashok C. Popat, Peng Xu,
Franz J. Och, Jeffrey Dean. 2007. Large Language Models in Machine
Translation. [https://www.aclweb.org/anthology/D07-1090](https://www.aclweb.org/anthology/D07-1090).
