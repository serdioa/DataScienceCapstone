---
title: "Splitting the Corpora"
author: "Alexey Serdyuk"
date: "30 May 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a part of the capstone project for the cycle of courses
[Data Science Specialization](https://www.coursera.org/specializations/jhu-data-science)
offered on [Coursera](https://www.coursera.org) by
[Johns Hopkins University](https://www.jhu.edu).

The dataset to be used for the Capstone project may be downloaded from the
location provided in the course: https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

The zip file contains corpora in several languages: English, German, Russian
and Finnish. In our project we will use only English corpora.

Corpora in each language, including English, contains 3 files with content
obtained from different sources: news, blogs and twitter.

As the first step, we will split each relevant file on 3 parts:

* Training set (60%) will be used to build and train the algorithm.
* Testing set (20%) will be used to test the algorithm during it's development.
This set may be used more than once.
* Validation set (20%) will be used for a final validation and estimation
of out-of-sample performance. This set will be used only once.

We define a function which splits the specified file on parts described above:

```{r}
splitFile <- function(name, out.dir) {
    # Reading dataset from the input file.
    data <- readr::read_lines(name)

    # Prepare list with indexes of all data items.
    data.index <- 1:length(data)

    # Sample indices for the training data set, and create a set with remaining
    # indices.
    training.index <- sample(data.index, 0.6 * length(data.index))
    remaining.index <- data.index[! data.index %in% training.index]

    # Sample indices for the testing data set, and use remaining indices
    # for a validation data set.
    testing.index <- sample(remaining.index, 0.5 * length(remaining.index))
    validation.index <- remaining.index[! remaining.index %in% testing.index]

    # Split the data.
    data.training <- data[training.index]
    data.testing <- data[testing.index]
    data.validation <- data[validation.index]
    
    # Create an output directory, if it does not exist.
    if (!dir.exists(out.dir)) {
        dir.create(out.dir)
    }
    
    # Prepare names for output files. We append suffixes "training", "testing"
    # and "validation" to the input file name before the extension.
    base <- basename(name)
    outTraining <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.training.txt", base))
    outTesting <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.testing.txt", base))
    outValidation <- file.path(out.dir, sub("(.)\\.[^.]+$", "\\1.validation.txt", base))

    # Writing datasets to output files.
    readr::write_lines(data.training, outTraining)
    readr::write_lines(data.testing, outTesting)
    readr::write_lines(data.validation, outValidation)
}
```

To make results reproduceable, we set the seed of the random number generator.

```{r}
set.seed(20190530)
```

Finally, we split each of the data files.

```{r}
splitFile("data/en_US.blogs.txt", "data-split")
splitFile("data/en_US.news.txt", "data-split")
splitFile("data/en_US.twitter.txt", "data-split")
```

As a sanity check, we count a number of lines in each source file, as well in
the partial files produced by the split.

```{r message=FALSE}
library(R.utils)

count.blogs <- countLines("data/en_US.blogs.txt")
count.blogs.training <- countLines("data-split/en_US.blogs.training.txt")
count.blogs.testing <- countLines("data-split/en_US.blogs.testing.txt")
count.blogs.validation <- countLines("data-split/en_US.blogs.validation.txt")

count.news <- countLines("data/en_US.news.txt")
count.news.training <- countLines("data-split/en_US.news.training.txt")
count.news.testing <- countLines("data-split/en_US.news.testing.txt")
count.news.validation <- countLines("data-split/en_US.news.validation.txt")

count.twitter <- countLines("data/en_US.twitter.txt")
count.twitter.training <- countLines("data-split/en_US.twitter.training.txt")
count.twitter.testing <- countLines("data-split/en_US.twitter.testing.txt")
count.twitter.validation <- countLines("data-split/en_US.twitter.validation.txt")
```

```{r echo=FALSE}
library(kableExtra)

corpora.count <- data.frame("blogs" = c(count.blogs.training,
                                        count.blogs.testing,
                                        count.blogs.validation,
                                        count.blogs,
                                        count.blogs - (count.blogs.training +
                                                           count.blogs.testing +
                                                           count.blogs.validation)),
                            "blogs.pct" = c(count.blogs.training / count.blogs * 100,
                                            count.blogs.testing / count.blogs * 100,
                                            count.blogs.validation / count.blogs * 100,
                                            count.blogs / count.blogs * 100,
                                            NA),
                            "news" = c(count.news.training,
                                       count.news.testing,
                                       count.news.validation,
                                       count.news,
                                       count.news - (count.news.training +
                                                         count.news.testing +
                                                         count.news.validation)),
                            "news.pct" = c(count.news.training / count.news * 100,
                                            count.news.testing / count.news * 100,
                                            count.news.validation / count.news * 100,
                                            count.news / count.news * 100,
                                            NA),
                            "twitter" = c(count.twitter.training,
                                          count.twitter.testing,
                                          count.twitter.validation,
                                          count.twitter,
                                          count.twitter - (count.twitter.training +
                                                               count.twitter.testing +
                                                               count.twitter.validation)),
                            "twitter.pct" = c(count.twitter.training / count.twitter * 100,
                                            count.twitter.testing / count.twitter * 100,
                                            count.twitter.validation / count.twitter * 100,
                                            count.twitter / count.twitter * 100,
                                            NA)
)
rownames(corpora.count) <- c("Training", "Testing", "Validation", "Total",
                             "Control (expected to be 0)")
kable(corpora.count,
      booktabs = TRUE,
      col.names = c("Rows", "%", "Rows", "%", "Rows", "%")) %>%
    add_header_above(header = c("",
                                "Blogs" = 2,
                                "News" = 2,
                                "Twitter" = 2)) %>%
    kable_styling(bootstrap_options = c("striped", "bordered", "hover"),
                  full_width = FALSE)
```

As the table shows, we have splitted the data on sub-sets as intended.
